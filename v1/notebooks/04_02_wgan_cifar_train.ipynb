{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# WGAN Training - CIFAR-10 Horses\n",
                "\n",
                "This notebook trains a Wasserstein GAN (WGAN) on the CIFAR-10 dataset to\n",
                "generate realistic horse images.\n",
                "\n",
                "## Features\n",
                "\n",
                "- **Weight Clipping**: Lipschitz constraint via weight clipping\n",
                "- **Wasserstein Loss**: Meaningful loss for training stability\n",
                "- **No BatchNorm in Critic**: Standard WGAN architecture\n",
                "- **W&B Integration**: Full experiment tracking with Weights & Biases\n",
                "\n",
                "## Architecture\n",
                "\n",
                "- **Critic**: 4-layer CNN (no sigmoid, outputs unbounded scores)\n",
                "- **Generator**: 4-layer deconvolution with upsampling\n",
                "\n",
                "## References\n",
                "\n",
                "- Arjovsky et al. \"Wasserstein GAN\" (2017)\n",
                "- Chapter 4 of \"Generative Deep Learning\" book"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## GPU Memory Setup\n",
                "\n",
                "Configure TensorFlow to use memory growth, preventing OOM errors."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-01-07 06:45:01.937894: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
                        "To enable the following instructions: FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
                        "/home/cataluna84/Workspace-Antigravity/Generative_Deep_Learning/.venv/lib/python3.13/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
                        "  if not hasattr(np, \"object\"):\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u2713 GPU(s) available: ['/physical_device:GPU:0']\n"
                    ]
                }
            ],
            "source": [
                "# =============================================================================\n",
                "# GPU MEMORY CONFIGURATION\n",
                "# =============================================================================\n",
                "# Enable memory growth to prevent TensorFlow from allocating all GPU memory\n",
                "# at once. This must be done BEFORE any other TensorFlow operations.\n",
                "\n",
                "import tensorflow as tf\n",
                "\n",
                "# Get list of available GPUs\n",
                "gpus = tf.config.list_physical_devices('GPU')\n",
                "\n",
                "if gpus:\n",
                "    # Enable memory growth for each GPU to prevent OOM errors\n",
                "    for gpu in gpus:\n",
                "        tf.config.experimental.set_memory_growth(gpu, True)\n",
                "    print(f\"\u2713 GPU(s) available: {[gpu.name for gpu in gpus]}\")\n",
                "else:\n",
                "    print(\"\u26a0 WARNING: No GPU detected, running on CPU\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Imports\n",
                "\n",
                "Import all required modules including TensorFlow, visualization libraries,\n",
                "project-specific WGAN model, data loaders, and experiment tracking."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# IMPORTS\n",
                "# =============================================================================\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# Enable inline plotting for Jupyter notebooks\n",
                "# -----------------------------------------------------------------------------\n",
                "%matplotlib inline\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# Standard Library\n",
                "# -----------------------------------------------------------------------------\n",
                "import os\n",
                "import sys\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# Third-Party Libraries\n",
                "# -----------------------------------------------------------------------------\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# Path Configuration\n",
                "# Add parent directories to system path for accessing modules\n",
                "# -----------------------------------------------------------------------------\n",
                "sys.path.insert(0, \"..\")       # For v1/src modules\n",
                "sys.path.insert(0, \"../..\")    # For project root utils/\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# Project Modules\n",
                "# -----------------------------------------------------------------------------\n",
                "# WGAN: Wasserstein GAN implementation with critic (not discriminator)\n",
                "from src.models.WGAN import WGAN\n",
                "\n",
                "# load_cifar: Loads CIFAR-10/100 dataset filtered by class label\n",
                "# Returns images normalized to [-1, 1] range\n",
                "from src.utils.loaders import load_cifar\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# Experiment Tracking\n",
                "# -----------------------------------------------------------------------------\n",
                "import wandb\n",
                "from utils.wandb_utils import init_wandb, log_images"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Global Configuration\n",
                "\n",
                "Define run parameters including experiment identification, output directories,\n",
                "and training mode (build new model or load existing weights)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Run folder: ../run/gan/0002_horses\n",
                        "Mode: build\n"
                    ]
                }
            ],
            "source": [
                "# =============================================================================\n",
                "# GLOBAL CONFIGURATION\n",
                "# =============================================================================\n",
                "# These values define the experiment identity and output locations.\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# Run Identification\n",
                "# -----------------------------------------------------------------------------\n",
                "SECTION = 'gan'          # Parent folder for all GAN experiments\n",
                "RUN_ID = '0002'          # Unique identifier for this run\n",
                "DATA_NAME = 'horses'     # Dataset/class name (CIFAR-10 class 7)\n",
                "RUN_FOLDER = f'../run/{SECTION}/{RUN_ID}_{DATA_NAME}'\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# Create Output Directories\n",
                "# - viz: Training visualization plots (loss curves, etc.)\n",
                "# - images: Generated sample images at checkpoints\n",
                "# - weights: Model weight checkpoints\n",
                "# -----------------------------------------------------------------------------\n",
                "if not os.path.exists(RUN_FOLDER):\n",
                "    os.makedirs(RUN_FOLDER)\n",
                "    os.makedirs(os.path.join(RUN_FOLDER, 'viz'))\n",
                "    os.makedirs(os.path.join(RUN_FOLDER, 'images'))\n",
                "    os.makedirs(os.path.join(RUN_FOLDER, 'weights'))\n",
                "\n",
                "# Training mode: 'build' creates new model, 'load' resumes from checkpoint\n",
                "MODE = 'build'\n",
                "\n",
                "print(f\"Run folder: {RUN_FOLDER}\")\n",
                "print(f\"Mode: {MODE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Loading\n",
                "\n",
                "Load the CIFAR-10 dataset filtered to a specific class (horses = label 7).\n",
                "Images are 32x32 RGB and normalized to [-1, 1] range for GAN training.\n",
                "\n",
                "**CIFAR-10 Class Labels:**\n",
                "- 0: airplane, 1: automobile, 2: bird, 3: cat, 4: deer\n",
                "- 5: dog, 6: frog, 7: horse, 8: ship, 9: truck"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/cataluna84/Workspace-Antigravity/Generative_Deep_Learning/.venv/lib/python3.13/site-packages/keras/src/datasets/cifar.py:18: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
                        "  d = cPickle.load(f, encoding=\"bytes\")\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset: CIFAR-10 (horses)\n",
                        "Training samples: 6,000\n",
                        "Image dimensions: (32, 32, 3)\n",
                        "Data type: float32\n",
                        "Value range: [-1.00, 1.00]\n"
                    ]
                }
            ],
            "source": [
                "# =============================================================================\n",
                "# DATA LOADING\n",
                "# =============================================================================\n",
                "# Load CIFAR-10 dataset filtered by class label.\n",
                "# The load_cifar function:\n",
                "#   1. Downloads CIFAR-10 if not present\n",
                "#   2. Filters images by the specified label\n",
                "#   3. Normalizes pixel values from [0, 255] to [-1, 1]\n",
                "\n",
                "# Map data name to CIFAR-10 class label\n",
                "CIFAR_LABELS = {\n",
                "    'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4,\n",
                "    'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9,\n",
                "    # Alternate names\n",
                "    'cars': 1, 'horses': 7\n",
                "}\n",
                "\n",
                "# Get label from data name (default to horses if not found)\n",
                "label = CIFAR_LABELS.get(DATA_NAME, 7)\n",
                "\n",
                "# Load filtered dataset\n",
                "# Parameters:\n",
                "#   - label: Class index to filter (7 = horses)\n",
                "#   - 10: Use CIFAR-10 (vs CIFAR-100)\n",
                "(x_train, y_train) = load_cifar(label, 10)\n",
                "\n",
                "# Print dataset information\n",
                "print(f\"Dataset: CIFAR-10 ({DATA_NAME})\")\n",
                "print(f\"Training samples: {x_train.shape[0]:,}\")\n",
                "print(f\"Image dimensions: {x_train.shape[1:]}\")\n",
                "print(f\"Data type: {x_train.dtype}\")\n",
                "print(f\"Value range: [{x_train.min():.2f}, {x_train.max():.2f}]\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 400x400 with 1 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# =============================================================================\n",
                "# SAMPLE VISUALIZATION\n",
                "# =============================================================================\n",
                "# Display a sample image from the dataset.\n",
                "# Note: Images are rescaled from [-1, 1] to [0, 1] for proper display.\n",
                "\n",
                "# Select random sample and rescale for visualization\n",
                "sample_idx = 150\n",
                "sample_img = (x_train[sample_idx, :, :, :] + 1) / 2  # Rescale [-1,1] -> [0,1]\n",
                "\n",
                "plt.figure(figsize=(4, 4))\n",
                "plt.imshow(sample_img)\n",
                "plt.title(f'Sample {DATA_NAME.capitalize()} Image (idx={sample_idx})')\n",
                "plt.axis('off')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Model Hyperparameters\n",
                "\n",
                "Define all model architecture hyperparameters as global constants.\n",
                "\n",
                "**Why global constants?** Following the DRY (Don't Repeat Yourself) principle,\n",
                "we define hyperparameters once and reference them in both model creation\n",
                "and W&B config. This prevents inconsistencies and makes experiments easier to modify."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u2713 Model hyperparameters defined\n"
                    ]
                }
            ],
            "source": [
                "# =============================================================================\n",
                "# MODEL HYPERPARAMETERS\n",
                "# =============================================================================\n",
                "# Define all architecture hyperparameters as global constants.\n",
                "# These are used for both model creation AND W&B config logging.\n",
                "#\n",
                "# Best Practice: Define hyperparameters once in a central location to:\n",
                "#   1. Avoid duplication and potential inconsistencies\n",
                "#   2. Make experiment configuration changes easy\n",
                "#   3. Ensure W&B logs match actual model parameters\n",
                "# =============================================================================\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# Input Configuration\n",
                "# -----------------------------------------------------------------------------\n",
                "INPUT_DIM = (32, 32, 3)       # CIFAR-10 image dimensions\n",
                "Z_DIM = 100                    # Latent space dimension\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# Critic (Discriminator) Configuration\n",
                "# -----------------------------------------------------------------------------\n",
                "CRITIC_FILTERS = [32, 64, 128, 128]\n",
                "CRITIC_KERNEL_SIZE = [5, 5, 5, 5]\n",
                "CRITIC_STRIDES = [2, 2, 2, 1]\n",
                "CRITIC_ACTIVATION = 'leaky_relu'\n",
                "CRITIC_LEARNING_RATE = 0.00005\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# Generator Configuration\n",
                "# -----------------------------------------------------------------------------\n",
                "GENERATOR_INITIAL_SIZE = (4, 4, 128)\n",
                "GENERATOR_UPSAMPLE = [2, 2, 2, 1]\n",
                "GENERATOR_FILTERS = [128, 64, 32, 3]\n",
                "GENERATOR_KERNEL_SIZE = [5, 5, 5, 5]\n",
                "GENERATOR_STRIDES = [1, 1, 1, 1]\n",
                "GENERATOR_BATCH_NORM_MOMENTUM = 0.8\n",
                "GENERATOR_ACTIVATION = 'leaky_relu'\n",
                "GENERATOR_LEARNING_RATE = 0.00005\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# Optimizer\n",
                "# -----------------------------------------------------------------------------\n",
                "OPTIMIZER = 'rmsprop'\n",
                "\n",
                "print(\"\u2713 Model hyperparameters defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Model Architecture\n",
                "\n",
                "Build the WGAN with critic and generator networks using the global hyperparameters.\n",
                "\n",
                "### Critic Architecture (replaces Discriminator)\n",
                "- 4 convolutional layers with increasing filters\n",
                "- LeakyReLU activation (no BatchNorm - important for WGAN stability)\n",
                "- No sigmoid output - produces unbounded \"realness\" score\n",
                "- Uses RMSprop optimizer with low learning rate\n",
                "\n",
                "### Generator Architecture\n",
                "- Dense layer \u2192 Reshape to 4\u00d74\u00d7128\n",
                "- 4 upsampling + conv layers to reach 32\u00d732\u00d73\n",
                "- BatchNorm + LeakyReLU after each conv layer\n",
                "- Tanh output activation for [-1, 1] range\n",
                "\n",
                "### Key WGAN Differences from Standard GAN\n",
                "- **Critic vs Discriminator**: Outputs real number (not probability)\n",
                "- **Weight Clipping**: Enforces Lipschitz constraint\n",
                "- **Wasserstein Loss**: More meaningful gradient signal"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
                        "I0000 00:00:1767768316.817643    5497 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6094 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u2713 WGAN model built successfully\n"
                    ]
                }
            ],
            "source": [
                "# =============================================================================\n",
                "# MODEL ARCHITECTURE\n",
                "# =============================================================================\n",
                "# Build the WGAN using the global hyperparameters defined above.\n",
                "# This ensures consistency between model and W&B config.\n",
                "\n",
                "if MODE == 'build':\n",
                "    gan = WGAN(\n",
                "        # Input configuration (uses global constants)\n",
                "        input_dim=INPUT_DIM,\n",
                "        z_dim=Z_DIM,\n",
                "\n",
                "        # Critic configuration (uses global constants)\n",
                "        critic_conv_filters=CRITIC_FILTERS,\n",
                "        critic_conv_kernel_size=CRITIC_KERNEL_SIZE,\n",
                "        critic_conv_strides=CRITIC_STRIDES,\n",
                "        critic_batch_norm_momentum=None,\n",
                "        critic_activation=CRITIC_ACTIVATION,\n",
                "        critic_dropout_rate=None,\n",
                "        critic_learning_rate=CRITIC_LEARNING_RATE,\n",
                "\n",
                "        # Generator configuration (uses global constants)\n",
                "        generator_initial_dense_layer_size=GENERATOR_INITIAL_SIZE,\n",
                "        generator_upsample=GENERATOR_UPSAMPLE,\n",
                "        generator_conv_filters=GENERATOR_FILTERS,\n",
                "        generator_conv_kernel_size=GENERATOR_KERNEL_SIZE,\n",
                "        generator_conv_strides=GENERATOR_STRIDES,\n",
                "        generator_batch_norm_momentum=GENERATOR_BATCH_NORM_MOMENTUM,\n",
                "        generator_activation=GENERATOR_ACTIVATION,\n",
                "        generator_dropout_rate=None,\n",
                "        generator_learning_rate=GENERATOR_LEARNING_RATE,\n",
                "\n",
                "        # Optimizer (uses global constant)\n",
                "        optimiser=OPTIMIZER\n",
                "    )\n",
                "    # Save model architecture and config to run folder\n",
                "    gan.save(RUN_FOLDER)\n",
                "    print(\"\u2713 WGAN model built successfully\")\n",
                "\n",
                "else:\n",
                "    # Load pre-trained weights for continued training or inference\n",
                "    gan.load_weights(os.path.join(RUN_FOLDER, 'weights/weights.h5'))\n",
                "    print(\"\u2713 Loaded weights from checkpoint\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Critic Summary\n",
                "\n",
                "Display the critic network architecture showing layer dimensions and parameters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[1mModel: \"functional\"\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n",
                            "\u2503<span style=\"font-weight: bold\"> Layer (type)                    </span>\u2503<span style=\"font-weight: bold\"> Output Shape           </span>\u2503<span style=\"font-weight: bold\">       Param # </span>\u2503\n",
                            "\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n",
                            "\u2502 critic_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)       \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      \u2502             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 critic_conv_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     \u2502         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,432</span> \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     \u2502             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 critic_conv_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       \u2502        <span style=\"color: #00af00; text-decoration-color: #00af00\">51,264</span> \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       \u2502             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 critic_conv_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      \u2502       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 leaky_re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      \u2502             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 critic_conv_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      \u2502       <span style=\"color: #00af00; text-decoration-color: #00af00\">409,728</span> \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 leaky_re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      \u2502             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           \u2502             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              \u2502         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,049</span> \u2502\n",
                            "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n",
                            "\u2503\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u2503\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u2503\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\u2503\n",
                            "\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n",
                            "\u2502 critic_input (\u001b[38;5;33mInputLayer\u001b[0m)       \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m3\u001b[0m)      \u2502             \u001b[38;5;34m0\u001b[0m \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 critic_conv_0 (\u001b[38;5;33mConv2D\u001b[0m)          \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     \u2502         \u001b[38;5;34m2,432\u001b[0m \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)         \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     \u2502             \u001b[38;5;34m0\u001b[0m \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 critic_conv_1 (\u001b[38;5;33mConv2D\u001b[0m)          \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u2502        \u001b[38;5;34m51,264\u001b[0m \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)       \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u2502             \u001b[38;5;34m0\u001b[0m \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 critic_conv_2 (\u001b[38;5;33mConv2D\u001b[0m)          \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      \u2502       \u001b[38;5;34m204,928\u001b[0m \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 leaky_re_lu_2 (\u001b[38;5;33mLeakyReLU\u001b[0m)       \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      \u2502             \u001b[38;5;34m0\u001b[0m \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 critic_conv_3 (\u001b[38;5;33mConv2D\u001b[0m)          \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      \u2502       \u001b[38;5;34m409,728\u001b[0m \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 leaky_re_lu_3 (\u001b[38;5;33mLeakyReLU\u001b[0m)       \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      \u2502             \u001b[38;5;34m0\u001b[0m \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 flatten (\u001b[38;5;33mFlatten\u001b[0m)               \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           \u2502             \u001b[38;5;34m0\u001b[0m \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 dense (\u001b[38;5;33mDense\u001b[0m)                   \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              \u2502         \u001b[38;5;34m2,049\u001b[0m \u2502\n",
                            "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">670,401</span> (2.56 MB)\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m670,401\u001b[0m (2.56 MB)\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">670,401</span> (2.56 MB)\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m670,401\u001b[0m (2.56 MB)\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# =============================================================================\n",
                "# CRITIC ARCHITECTURE SUMMARY\n",
                "# =============================================================================\n",
                "# The critic is trained to distinguish real from generated images,\n",
                "# outputting a higher score for more \"real-looking\" inputs.\n",
                "\n",
                "gan.critic.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Generator Summary\n",
                "\n",
                "Display the generator network architecture showing layer dimensions and parameters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n",
                            "\u2503<span style=\"font-weight: bold\"> Layer (type)                    </span>\u2503<span style=\"font-weight: bold\"> Output Shape           </span>\u2503<span style=\"font-weight: bold\">       Param # </span>\u2503\n",
                            "\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n",
                            "\u2502 generator_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            \u2502             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           \u2502       <span style=\"color: #00af00; text-decoration-color: #00af00\">206,848</span> \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 batch_normalization             \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           \u2502         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> \u2502\n",
                            "\u2502 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            \u2502                        \u2502               \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 leaky_re_lu_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           \u2502             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      \u2502             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 up_sampling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)    \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      \u2502             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 generator_conv_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)       \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      \u2502       <span style=\"color: #00af00; text-decoration-color: #00af00\">409,728</span> \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 batch_normalization_1           \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      \u2502           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> \u2502\n",
                            "\u2502 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            \u2502                        \u2502               \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 leaky_re_lu_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      \u2502             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 up_sampling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)  \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    \u2502             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 generator_conv_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)       \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     \u2502       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,864</span> \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 batch_normalization_2           \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     \u2502           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> \u2502\n",
                            "\u2502 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            \u2502                        \u2502               \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 leaky_re_lu_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     \u2502             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 up_sampling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)  \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     \u2502             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 generator_conv_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)       \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     \u2502        <span style=\"color: #00af00; text-decoration-color: #00af00\">51,232</span> \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 batch_normalization_3           \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     \u2502           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> \u2502\n",
                            "\u2502 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            \u2502                        \u2502               \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 leaky_re_lu_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     \u2502             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 generator_conv_3                \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      \u2502         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,403</span> \u2502\n",
                            "\u2502 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               \u2502                        \u2502               \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         \u2502 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      \u2502             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \u2502\n",
                            "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n",
                            "\u2503\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u2503\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u2503\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\u2503\n",
                            "\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n",
                            "\u2502 generator_input (\u001b[38;5;33mInputLayer\u001b[0m)    \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            \u2502             \u001b[38;5;34m0\u001b[0m \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 dense_1 (\u001b[38;5;33mDense\u001b[0m)                 \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           \u2502       \u001b[38;5;34m206,848\u001b[0m \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 batch_normalization             \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           \u2502         \u001b[38;5;34m8,192\u001b[0m \u2502\n",
                            "\u2502 (\u001b[38;5;33mBatchNormalization\u001b[0m)            \u2502                        \u2502               \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 leaky_re_lu_4 (\u001b[38;5;33mLeakyReLU\u001b[0m)       \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           \u2502             \u001b[38;5;34m0\u001b[0m \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 reshape (\u001b[38;5;33mReshape\u001b[0m)               \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      \u2502             \u001b[38;5;34m0\u001b[0m \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 up_sampling2d (\u001b[38;5;33mUpSampling2D\u001b[0m)    \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      \u2502             \u001b[38;5;34m0\u001b[0m \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 generator_conv_0 (\u001b[38;5;33mConv2D\u001b[0m)       \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      \u2502       \u001b[38;5;34m409,728\u001b[0m \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 batch_normalization_1           \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      \u2502           \u001b[38;5;34m512\u001b[0m \u2502\n",
                            "\u2502 (\u001b[38;5;33mBatchNormalization\u001b[0m)            \u2502                        \u2502               \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 leaky_re_lu_5 (\u001b[38;5;33mLeakyReLU\u001b[0m)       \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      \u2502             \u001b[38;5;34m0\u001b[0m \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 up_sampling2d_1 (\u001b[38;5;33mUpSampling2D\u001b[0m)  \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    \u2502             \u001b[38;5;34m0\u001b[0m \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 generator_conv_1 (\u001b[38;5;33mConv2D\u001b[0m)       \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     \u2502       \u001b[38;5;34m204,864\u001b[0m \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 batch_normalization_2           \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     \u2502           \u001b[38;5;34m256\u001b[0m \u2502\n",
                            "\u2502 (\u001b[38;5;33mBatchNormalization\u001b[0m)            \u2502                        \u2502               \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 leaky_re_lu_6 (\u001b[38;5;33mLeakyReLU\u001b[0m)       \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     \u2502             \u001b[38;5;34m0\u001b[0m \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 up_sampling2d_2 (\u001b[38;5;33mUpSampling2D\u001b[0m)  \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     \u2502             \u001b[38;5;34m0\u001b[0m \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 generator_conv_2 (\u001b[38;5;33mConv2D\u001b[0m)       \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     \u2502        \u001b[38;5;34m51,232\u001b[0m \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 batch_normalization_3           \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     \u2502           \u001b[38;5;34m128\u001b[0m \u2502\n",
                            "\u2502 (\u001b[38;5;33mBatchNormalization\u001b[0m)            \u2502                        \u2502               \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 leaky_re_lu_7 (\u001b[38;5;33mLeakyReLU\u001b[0m)       \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     \u2502             \u001b[38;5;34m0\u001b[0m \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 generator_conv_3                \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m3\u001b[0m)      \u2502         \u001b[38;5;34m2,403\u001b[0m \u2502\n",
                            "\u2502 (\u001b[38;5;33mConv2DTranspose\u001b[0m)               \u2502                        \u2502               \u2502\n",
                            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
                            "\u2502 activation (\u001b[38;5;33mActivation\u001b[0m)         \u2502 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m3\u001b[0m)      \u2502             \u001b[38;5;34m0\u001b[0m \u2502\n",
                            "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">884,163</span> (3.37 MB)\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m884,163\u001b[0m (3.37 MB)\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">879,619</span> (3.36 MB)\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m879,619\u001b[0m (3.36 MB)\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,544</span> (17.75 KB)\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,544\u001b[0m (17.75 KB)\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# =============================================================================\n",
                "# GENERATOR ARCHITECTURE SUMMARY\n",
                "# =============================================================================\n",
                "# The generator transforms random noise vectors into realistic images,\n",
                "# trained to maximize the critic's score on generated samples.\n",
                "\n",
                "gan.generator.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training Configuration\n",
                "\n",
                "Define training hyperparameters specific to WGAN training.\n",
                "\n",
                "**Key WGAN Training Parameters:**\n",
                "- **N_CRITIC**: Number of critic updates per generator update (typically 5)\n",
                "- **CLIP_THRESHOLD**: Weight clipping value for Lipschitz constraint\n",
                "- **BATCH_SIZE**: Samples per training step\n",
                "- **EPOCHS**: Total training iterations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
                        "WGAN TRAINING CONFIGURATION\n",
                        "\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
                        "Batch Size:       512\n",
                        "Epochs:           6000\n",
                        "Critic Steps:     5 per generator step\n",
                        "Weight Clip:      \u00b10.01\n",
                        "Checkpoint:       Every 50 batches\n",
                        "\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n"
                    ]
                }
            ],
            "source": [
                "# =============================================================================\n",
                "# TRAINING CONFIGURATION\n",
                "# =============================================================================\n",
                "# WGAN-specific hyperparameters for stable training.\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# Batch and Epoch Settings\n",
                "# -----------------------------------------------------------------------------\n",
                "BATCH_SIZE = 512          # Number of samples per training step\n",
                "EPOCHS = 6000             # Total training epochs\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# Checkpoint Settings\n",
                "# -----------------------------------------------------------------------------\n",
                "PRINT_EVERY_N_BATCHES = 50  # Save images/weights every N batches\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# WGAN-Specific Parameters\n",
                "# -----------------------------------------------------------------------------\n",
                "# N_CRITIC: Train critic N times per generator update\n",
                "# This ensures the critic provides accurate gradients to the generator\n",
                "N_CRITIC = 5\n",
                "\n",
                "# CLIP_THRESHOLD: Clip critic weights to [-c, c] after each update\n",
                "# This enforces the Lipschitz constraint required for Wasserstein distance\n",
                "CLIP_THRESHOLD = 0.01\n",
                "\n",
                "print(\"\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\")\n",
                "print(\"WGAN TRAINING CONFIGURATION\")\n",
                "print(\"\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\")\n",
                "print(f\"Batch Size:       {BATCH_SIZE}\")\n",
                "print(f\"Epochs:           {EPOCHS}\")\n",
                "print(f\"Critic Steps:     {N_CRITIC} per generator step\")\n",
                "print(f\"Weight Clip:      \u00b1{CLIP_THRESHOLD}\")\n",
                "print(f\"Checkpoint:       Every {PRINT_EVERY_N_BATCHES} batches\")\n",
                "print(\"\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## W&B Experiment Tracking\n",
                "\n",
                "Initialize Weights & Biases for logging training metrics and generated samples.\n",
                "All hyperparameters reference the global constants defined above to ensure consistency."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcataluna84\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Tracking run with wandb version 0.23.1"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Run data is saved locally in <code>/home/cataluna84/Workspace-Antigravity/Generative_Deep_Learning/v1/notebooks/wandb/run-20260107_064520-x5ln97by</code>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Syncing run <strong><a href='https://wandb.ai/cataluna84/generative-deep-learning/runs/x5ln97by' target=\"_blank\">wgan_horses_0002</a></strong> to <a href='https://wandb.ai/cataluna84/generative-deep-learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View project at <a href='https://wandb.ai/cataluna84/generative-deep-learning' target=\"_blank\">https://wandb.ai/cataluna84/generative-deep-learning</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View run at <a href='https://wandb.ai/cataluna84/generative-deep-learning/runs/x5ln97by' target=\"_blank\">https://wandb.ai/cataluna84/generative-deep-learning/runs/x5ln97by</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u2713 W&B run initialized: wgan_horses_0002\n",
                        "  View at: https://wandb.ai/cataluna84/generative-deep-learning/runs/x5ln97by\n"
                    ]
                }
            ],
            "source": [
                "# =============================================================================\n",
                "# W&B INITIALIZATION\n",
                "# =============================================================================\n",
                "# Initialize Weights & Biases for experiment tracking.\n",
                "# Config uses the global constants defined above - no duplication!\n",
                "\n",
                "run = init_wandb(\n",
                "    name=f\"wgan_{DATA_NAME}_{RUN_ID}\",\n",
                "    project=\"generative-deep-learning\",\n",
                "    config={\n",
                "        # Model type and dataset\n",
                "        \"model_type\": \"WGAN\",\n",
                "        \"dataset\": f\"cifar10_{DATA_NAME}\",\n",
                "\n",
                "        # Input configuration (references global constants)\n",
                "        \"input_dim\": INPUT_DIM,\n",
                "        \"z_dim\": Z_DIM,\n",
                "\n",
                "        # Critic configuration (references global constants)\n",
                "        \"critic_filters\": CRITIC_FILTERS,\n",
                "        \"critic_activation\": CRITIC_ACTIVATION,\n",
                "        \"critic_learning_rate\": CRITIC_LEARNING_RATE,\n",
                "\n",
                "        # Generator configuration (references global constants)\n",
                "        \"generator_filters\": GENERATOR_FILTERS,\n",
                "        \"generator_activation\": GENERATOR_ACTIVATION,\n",
                "        \"generator_learning_rate\": GENERATOR_LEARNING_RATE,\n",
                "        \"generator_batch_norm_momentum\": GENERATOR_BATCH_NORM_MOMENTUM,\n",
                "\n",
                "        # Training configuration (references global constants)\n",
                "        \"batch_size\": BATCH_SIZE,\n",
                "        \"epochs\": EPOCHS,\n",
                "        \"n_critic\": N_CRITIC,\n",
                "        \"clip_threshold\": CLIP_THRESHOLD,\n",
                "        \"optimizer\": OPTIMIZER,\n",
                "    }\n",
                ")\n",
                "print(f\"\u2713 W&B run initialized: {run.name}\")\n",
                "print(f\"  View at: {run.url}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Training Execution\n",
                "\n",
                "Start the WGAN training loop. The training process:\n",
                "1. **Critic Phase**: Update critic N_CRITIC times using real and fake batches\n",
                "2. **Weight Clipping**: Clip critic weights to enforce Lipschitz constraint\n",
                "3. **Generator Phase**: Update generator once to fool the critic\n",
                "4. **Logging**: Save samples and weights at regular intervals"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-01-07 06:45:25.051388: I external/local_xla/xla/service/service.cc:163] XLA service 0x782ca8004be0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
                        "2026-01-07 06:45:25.051442: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 2070, Compute Capability 7.5\n",
                        "2026-01-07 06:45:25.104102: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
                        "2026-01-07 06:45:25.224300: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91701\n",
                        "2026-01-07 06:45:26.196627: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:546] Omitted potentially buggy algorithm eng14{k25=2} for conv (f32[32,64,16,16]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,128,16,16]{3,2,1,0}, f32[64,128,5,5]{3,2,1,0}, f32[64]{0}), window={size=5x5 pad=2_2x2_2}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]}\n",
                        "2026-01-07 06:45:26.469836: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:546] Omitted potentially buggy algorithm eng14{k25=2} for conv (f32[32,32,32,32]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,64,32,32]{3,2,1,0}, f32[32,64,5,5]{3,2,1,0}, f32[32]{0}), window={size=5x5 pad=2_2x2_2}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]}\n",
                        "I0000 00:00:1767768327.233463    5643 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
                        "2026-01-07 06:45:36.527703: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:546] Omitted potentially buggy algorithm eng14{k25=2} for conv (f32[512,128,8,8]{3,2,1,0}, u8[0]{0}) custom-call(f32[512,128,8,8]{3,2,1,0}, f32[128,128,5,5]{3,2,1,0}, f32[128]{0}), window={size=5x5 pad=2_2x2_2}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]}\n",
                        "2026-01-07 06:45:36.953017: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:546] Omitted potentially buggy algorithm eng14{k25=2} for conv (f32[512,64,16,16]{3,2,1,0}, u8[0]{0}) custom-call(f32[512,128,16,16]{3,2,1,0}, f32[64,128,5,5]{3,2,1,0}, f32[64]{0}), window={size=5x5 pad=2_2x2_2}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]}\n",
                        "2026-01-07 06:45:37.932830: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:546] Omitted potentially buggy algorithm eng14{k25=2} for conv (f32[512,32,32,32]{3,2,1,0}, u8[0]{0}) custom-call(f32[512,64,32,32]{3,2,1,0}, f32[32,64,5,5]{3,2,1,0}, f32[32]{0}), window={size=5x5 pad=2_2x2_2}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]}\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "0 [D loss: (0.000)(R 0.000, F 0.000)]  [G loss: -0.000] \n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-01-07 06:45:53.502155: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:546] Omitted potentially buggy algorithm eng14{k25=2} for conv (f32[25,64,16,16]{3,2,1,0}, u8[0]{0}) custom-call(f32[25,128,16,16]{3,2,1,0}, f32[64,128,5,5]{3,2,1,0}, f32[64]{0}), window={size=5x5 pad=2_2x2_2}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]}\n",
                        "2026-01-07 06:45:53.630671: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:546] Omitted potentially buggy algorithm eng14{k25=2} for conv (f32[25,32,32,32]{3,2,1,0}, u8[0]{0}) custom-call(f32[25,64,32,32]{3,2,1,0}, f32[32,64,5,5]{3,2,1,0}, f32[32]{0}), window={size=5x5 pad=2_2x2_2}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]}\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1 [D loss: (0.000)(R 0.000, F 0.000)]  [G loss: -0.001] \n",
                        "2 [D loss: (-0.000)(R -0.000, F 0.000)]  [G loss: -0.001] \n",
                        "3 [D loss: (-0.000)(R -0.000, F -0.000)]  [G loss: -0.001] \n",
                        "4 [D loss: (-0.000)(R -0.000, F -0.000)]  [G loss: -0.001] \n",
                        "5 [D loss: (-0.000)(R -0.000, F -0.000)]  [G loss: -0.002] \n",
                        "6 [D loss: (-0.000)(R -0.000, F -0.000)]  [G loss: -0.002] \n",
                        "7 [D loss: (-0.000)(R -0.000, F -0.000)]  [G loss: -0.002] \n",
                        "8 [D loss: (-0.000)(R -0.000, F -0.000)]  [G loss: -0.002] \n",
                        "9 [D loss: (-0.000)(R -0.000, F -0.000)]  [G loss: -0.003] \n",
                        "10 [D loss: (-0.000)(R -0.000, F -0.000)]  [G loss: -0.003] \n",
                        "11 [D loss: (-0.000)(R -0.000, F -0.000)]  [G loss: -0.003] \n",
                        "12 [D loss: (-0.000)(R -0.000, F -0.000)]  [G loss: -0.003] \n",
                        "13 [D loss: (-0.000)(R -0.000, F -0.000)]  [G loss: -0.004] \n",
                        "14 [D loss: (-0.000)(R -0.000, F -0.000)]  [G loss: -0.004] \n",
                        "15 [D loss: (-0.000)(R -0.000, F -0.000)]  [G loss: -0.004] \n",
                        "16 [D loss: (-0.000)(R -0.000, F -0.000)]  [G loss: -0.005] \n",
                        "17 [D loss: (-0.000)(R -0.000, F -0.000)]  [G loss: -0.005] \n",
                        "18 [D loss: (-0.001)(R -0.001, F -0.001)]  [G loss: -0.005] \n",
                        "19 [D loss: (-0.001)(R -0.001, F -0.001)]  [G loss: -0.006] \n",
                        "20 [D loss: (-0.001)(R -0.001, F -0.001)]  [G loss: -0.006] \n",
                        "21 [D loss: (-0.001)(R -0.001, F -0.001)]  [G loss: -0.007] \n",
                        "22 [D loss: (-0.001)(R -0.001, F -0.001)]  [G loss: -0.008] \n",
                        "23 [D loss: (-0.001)(R -0.001, F -0.001)]  [G loss: -0.009] \n",
                        "24 [D loss: (-0.001)(R -0.001, F -0.001)]  [G loss: -0.010] \n",
                        "25 [D loss: (-0.002)(R -0.002, F -0.002)]  [G loss: -0.012] \n",
                        "26 [D loss: (-0.003)(R -0.003, F -0.003)]  [G loss: -0.014] \n",
                        "27 [D loss: (-0.004)(R -0.004, F -0.004)]  [G loss: -0.019] \n",
                        "28 [D loss: (-0.005)(R -0.005, F -0.004)]  [G loss: -0.024] \n",
                        "29 [D loss: (-0.002)(R -0.002, F -0.001)]  [G loss: -0.030] \n",
                        "30 [D loss: (0.002)(R 0.002, F 0.003)]  [G loss: -0.036] \n",
                        "31 [D loss: (0.005)(R 0.005, F 0.006)]  [G loss: -0.041] \n",
                        "32 [D loss: (0.007)(R 0.006, F 0.007)]  [G loss: -0.046] \n",
                        "33 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.052] \n",
                        "34 [D loss: (0.008)(R 0.008, F 0.009)]  [G loss: -0.058] \n",
                        "35 [D loss: (0.011)(R 0.010, F 0.011)]  [G loss: -0.065] \n",
                        "36 [D loss: (0.013)(R 0.012, F 0.013)]  [G loss: -0.070] \n",
                        "37 [D loss: (0.014)(R 0.014, F 0.015)]  [G loss: -0.075] \n",
                        "38 [D loss: (0.015)(R 0.015, F 0.016)]  [G loss: -0.079] \n",
                        "39 [D loss: (0.016)(R 0.016, F 0.016)]  [G loss: -0.082] \n",
                        "40 [D loss: (0.016)(R 0.016, F 0.017)]  [G loss: -0.085] \n",
                        "41 [D loss: (0.017)(R 0.016, F 0.017)]  [G loss: -0.087] \n",
                        "42 [D loss: (0.017)(R 0.017, F 0.017)]  [G loss: -0.089] \n",
                        "43 [D loss: (0.017)(R 0.017, F 0.017)]  [G loss: -0.091] \n",
                        "44 [D loss: (0.017)(R 0.017, F 0.017)]  [G loss: -0.092] \n",
                        "45 [D loss: (0.018)(R 0.017, F 0.018)]  [G loss: -0.093] \n",
                        "46 [D loss: (0.018)(R 0.018, F 0.018)]  [G loss: -0.095] \n",
                        "47 [D loss: (0.018)(R 0.018, F 0.018)]  [G loss: -0.096] \n",
                        "48 [D loss: (0.018)(R 0.018, F 0.018)]  [G loss: -0.097] \n",
                        "49 [D loss: (0.018)(R 0.018, F 0.018)]  [G loss: -0.098] \n",
                        "50 [D loss: (0.018)(R 0.018, F 0.018)]  [G loss: -0.098] \n",
                        "51 [D loss: (0.018)(R 0.018, F 0.018)]  [G loss: -0.098] \n",
                        "52 [D loss: (0.018)(R 0.018, F 0.018)]  [G loss: -0.098] \n",
                        "53 [D loss: (0.017)(R 0.017, F 0.017)]  [G loss: -0.097] \n",
                        "54 [D loss: (0.017)(R 0.017, F 0.017)]  [G loss: -0.096] \n",
                        "55 [D loss: (0.016)(R 0.016, F 0.016)]  [G loss: -0.096] \n",
                        "56 [D loss: (0.016)(R 0.016, F 0.016)]  [G loss: -0.095] \n",
                        "57 [D loss: (0.015)(R 0.015, F 0.015)]  [G loss: -0.095] \n",
                        "58 [D loss: (0.015)(R 0.015, F 0.015)]  [G loss: -0.095] \n",
                        "59 [D loss: (0.015)(R 0.015, F 0.015)]  [G loss: -0.095] \n",
                        "60 [D loss: (0.015)(R 0.015, F 0.015)]  [G loss: -0.095] \n",
                        "61 [D loss: (0.014)(R 0.014, F 0.014)]  [G loss: -0.094] \n",
                        "62 [D loss: (0.014)(R 0.014, F 0.014)]  [G loss: -0.094] \n",
                        "63 [D loss: (0.014)(R 0.014, F 0.014)]  [G loss: -0.092] \n",
                        "64 [D loss: (0.014)(R 0.014, F 0.014)]  [G loss: -0.090] \n",
                        "65 [D loss: (0.014)(R 0.014, F 0.014)]  [G loss: -0.088] \n",
                        "66 [D loss: (0.014)(R 0.014, F 0.014)]  [G loss: -0.086] \n",
                        "67 [D loss: (0.013)(R 0.014, F 0.013)]  [G loss: -0.084] \n",
                        "68 [D loss: (0.013)(R 0.013, F 0.013)]  [G loss: -0.081] \n",
                        "69 [D loss: (0.013)(R 0.013, F 0.013)]  [G loss: -0.079] \n",
                        "70 [D loss: (0.012)(R 0.013, F 0.012)]  [G loss: -0.076] \n",
                        "71 [D loss: (0.012)(R 0.012, F 0.012)]  [G loss: -0.074] \n",
                        "72 [D loss: (0.012)(R 0.012, F 0.012)]  [G loss: -0.072] \n",
                        "73 [D loss: (0.012)(R 0.012, F 0.012)]  [G loss: -0.069] \n",
                        "74 [D loss: (0.011)(R 0.011, F 0.011)]  [G loss: -0.067] \n",
                        "75 [D loss: (0.011)(R 0.011, F 0.011)]  [G loss: -0.065] \n",
                        "76 [D loss: (0.011)(R 0.011, F 0.011)]  [G loss: -0.063] \n",
                        "77 [D loss: (0.010)(R 0.010, F 0.010)]  [G loss: -0.060] \n",
                        "78 [D loss: (0.010)(R 0.010, F 0.010)]  [G loss: -0.058] \n",
                        "79 [D loss: (0.010)(R 0.010, F 0.009)]  [G loss: -0.056] \n",
                        "80 [D loss: (0.009)(R 0.009, F 0.009)]  [G loss: -0.054] \n",
                        "81 [D loss: (0.009)(R 0.009, F 0.009)]  [G loss: -0.053] \n",
                        "82 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.051] \n",
                        "83 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.050] \n",
                        "84 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.050] \n",
                        "85 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.049] \n",
                        "86 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.050] \n",
                        "87 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.050] \n",
                        "88 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.052] \n",
                        "89 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.053] \n",
                        "90 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.055] \n",
                        "91 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.058] \n",
                        "92 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.060] \n",
                        "93 [D loss: (0.007)(R 0.007, F 0.008)]  [G loss: -0.063] \n",
                        "94 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.066] \n",
                        "95 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.068] \n",
                        "96 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.071] \n",
                        "97 [D loss: (0.009)(R 0.008, F 0.009)]  [G loss: -0.075] \n",
                        "98 [D loss: (0.009)(R 0.009, F 0.009)]  [G loss: -0.078] \n",
                        "99 [D loss: (0.009)(R 0.009, F 0.009)]  [G loss: -0.081] \n",
                        "100 [D loss: (0.010)(R 0.010, F 0.010)]  [G loss: -0.084] \n",
                        "101 [D loss: (0.010)(R 0.010, F 0.010)]  [G loss: -0.087] \n",
                        "102 [D loss: (0.010)(R 0.010, F 0.011)]  [G loss: -0.090] \n",
                        "103 [D loss: (0.011)(R 0.011, F 0.011)]  [G loss: -0.092] \n",
                        "104 [D loss: (0.011)(R 0.011, F 0.011)]  [G loss: -0.094] \n",
                        "105 [D loss: (0.011)(R 0.011, F 0.012)]  [G loss: -0.096] \n",
                        "106 [D loss: (0.012)(R 0.012, F 0.012)]  [G loss: -0.098] \n",
                        "107 [D loss: (0.012)(R 0.012, F 0.012)]  [G loss: -0.100] \n",
                        "108 [D loss: (0.012)(R 0.012, F 0.012)]  [G loss: -0.101] \n",
                        "109 [D loss: (0.013)(R 0.012, F 0.013)]  [G loss: -0.102] \n",
                        "110 [D loss: (0.013)(R 0.013, F 0.013)]  [G loss: -0.103] \n",
                        "111 [D loss: (0.013)(R 0.013, F 0.013)]  [G loss: -0.104] \n",
                        "112 [D loss: (0.013)(R 0.013, F 0.013)]  [G loss: -0.105] \n",
                        "113 [D loss: (0.013)(R 0.013, F 0.013)]  [G loss: -0.105] \n",
                        "114 [D loss: (0.013)(R 0.013, F 0.013)]  [G loss: -0.105] \n",
                        "115 [D loss: (0.013)(R 0.013, F 0.013)]  [G loss: -0.105] \n",
                        "116 [D loss: (0.013)(R 0.013, F 0.013)]  [G loss: -0.105] \n",
                        "117 [D loss: (0.012)(R 0.012, F 0.012)]  [G loss: -0.105] \n",
                        "118 [D loss: (0.012)(R 0.012, F 0.012)]  [G loss: -0.105] \n",
                        "119 [D loss: (0.012)(R 0.012, F 0.012)]  [G loss: -0.105] \n",
                        "120 [D loss: (0.012)(R 0.012, F 0.012)]  [G loss: -0.104] \n",
                        "121 [D loss: (0.012)(R 0.012, F 0.012)]  [G loss: -0.103] \n",
                        "122 [D loss: (0.012)(R 0.012, F 0.012)]  [G loss: -0.102] \n",
                        "123 [D loss: (0.012)(R 0.012, F 0.012)]  [G loss: -0.101] \n",
                        "124 [D loss: (0.012)(R 0.012, F 0.012)]  [G loss: -0.100] \n",
                        "125 [D loss: (0.012)(R 0.012, F 0.012)]  [G loss: -0.099] \n",
                        "126 [D loss: (0.011)(R 0.012, F 0.011)]  [G loss: -0.097] \n",
                        "127 [D loss: (0.011)(R 0.011, F 0.011)]  [G loss: -0.096] \n",
                        "128 [D loss: (0.011)(R 0.011, F 0.011)]  [G loss: -0.094] \n",
                        "129 [D loss: (0.011)(R 0.011, F 0.011)]  [G loss: -0.093] \n",
                        "130 [D loss: (0.011)(R 0.011, F 0.011)]  [G loss: -0.092] \n",
                        "131 [D loss: (0.010)(R 0.010, F 0.010)]  [G loss: -0.091] \n",
                        "132 [D loss: (0.010)(R 0.010, F 0.010)]  [G loss: -0.090] \n",
                        "133 [D loss: (0.010)(R 0.010, F 0.010)]  [G loss: -0.089] \n",
                        "134 [D loss: (0.010)(R 0.010, F 0.010)]  [G loss: -0.088] \n",
                        "135 [D loss: (0.010)(R 0.010, F 0.010)]  [G loss: -0.087] \n",
                        "136 [D loss: (0.010)(R 0.010, F 0.010)]  [G loss: -0.086] \n",
                        "137 [D loss: (0.010)(R 0.010, F 0.010)]  [G loss: -0.086] \n",
                        "138 [D loss: (0.010)(R 0.010, F 0.010)]  [G loss: -0.085] \n",
                        "139 [D loss: (0.010)(R 0.010, F 0.010)]  [G loss: -0.085] \n",
                        "140 [D loss: (0.010)(R 0.010, F 0.010)]  [G loss: -0.084] \n",
                        "141 [D loss: (0.010)(R 0.010, F 0.010)]  [G loss: -0.084] \n",
                        "142 [D loss: (0.010)(R 0.010, F 0.010)]  [G loss: -0.083] \n",
                        "143 [D loss: (0.010)(R 0.010, F 0.010)]  [G loss: -0.083] \n",
                        "144 [D loss: (0.010)(R 0.009, F 0.010)]  [G loss: -0.082] \n",
                        "145 [D loss: (0.009)(R 0.009, F 0.009)]  [G loss: -0.082] \n",
                        "146 [D loss: (0.009)(R 0.009, F 0.009)]  [G loss: -0.081] \n",
                        "147 [D loss: (0.009)(R 0.009, F 0.009)]  [G loss: -0.081] \n",
                        "148 [D loss: (0.009)(R 0.009, F 0.009)]  [G loss: -0.080] \n",
                        "149 [D loss: (0.009)(R 0.009, F 0.009)]  [G loss: -0.080] \n",
                        "150 [D loss: (0.009)(R 0.009, F 0.009)]  [G loss: -0.079] \n",
                        "151 [D loss: (0.009)(R 0.009, F 0.009)]  [G loss: -0.079] \n",
                        "152 [D loss: (0.009)(R 0.009, F 0.009)]  [G loss: -0.078] \n",
                        "153 [D loss: (0.009)(R 0.009, F 0.009)]  [G loss: -0.078] \n",
                        "154 [D loss: (0.009)(R 0.009, F 0.009)]  [G loss: -0.077] \n",
                        "155 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.077] \n",
                        "156 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.076] \n",
                        "157 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.076] \n",
                        "158 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.075] \n",
                        "159 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.075] \n",
                        "160 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.074] \n",
                        "161 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.074] \n",
                        "162 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.074] \n",
                        "163 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.074] \n",
                        "164 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.074] \n",
                        "165 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.074] \n",
                        "166 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.074] \n",
                        "167 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.074] \n",
                        "168 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.074] \n",
                        "169 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.075] \n",
                        "170 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.075] \n",
                        "171 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.075] \n",
                        "172 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.075] \n",
                        "173 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.075] \n",
                        "174 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.076] \n",
                        "175 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.076] \n",
                        "176 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.076] \n",
                        "177 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.076] \n",
                        "178 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.076] \n",
                        "179 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.076] \n",
                        "180 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.076] \n",
                        "181 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.076] \n",
                        "182 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.076] \n",
                        "183 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.077] \n",
                        "184 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.077] \n",
                        "185 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.077] \n",
                        "186 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.077] \n",
                        "187 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.077] \n",
                        "188 [D loss: (0.008)(R 0.007, F 0.008)]  [G loss: -0.077] \n",
                        "189 [D loss: (0.007)(R 0.007, F 0.008)]  [G loss: -0.077] \n",
                        "190 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.077] \n",
                        "191 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.077] \n",
                        "192 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.077] \n",
                        "193 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.077] \n",
                        "194 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.077] \n",
                        "195 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.077] \n",
                        "196 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.077] \n",
                        "197 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.077] \n",
                        "198 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.077] \n",
                        "199 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.077] \n",
                        "200 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.078] \n",
                        "201 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.078] \n",
                        "202 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.078] \n",
                        "203 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.078] \n",
                        "204 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.078] \n",
                        "205 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.079] \n",
                        "206 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.079] \n",
                        "207 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.079] \n",
                        "208 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.080] \n",
                        "209 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.080] \n",
                        "210 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.080] \n",
                        "211 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.081] \n",
                        "212 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.081] \n",
                        "213 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.081] \n",
                        "214 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.082] \n",
                        "215 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.082] \n",
                        "216 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.083] \n",
                        "217 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.083] \n",
                        "218 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.084] \n",
                        "219 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.084] \n",
                        "220 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.085] \n",
                        "221 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.085] \n",
                        "222 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.085] \n",
                        "223 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.086] \n",
                        "224 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.086] \n",
                        "225 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.087] \n",
                        "226 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.087] \n",
                        "227 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.088] \n",
                        "228 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.088] \n",
                        "229 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.088] \n",
                        "230 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.089] \n",
                        "231 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.089] \n",
                        "232 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.090] \n",
                        "233 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.090] \n",
                        "234 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.091] \n",
                        "235 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.091] \n",
                        "236 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.091] \n",
                        "237 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.092] \n",
                        "238 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.092] \n",
                        "239 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.092] \n",
                        "240 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.093] \n",
                        "241 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.093] \n",
                        "242 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.094] \n",
                        "243 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.094] \n",
                        "244 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.094] \n",
                        "245 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.095] \n",
                        "246 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.095] \n",
                        "247 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.096] \n",
                        "248 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.096] \n",
                        "249 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.096] \n",
                        "250 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.097] \n",
                        "251 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.097] \n",
                        "252 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.098] \n",
                        "253 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.098] \n",
                        "254 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.099] \n",
                        "255 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.099] \n",
                        "256 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.100] \n",
                        "257 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.101] \n",
                        "258 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.101] \n",
                        "259 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.102] \n",
                        "260 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.102] \n",
                        "261 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.103] \n",
                        "262 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.104] \n",
                        "263 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.105] \n",
                        "264 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.105] \n",
                        "265 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.106] \n",
                        "266 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.107] \n",
                        "267 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.108] \n",
                        "268 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.109] \n",
                        "269 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.109] \n",
                        "270 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.110] \n",
                        "271 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.111] \n",
                        "272 [D loss: (0.007)(R 0.007, F 0.007)]  [G loss: -0.112] \n",
                        "273 [D loss: (0.007)(R 0.007, F 0.008)]  [G loss: -0.113] \n",
                        "274 [D loss: (0.007)(R 0.007, F 0.008)]  [G loss: -0.114] \n",
                        "275 [D loss: (0.007)(R 0.007, F 0.008)]  [G loss: -0.115] \n",
                        "276 [D loss: (0.008)(R 0.007, F 0.008)]  [G loss: -0.116] \n",
                        "277 [D loss: (0.008)(R 0.007, F 0.008)]  [G loss: -0.117] \n",
                        "278 [D loss: (0.008)(R 0.007, F 0.008)]  [G loss: -0.118] \n",
                        "279 [D loss: (0.008)(R 0.007, F 0.008)]  [G loss: -0.119] \n",
                        "280 [D loss: (0.008)(R 0.007, F 0.008)]  [G loss: -0.120] \n",
                        "281 [D loss: (0.008)(R 0.007, F 0.008)]  [G loss: -0.121] \n",
                        "282 [D loss: (0.008)(R 0.007, F 0.008)]  [G loss: -0.122] \n",
                        "283 [D loss: (0.008)(R 0.007, F 0.008)]  [G loss: -0.123] \n",
                        "284 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.124] \n",
                        "285 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.125] \n",
                        "286 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.126] \n",
                        "287 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.128] \n",
                        "288 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.129] \n",
                        "289 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.130] \n",
                        "290 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.131] \n",
                        "291 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.133] \n",
                        "292 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.134] \n",
                        "293 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.135] \n",
                        "294 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.136] \n",
                        "295 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.138] \n",
                        "296 [D loss: (0.008)(R 0.008, F 0.009)]  [G loss: -0.139] \n",
                        "297 [D loss: (0.008)(R 0.008, F 0.009)]  [G loss: -0.140] \n",
                        "298 [D loss: (0.008)(R 0.008, F 0.009)]  [G loss: -0.141] \n",
                        "299 [D loss: (0.008)(R 0.008, F 0.009)]  [G loss: -0.143] \n",
                        "300 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.144] \n",
                        "301 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.145] \n",
                        "302 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.146] \n",
                        "303 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.148] \n",
                        "304 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.149] \n",
                        "305 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.151] \n",
                        "306 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.152] \n",
                        "307 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.154] \n",
                        "308 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.156] \n",
                        "309 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.158] \n",
                        "310 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.160] \n",
                        "311 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.162] \n",
                        "312 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.164] \n",
                        "313 [D loss: (0.008)(R 0.008, F 0.008)]  [G loss: -0.167] \n",
                        "314 [D loss: (0.008)(R 0.008, F 0.009)]  [G loss: -0.170] \n",
                        "315 [D loss: (0.009)(R 0.008, F 0.009)]  [G loss: -0.173] \n",
                        "316 [D loss: (0.009)(R 0.009, F 0.009)]  [G loss: -0.176] \n",
                        "317 [D loss: (0.009)(R 0.009, F 0.009)]  [G loss: -0.179] \n",
                        "318 [D loss: (0.009)(R 0.009, F 0.010)]  [G loss: -0.182] \n",
                        "319 [D loss: (0.010)(R 0.009, F 0.010)]  [G loss: -0.185] \n",
                        "320 [D loss: (0.010)(R 0.010, F 0.010)]  [G loss: -0.189] \n",
                        "321 [D loss: (0.010)(R 0.010, F 0.010)]  [G loss: -0.192] \n",
                        "322 [D loss: (0.010)(R 0.010, F 0.011)]  [G loss: -0.195] \n",
                        "323 [D loss: (0.011)(R 0.010, F 0.011)]  [G loss: -0.199] \n",
                        "324 [D loss: (0.011)(R 0.011, F 0.011)]  [G loss: -0.202] \n",
                        "325 [D loss: (0.011)(R 0.011, F 0.011)]  [G loss: -0.205] \n",
                        "326 [D loss: (0.011)(R 0.011, F 0.011)]  [G loss: -0.208] \n",
                        "327 [D loss: (0.011)(R 0.011, F 0.011)]  [G loss: -0.211] \n",
                        "328 [D loss: (0.011)(R 0.011, F 0.012)]  [G loss: -0.214] \n",
                        "329 [D loss: (0.011)(R 0.011, F 0.012)]  [G loss: -0.217] \n",
                        "330 [D loss: (0.011)(R 0.011, F 0.012)]  [G loss: -0.220] \n",
                        "331 [D loss: (0.011)(R 0.011, F 0.012)]  [G loss: -0.223] \n",
                        "332 [D loss: (0.011)(R 0.011, F 0.012)]  [G loss: -0.226] \n",
                        "333 [D loss: (0.012)(R 0.011, F 0.012)]  [G loss: -0.229] \n",
                        "334 [D loss: (0.012)(R 0.011, F 0.012)]  [G loss: -0.232] \n",
                        "335 [D loss: (0.012)(R 0.011, F 0.012)]  [G loss: -0.236] \n",
                        "336 [D loss: (0.012)(R 0.012, F 0.012)]  [G loss: -0.239] \n",
                        "337 [D loss: (0.012)(R 0.012, F 0.012)]  [G loss: -0.242] \n",
                        "338 [D loss: (0.012)(R 0.012, F 0.012)]  [G loss: -0.246] \n",
                        "339 [D loss: (0.012)(R 0.012, F 0.013)]  [G loss: -0.249] \n",
                        "340 [D loss: (0.013)(R 0.012, F 0.013)]  [G loss: -0.253] \n",
                        "341 [D loss: (0.013)(R 0.013, F 0.013)]  [G loss: -0.257] \n",
                        "342 [D loss: (0.013)(R 0.013, F 0.013)]  [G loss: -0.261] \n",
                        "343 [D loss: (0.014)(R 0.013, F 0.014)]  [G loss: -0.265] \n",
                        "344 [D loss: (0.014)(R 0.014, F 0.014)]  [G loss: -0.269] \n",
                        "345 [D loss: (0.014)(R 0.014, F 0.015)]  [G loss: -0.273] \n",
                        "346 [D loss: (0.015)(R 0.014, F 0.015)]  [G loss: -0.277] \n",
                        "347 [D loss: (0.015)(R 0.015, F 0.015)]  [G loss: -0.281] \n",
                        "348 [D loss: (0.015)(R 0.015, F 0.016)]  [G loss: -0.285] \n",
                        "349 [D loss: (0.016)(R 0.015, F 0.016)]  [G loss: -0.288] \n",
                        "350 [D loss: (0.016)(R 0.016, F 0.016)]  [G loss: -0.292] \n",
                        "351 [D loss: (0.016)(R 0.016, F 0.016)]  [G loss: -0.296] \n",
                        "352 [D loss: (0.016)(R 0.016, F 0.017)]  [G loss: -0.299] \n",
                        "353 [D loss: (0.017)(R 0.016, F 0.017)]  [G loss: -0.303] \n",
                        "354 [D loss: (0.017)(R 0.016, F 0.017)]  [G loss: -0.306] \n",
                        "355 [D loss: (0.017)(R 0.017, F 0.017)]  [G loss: -0.309] \n",
                        "356 [D loss: (0.017)(R 0.017, F 0.017)]  [G loss: -0.313] \n",
                        "357 [D loss: (0.017)(R 0.017, F 0.017)]  [G loss: -0.316] \n",
                        "358 [D loss: (0.017)(R 0.017, F 0.017)]  [G loss: -0.320] \n",
                        "359 [D loss: (0.017)(R 0.017, F 0.017)]  [G loss: -0.323] \n",
                        "360 [D loss: (0.017)(R 0.017, F 0.017)]  [G loss: -0.327] \n",
                        "361 [D loss: (0.017)(R 0.017, F 0.017)]  [G loss: -0.330] \n",
                        "362 [D loss: (0.017)(R 0.017, F 0.018)]  [G loss: -0.334] \n",
                        "363 [D loss: (0.017)(R 0.017, F 0.018)]  [G loss: -0.338] \n",
                        "364 [D loss: (0.018)(R 0.017, F 0.018)]  [G loss: -0.342] \n",
                        "365 [D loss: (0.018)(R 0.017, F 0.018)]  [G loss: -0.346] \n",
                        "366 [D loss: (0.018)(R 0.018, F 0.018)]  [G loss: -0.350] \n",
                        "367 [D loss: (0.018)(R 0.018, F 0.018)]  [G loss: -0.355] \n",
                        "368 [D loss: (0.018)(R 0.018, F 0.019)]  [G loss: -0.359] \n",
                        "369 [D loss: (0.019)(R 0.018, F 0.019)]  [G loss: -0.363] \n",
                        "370 [D loss: (0.019)(R 0.018, F 0.019)]  [G loss: -0.367] \n",
                        "371 [D loss: (0.019)(R 0.019, F 0.019)]  [G loss: -0.372] \n",
                        "372 [D loss: (0.019)(R 0.019, F 0.019)]  [G loss: -0.376] \n",
                        "373 [D loss: (0.019)(R 0.019, F 0.019)]  [G loss: -0.380] \n",
                        "374 [D loss: (0.019)(R 0.019, F 0.020)]  [G loss: -0.384] \n",
                        "375 [D loss: (0.019)(R 0.019, F 0.020)]  [G loss: -0.388] \n",
                        "376 [D loss: (0.020)(R 0.019, F 0.020)]  [G loss: -0.393] \n",
                        "377 [D loss: (0.020)(R 0.019, F 0.020)]  [G loss: -0.397] \n",
                        "378 [D loss: (0.020)(R 0.020, F 0.020)]  [G loss: -0.402] \n",
                        "379 [D loss: (0.020)(R 0.020, F 0.020)]  [G loss: -0.407] \n",
                        "380 [D loss: (0.020)(R 0.020, F 0.021)]  [G loss: -0.412] \n",
                        "381 [D loss: (0.021)(R 0.020, F 0.021)]  [G loss: -0.417] \n",
                        "382 [D loss: (0.021)(R 0.021, F 0.021)]  [G loss: -0.423] \n",
                        "383 [D loss: (0.021)(R 0.021, F 0.022)]  [G loss: -0.429] \n",
                        "384 [D loss: (0.022)(R 0.021, F 0.022)]  [G loss: -0.435] \n",
                        "385 [D loss: (0.022)(R 0.022, F 0.023)]  [G loss: -0.441] \n",
                        "386 [D loss: (0.023)(R 0.022, F 0.023)]  [G loss: -0.447] \n",
                        "387 [D loss: (0.023)(R 0.023, F 0.024)]  [G loss: -0.453] \n",
                        "388 [D loss: (0.024)(R 0.024, F 0.024)]  [G loss: -0.460] \n",
                        "389 [D loss: (0.025)(R 0.024, F 0.025)]  [G loss: -0.467] \n",
                        "390 [D loss: (0.025)(R 0.025, F 0.026)]  [G loss: -0.473] \n",
                        "391 [D loss: (0.026)(R 0.026, F 0.027)]  [G loss: -0.480] \n",
                        "392 [D loss: (0.027)(R 0.027, F 0.027)]  [G loss: -0.487] \n",
                        "393 [D loss: (0.028)(R 0.027, F 0.028)]  [G loss: -0.493] \n",
                        "394 [D loss: (0.028)(R 0.028, F 0.029)]  [G loss: -0.500] \n",
                        "395 [D loss: (0.029)(R 0.029, F 0.029)]  [G loss: -0.506] \n",
                        "396 [D loss: (0.030)(R 0.029, F 0.030)]  [G loss: -0.512] \n",
                        "397 [D loss: (0.030)(R 0.030, F 0.031)]  [G loss: -0.517] \n",
                        "398 [D loss: (0.031)(R 0.030, F 0.031)]  [G loss: -0.523] \n",
                        "399 [D loss: (0.031)(R 0.031, F 0.031)]  [G loss: -0.528] \n",
                        "400 [D loss: (0.031)(R 0.031, F 0.032)]  [G loss: -0.533] \n",
                        "401 [D loss: (0.032)(R 0.031, F 0.032)]  [G loss: -0.538] \n",
                        "402 [D loss: (0.032)(R 0.032, F 0.032)]  [G loss: -0.543] \n",
                        "403 [D loss: (0.032)(R 0.032, F 0.033)]  [G loss: -0.548] \n",
                        "404 [D loss: (0.033)(R 0.032, F 0.033)]  [G loss: -0.552] \n",
                        "405 [D loss: (0.033)(R 0.032, F 0.033)]  [G loss: -0.557] \n",
                        "406 [D loss: (0.033)(R 0.033, F 0.033)]  [G loss: -0.561] \n",
                        "407 [D loss: (0.033)(R 0.033, F 0.034)]  [G loss: -0.564] \n",
                        "408 [D loss: (0.033)(R 0.033, F 0.034)]  [G loss: -0.568] \n",
                        "409 [D loss: (0.034)(R 0.033, F 0.034)]  [G loss: -0.572] \n",
                        "410 [D loss: (0.034)(R 0.034, F 0.034)]  [G loss: -0.575] \n",
                        "411 [D loss: (0.034)(R 0.034, F 0.034)]  [G loss: -0.579] \n",
                        "412 [D loss: (0.034)(R 0.034, F 0.034)]  [G loss: -0.582] \n",
                        "413 [D loss: (0.034)(R 0.034, F 0.035)]  [G loss: -0.585] \n",
                        "414 [D loss: (0.034)(R 0.034, F 0.035)]  [G loss: -0.588] \n",
                        "415 [D loss: (0.034)(R 0.034, F 0.035)]  [G loss: -0.590] \n",
                        "416 [D loss: (0.034)(R 0.034, F 0.035)]  [G loss: -0.593] \n",
                        "417 [D loss: (0.034)(R 0.034, F 0.035)]  [G loss: -0.596] \n",
                        "418 [D loss: (0.034)(R 0.034, F 0.035)]  [G loss: -0.599] \n",
                        "419 [D loss: (0.034)(R 0.034, F 0.035)]  [G loss: -0.602] \n",
                        "420 [D loss: (0.034)(R 0.034, F 0.035)]  [G loss: -0.604] \n",
                        "421 [D loss: (0.034)(R 0.034, F 0.035)]  [G loss: -0.607] \n",
                        "422 [D loss: (0.034)(R 0.034, F 0.035)]  [G loss: -0.611] \n",
                        "423 [D loss: (0.034)(R 0.034, F 0.035)]  [G loss: -0.614] \n",
                        "424 [D loss: (0.034)(R 0.034, F 0.035)]  [G loss: -0.617] \n",
                        "425 [D loss: (0.034)(R 0.034, F 0.035)]  [G loss: -0.621] \n",
                        "426 [D loss: (0.035)(R 0.034, F 0.035)]  [G loss: -0.624] \n",
                        "427 [D loss: (0.035)(R 0.034, F 0.035)]  [G loss: -0.628] \n",
                        "428 [D loss: (0.035)(R 0.034, F 0.035)]  [G loss: -0.632] \n",
                        "429 [D loss: (0.035)(R 0.034, F 0.035)]  [G loss: -0.636] \n",
                        "430 [D loss: (0.035)(R 0.035, F 0.035)]  [G loss: -0.640] \n",
                        "431 [D loss: (0.035)(R 0.035, F 0.035)]  [G loss: -0.645] \n",
                        "432 [D loss: (0.035)(R 0.035, F 0.036)]  [G loss: -0.649] \n",
                        "433 [D loss: (0.035)(R 0.035, F 0.036)]  [G loss: -0.654] \n",
                        "434 [D loss: (0.036)(R 0.035, F 0.036)]  [G loss: -0.660] \n",
                        "435 [D loss: (0.036)(R 0.035, F 0.036)]  [G loss: -0.665] \n",
                        "436 [D loss: (0.036)(R 0.035, F 0.036)]  [G loss: -0.671] \n",
                        "437 [D loss: (0.036)(R 0.036, F 0.036)]  [G loss: -0.677] \n",
                        "438 [D loss: (0.036)(R 0.036, F 0.037)]  [G loss: -0.683] \n",
                        "439 [D loss: (0.037)(R 0.036, F 0.037)]  [G loss: -0.690] \n",
                        "440 [D loss: (0.037)(R 0.036, F 0.037)]  [G loss: -0.696] \n",
                        "441 [D loss: (0.037)(R 0.037, F 0.037)]  [G loss: -0.703] \n",
                        "442 [D loss: (0.037)(R 0.037, F 0.038)]  [G loss: -0.710] \n",
                        "443 [D loss: (0.037)(R 0.037, F 0.038)]  [G loss: -0.718] \n",
                        "444 [D loss: (0.038)(R 0.037, F 0.038)]  [G loss: -0.725] \n",
                        "445 [D loss: (0.038)(R 0.037, F 0.038)]  [G loss: -0.733] \n",
                        "446 [D loss: (0.038)(R 0.038, F 0.039)]  [G loss: -0.741] \n",
                        "447 [D loss: (0.038)(R 0.038, F 0.039)]  [G loss: -0.750] \n",
                        "448 [D loss: (0.039)(R 0.038, F 0.039)]  [G loss: -0.758] \n",
                        "449 [D loss: (0.039)(R 0.038, F 0.040)]  [G loss: -0.767] \n",
                        "450 [D loss: (0.039)(R 0.039, F 0.040)]  [G loss: -0.777] \n",
                        "451 [D loss: (0.040)(R 0.039, F 0.040)]  [G loss: -0.786] \n",
                        "452 [D loss: (0.040)(R 0.040, F 0.041)]  [G loss: -0.796] \n",
                        "453 [D loss: (0.041)(R 0.040, F 0.041)]  [G loss: -0.806] \n",
                        "454 [D loss: (0.041)(R 0.041, F 0.042)]  [G loss: -0.816] \n",
                        "455 [D loss: (0.042)(R 0.041, F 0.043)]  [G loss: -0.826] \n",
                        "456 [D loss: (0.043)(R 0.042, F 0.043)]  [G loss: -0.837] \n",
                        "457 [D loss: (0.043)(R 0.043, F 0.044)]  [G loss: -0.848] \n",
                        "458 [D loss: (0.044)(R 0.044, F 0.045)]  [G loss: -0.858] \n",
                        "459 [D loss: (0.045)(R 0.044, F 0.046)]  [G loss: -0.869] \n",
                        "460 [D loss: (0.046)(R 0.045, F 0.047)]  [G loss: -0.880] \n",
                        "461 [D loss: (0.047)(R 0.046, F 0.047)]  [G loss: -0.891] \n",
                        "462 [D loss: (0.048)(R 0.047, F 0.048)]  [G loss: -0.901] \n",
                        "463 [D loss: (0.048)(R 0.048, F 0.049)]  [G loss: -0.912] \n",
                        "464 [D loss: (0.049)(R 0.048, F 0.050)]  [G loss: -0.922] \n",
                        "465 [D loss: (0.050)(R 0.049, F 0.050)]  [G loss: -0.933] \n",
                        "466 [D loss: (0.050)(R 0.050, F 0.051)]  [G loss: -0.942] \n",
                        "467 [D loss: (0.051)(R 0.050, F 0.052)]  [G loss: -0.952] \n",
                        "468 [D loss: (0.052)(R 0.051, F 0.052)]  [G loss: -0.962] \n",
                        "469 [D loss: (0.052)(R 0.052, F 0.053)]  [G loss: -0.971] \n",
                        "470 [D loss: (0.053)(R 0.052, F 0.053)]  [G loss: -0.981] \n",
                        "471 [D loss: (0.053)(R 0.053, F 0.054)]  [G loss: -0.989] \n",
                        "472 [D loss: (0.054)(R 0.053, F 0.054)]  [G loss: -0.998] \n",
                        "473 [D loss: (0.054)(R 0.053, F 0.055)]  [G loss: -1.007] \n",
                        "474 [D loss: (0.054)(R 0.054, F 0.055)]  [G loss: -1.015] \n",
                        "475 [D loss: (0.055)(R 0.054, F 0.055)]  [G loss: -1.024] \n",
                        "476 [D loss: (0.055)(R 0.055, F 0.056)]  [G loss: -1.031] \n",
                        "477 [D loss: (0.055)(R 0.055, F 0.056)]  [G loss: -1.039] \n",
                        "478 [D loss: (0.056)(R 0.055, F 0.056)]  [G loss: -1.048] \n",
                        "479 [D loss: (0.056)(R 0.056, F 0.057)]  [G loss: -1.055] \n",
                        "480 [D loss: (0.056)(R 0.056, F 0.057)]  [G loss: -1.063] \n",
                        "481 [D loss: (0.056)(R 0.056, F 0.057)]  [G loss: -1.071] \n",
                        "482 [D loss: (0.057)(R 0.056, F 0.058)]  [G loss: -1.078] \n",
                        "483 [D loss: (0.057)(R 0.057, F 0.058)]  [G loss: -1.086] \n",
                        "484 [D loss: (0.057)(R 0.057, F 0.058)]  [G loss: -1.094] \n",
                        "485 [D loss: (0.058)(R 0.057, F 0.058)]  [G loss: -1.101] \n",
                        "486 [D loss: (0.058)(R 0.057, F 0.059)]  [G loss: -1.109] \n",
                        "487 [D loss: (0.058)(R 0.058, F 0.059)]  [G loss: -1.117] \n",
                        "488 [D loss: (0.059)(R 0.058, F 0.059)]  [G loss: -1.125] \n",
                        "489 [D loss: (0.059)(R 0.058, F 0.059)]  [G loss: -1.133] \n",
                        "490 [D loss: (0.059)(R 0.058, F 0.060)]  [G loss: -1.142] \n",
                        "491 [D loss: (0.059)(R 0.059, F 0.060)]  [G loss: -1.150] \n",
                        "492 [D loss: (0.060)(R 0.059, F 0.060)]  [G loss: -1.159] \n",
                        "493 [D loss: (0.060)(R 0.059, F 0.061)]  [G loss: -1.169] \n",
                        "494 [D loss: (0.060)(R 0.060, F 0.061)]  [G loss: -1.178] \n",
                        "495 [D loss: (0.061)(R 0.060, F 0.061)]  [G loss: -1.187] \n",
                        "496 [D loss: (0.061)(R 0.060, F 0.062)]  [G loss: -1.197] \n",
                        "497 [D loss: (0.062)(R 0.061, F 0.062)]  [G loss: -1.207] \n",
                        "498 [D loss: (0.062)(R 0.061, F 0.063)]  [G loss: -1.218] \n",
                        "499 [D loss: (0.062)(R 0.062, F 0.063)]  [G loss: -1.228] \n",
                        "500 [D loss: (0.063)(R 0.062, F 0.063)]  [G loss: -1.239] \n",
                        "501 [D loss: (0.063)(R 0.063, F 0.064)]  [G loss: -1.250] \n",
                        "502 [D loss: (0.064)(R 0.063, F 0.064)]  [G loss: -1.262] \n",
                        "503 [D loss: (0.064)(R 0.064, F 0.065)]  [G loss: -1.273] \n",
                        "504 [D loss: (0.065)(R 0.064, F 0.066)]  [G loss: -1.285] \n",
                        "505 [D loss: (0.065)(R 0.065, F 0.066)]  [G loss: -1.297] \n",
                        "506 [D loss: (0.066)(R 0.065, F 0.067)]  [G loss: -1.309] \n",
                        "507 [D loss: (0.066)(R 0.066, F 0.067)]  [G loss: -1.322] \n",
                        "508 [D loss: (0.067)(R 0.066, F 0.068)]  [G loss: -1.336] \n",
                        "509 [D loss: (0.068)(R 0.067, F 0.069)]  [G loss: -1.349] \n",
                        "510 [D loss: (0.068)(R 0.067, F 0.069)]  [G loss: -1.363] \n",
                        "511 [D loss: (0.069)(R 0.068, F 0.070)]  [G loss: -1.376] \n",
                        "512 [D loss: (0.069)(R 0.068, F 0.070)]  [G loss: -1.391] \n",
                        "513 [D loss: (0.071)(R 0.070, F 0.072)]  [G loss: -1.405] \n",
                        "514 [D loss: (0.071)(R 0.070, F 0.072)]  [G loss: -1.419] \n",
                        "515 [D loss: (0.072)(R 0.071, F 0.073)]  [G loss: -1.435] \n",
                        "516 [D loss: (0.073)(R 0.072, F 0.074)]  [G loss: -1.450] \n",
                        "517 [D loss: (0.074)(R 0.073, F 0.075)]  [G loss: -1.464] \n",
                        "518 [D loss: (0.075)(R 0.074, F 0.076)]  [G loss: -1.480] \n",
                        "519 [D loss: (0.076)(R 0.075, F 0.077)]  [G loss: -1.495] \n",
                        "520 [D loss: (0.076)(R 0.075, F 0.077)]  [G loss: -1.510] \n",
                        "521 [D loss: (0.077)(R 0.076, F 0.078)]  [G loss: -1.526] \n",
                        "522 [D loss: (0.078)(R 0.077, F 0.079)]  [G loss: -1.541] \n",
                        "523 [D loss: (0.079)(R 0.078, F 0.080)]  [G loss: -1.558] \n",
                        "524 [D loss: (0.080)(R 0.079, F 0.081)]  [G loss: -1.573] \n",
                        "525 [D loss: (0.081)(R 0.080, F 0.082)]  [G loss: -1.589] \n",
                        "526 [D loss: (0.082)(R 0.081, F 0.083)]  [G loss: -1.605] \n",
                        "527 [D loss: (0.083)(R 0.082, F 0.084)]  [G loss: -1.621] \n",
                        "528 [D loss: (0.084)(R 0.083, F 0.085)]  [G loss: -1.637] \n",
                        "529 [D loss: (0.085)(R 0.084, F 0.086)]  [G loss: -1.654] \n",
                        "530 [D loss: (0.086)(R 0.085, F 0.087)]  [G loss: -1.670] \n",
                        "531 [D loss: (0.086)(R 0.085, F 0.088)]  [G loss: -1.687] \n",
                        "532 [D loss: (0.088)(R 0.087, F 0.089)]  [G loss: -1.704] \n",
                        "533 [D loss: (0.089)(R 0.087, F 0.090)]  [G loss: -1.720] \n",
                        "534 [D loss: (0.089)(R 0.088, F 0.090)]  [G loss: -1.737] \n",
                        "535 [D loss: (0.091)(R 0.090, F 0.092)]  [G loss: -1.753] \n",
                        "536 [D loss: (0.091)(R 0.090, F 0.092)]  [G loss: -1.770] \n",
                        "537 [D loss: (0.092)(R 0.091, F 0.093)]  [G loss: -1.787] \n",
                        "538 [D loss: (0.093)(R 0.092, F 0.094)]  [G loss: -1.804] \n",
                        "539 [D loss: (0.094)(R 0.093, F 0.095)]  [G loss: -1.820] \n",
                        "540 [D loss: (0.094)(R 0.093, F 0.095)]  [G loss: -1.838] \n",
                        "541 [D loss: (0.096)(R 0.095, F 0.097)]  [G loss: -1.853] \n",
                        "542 [D loss: (0.097)(R 0.096, F 0.098)]  [G loss: -1.868] \n",
                        "543 [D loss: (0.097)(R 0.096, F 0.098)]  [G loss: -1.886] \n",
                        "544 [D loss: (0.098)(R 0.097, F 0.100)]  [G loss: -1.902] \n",
                        "545 [D loss: (0.099)(R 0.098, F 0.100)]  [G loss: -1.918] \n",
                        "546 [D loss: (0.100)(R 0.099, F 0.101)]  [G loss: -1.934] \n",
                        "547 [D loss: (0.101)(R 0.100, F 0.102)]  [G loss: -1.950] \n",
                        "548 [D loss: (0.101)(R 0.100, F 0.102)]  [G loss: -1.967] \n",
                        "549 [D loss: (0.102)(R 0.101, F 0.103)]  [G loss: -1.983] \n",
                        "550 [D loss: (0.103)(R 0.102, F 0.104)]  [G loss: -1.998] \n",
                        "551 [D loss: (0.103)(R 0.102, F 0.104)]  [G loss: -2.015] \n",
                        "552 [D loss: (0.104)(R 0.103, F 0.105)]  [G loss: -2.030] \n",
                        "553 [D loss: (0.105)(R 0.104, F 0.106)]  [G loss: -2.046] \n",
                        "554 [D loss: (0.105)(R 0.104, F 0.106)]  [G loss: -2.062] \n",
                        "555 [D loss: (0.106)(R 0.105, F 0.107)]  [G loss: -2.077] \n",
                        "556 [D loss: (0.107)(R 0.106, F 0.108)]  [G loss: -2.094] \n",
                        "557 [D loss: (0.108)(R 0.107, F 0.109)]  [G loss: -2.109] \n",
                        "558 [D loss: (0.108)(R 0.107, F 0.109)]  [G loss: -2.126] \n",
                        "559 [D loss: (0.109)(R 0.108, F 0.110)]  [G loss: -2.142] \n",
                        "560 [D loss: (0.110)(R 0.108, F 0.111)]  [G loss: -2.159] \n",
                        "561 [D loss: (0.110)(R 0.109, F 0.111)]  [G loss: -2.176] \n",
                        "562 [D loss: (0.111)(R 0.110, F 0.112)]  [G loss: -2.192] \n",
                        "563 [D loss: (0.112)(R 0.110, F 0.113)]  [G loss: -2.209] \n",
                        "564 [D loss: (0.112)(R 0.111, F 0.113)]  [G loss: -2.226] \n",
                        "565 [D loss: (0.113)(R 0.112, F 0.114)]  [G loss: -2.244] \n",
                        "566 [D loss: (0.113)(R 0.112, F 0.115)]  [G loss: -2.260] \n",
                        "567 [D loss: (0.114)(R 0.112, F 0.115)]  [G loss: -2.279] \n",
                        "568 [D loss: (0.115)(R 0.114, F 0.116)]  [G loss: -2.296] \n",
                        "569 [D loss: (0.116)(R 0.114, F 0.117)]  [G loss: -2.313] \n",
                        "570 [D loss: (0.116)(R 0.115, F 0.117)]  [G loss: -2.331] \n",
                        "571 [D loss: (0.117)(R 0.116, F 0.118)]  [G loss: -2.349] \n",
                        "572 [D loss: (0.117)(R 0.116, F 0.119)]  [G loss: -2.367] \n",
                        "573 [D loss: (0.118)(R 0.117, F 0.119)]  [G loss: -2.386] \n",
                        "574 [D loss: (0.119)(R 0.118, F 0.120)]  [G loss: -2.403] \n",
                        "575 [D loss: (0.119)(R 0.118, F 0.120)]  [G loss: -2.422] \n",
                        "576 [D loss: (0.120)(R 0.119, F 0.121)]  [G loss: -2.441] \n",
                        "577 [D loss: (0.121)(R 0.120, F 0.122)]  [G loss: -2.461] \n",
                        "578 [D loss: (0.122)(R 0.121, F 0.123)]  [G loss: -2.480] \n",
                        "579 [D loss: (0.123)(R 0.121, F 0.124)]  [G loss: -2.500] \n",
                        "580 [D loss: (0.124)(R 0.122, F 0.125)]  [G loss: -2.521] \n",
                        "581 [D loss: (0.125)(R 0.124, F 0.126)]  [G loss: -2.540] \n",
                        "582 [D loss: (0.126)(R 0.125, F 0.127)]  [G loss: -2.561] \n",
                        "583 [D loss: (0.127)(R 0.126, F 0.128)]  [G loss: -2.582] \n",
                        "584 [D loss: (0.128)(R 0.127, F 0.130)]  [G loss: -2.604] \n",
                        "585 [D loss: (0.129)(R 0.128, F 0.131)]  [G loss: -2.625] \n",
                        "586 [D loss: (0.130)(R 0.129, F 0.132)]  [G loss: -2.647] \n",
                        "587 [D loss: (0.132)(R 0.131, F 0.134)]  [G loss: -2.668] \n",
                        "588 [D loss: (0.133)(R 0.132, F 0.134)]  [G loss: -2.689] \n",
                        "589 [D loss: (0.134)(R 0.133, F 0.135)]  [G loss: -2.712] \n",
                        "590 [D loss: (0.136)(R 0.134, F 0.137)]  [G loss: -2.734] \n",
                        "591 [D loss: (0.137)(R 0.135, F 0.138)]  [G loss: -2.756] \n",
                        "592 [D loss: (0.138)(R 0.136, F 0.139)]  [G loss: -2.778] \n",
                        "593 [D loss: (0.139)(R 0.137, F 0.140)]  [G loss: -2.800] \n",
                        "594 [D loss: (0.140)(R 0.138, F 0.141)]  [G loss: -2.822] \n",
                        "595 [D loss: (0.140)(R 0.139, F 0.142)]  [G loss: -2.845] \n",
                        "596 [D loss: (0.142)(R 0.140, F 0.143)]  [G loss: -2.867] \n",
                        "597 [D loss: (0.142)(R 0.141, F 0.144)]  [G loss: -2.890] \n",
                        "598 [D loss: (0.143)(R 0.142, F 0.145)]  [G loss: -2.913] \n",
                        "599 [D loss: (0.144)(R 0.143, F 0.146)]  [G loss: -2.935] \n",
                        "600 [D loss: (0.145)(R 0.144, F 0.147)]  [G loss: -2.959] \n",
                        "601 [D loss: (0.146)(R 0.145, F 0.148)]  [G loss: -2.981] \n",
                        "602 [D loss: (0.147)(R 0.146, F 0.149)]  [G loss: -3.004] \n",
                        "603 [D loss: (0.148)(R 0.147, F 0.150)]  [G loss: -3.027] \n",
                        "604 [D loss: (0.149)(R 0.148, F 0.151)]  [G loss: -3.049] \n",
                        "605 [D loss: (0.150)(R 0.148, F 0.151)]  [G loss: -3.073] \n",
                        "606 [D loss: (0.151)(R 0.149, F 0.153)]  [G loss: -3.096] \n",
                        "607 [D loss: (0.152)(R 0.150, F 0.153)]  [G loss: -3.119] \n",
                        "608 [D loss: (0.153)(R 0.151, F 0.154)]  [G loss: -3.143] \n",
                        "609 [D loss: (0.154)(R 0.152, F 0.155)]  [G loss: -3.165] \n",
                        "610 [D loss: (0.154)(R 0.153, F 0.156)]  [G loss: -3.189] \n",
                        "611 [D loss: (0.155)(R 0.154, F 0.157)]  [G loss: -3.213] \n",
                        "612 [D loss: (0.156)(R 0.155, F 0.158)]  [G loss: -3.236] \n",
                        "613 [D loss: (0.157)(R 0.155, F 0.159)]  [G loss: -3.261] \n",
                        "614 [D loss: (0.158)(R 0.157, F 0.160)]  [G loss: -3.284] \n",
                        "615 [D loss: (0.159)(R 0.158, F 0.161)]  [G loss: -3.309] \n",
                        "616 [D loss: (0.160)(R 0.158, F 0.162)]  [G loss: -3.333] \n",
                        "617 [D loss: (0.161)(R 0.159, F 0.163)]  [G loss: -3.358] \n",
                        "618 [D loss: (0.162)(R 0.160, F 0.163)]  [G loss: -3.383] \n",
                        "619 [D loss: (0.163)(R 0.162, F 0.165)]  [G loss: -3.407] \n",
                        "620 [D loss: (0.164)(R 0.163, F 0.166)]  [G loss: -3.432] \n",
                        "621 [D loss: (0.165)(R 0.163, F 0.167)]  [G loss: -3.458] \n",
                        "622 [D loss: (0.166)(R 0.164, F 0.168)]  [G loss: -3.483] \n",
                        "623 [D loss: (0.167)(R 0.165, F 0.169)]  [G loss: -3.509] \n",
                        "624 [D loss: (0.168)(R 0.166, F 0.170)]  [G loss: -3.535] \n",
                        "625 [D loss: (0.169)(R 0.167, F 0.171)]  [G loss: -3.560] \n",
                        "626 [D loss: (0.170)(R 0.168, F 0.172)]  [G loss: -3.587] \n",
                        "627 [D loss: (0.171)(R 0.169, F 0.173)]  [G loss: -3.614] \n",
                        "628 [D loss: (0.172)(R 0.170, F 0.174)]  [G loss: -3.641] \n",
                        "629 [D loss: (0.173)(R 0.172, F 0.175)]  [G loss: -3.668] \n",
                        "630 [D loss: (0.174)(R 0.173, F 0.176)]  [G loss: -3.695] \n",
                        "631 [D loss: (0.175)(R 0.173, F 0.177)]  [G loss: -3.722] \n",
                        "632 [D loss: (0.176)(R 0.175, F 0.178)]  [G loss: -3.749] \n",
                        "633 [D loss: (0.178)(R 0.176, F 0.180)]  [G loss: -3.778] \n",
                        "634 [D loss: (0.179)(R 0.177, F 0.181)]  [G loss: -3.806] \n",
                        "635 [D loss: (0.180)(R 0.178, F 0.182)]  [G loss: -3.833] \n",
                        "636 [D loss: (0.181)(R 0.179, F 0.183)]  [G loss: -3.861] \n",
                        "637 [D loss: (0.183)(R 0.181, F 0.184)]  [G loss: -3.890] \n",
                        "638 [D loss: (0.184)(R 0.182, F 0.186)]  [G loss: -3.919] \n",
                        "639 [D loss: (0.185)(R 0.183, F 0.187)]  [G loss: -3.948] \n",
                        "640 [D loss: (0.186)(R 0.184, F 0.188)]  [G loss: -3.978] \n",
                        "641 [D loss: (0.187)(R 0.186, F 0.189)]  [G loss: -4.007] \n",
                        "642 [D loss: (0.189)(R 0.187, F 0.191)]  [G loss: -4.036] \n",
                        "643 [D loss: (0.190)(R 0.188, F 0.192)]  [G loss: -4.065] \n",
                        "644 [D loss: (0.191)(R 0.189, F 0.193)]  [G loss: -4.096] \n",
                        "645 [D loss: (0.192)(R 0.190, F 0.194)]  [G loss: -4.125] \n",
                        "646 [D loss: (0.194)(R 0.191, F 0.196)]  [G loss: -4.156] \n",
                        "647 [D loss: (0.195)(R 0.193, F 0.197)]  [G loss: -4.188] \n",
                        "648 [D loss: (0.196)(R 0.194, F 0.198)]  [G loss: -4.218] \n",
                        "649 [D loss: (0.198)(R 0.196, F 0.200)]  [G loss: -4.250] \n",
                        "650 [D loss: (0.199)(R 0.197, F 0.201)]  [G loss: -4.281] \n",
                        "651 [D loss: (0.201)(R 0.199, F 0.203)]  [G loss: -4.312] \n",
                        "652 [D loss: (0.202)(R 0.200, F 0.204)]  [G loss: -4.344] \n",
                        "653 [D loss: (0.203)(R 0.201, F 0.205)]  [G loss: -4.376] \n",
                        "654 [D loss: (0.205)(R 0.202, F 0.207)]  [G loss: -4.409] \n",
                        "655 [D loss: (0.206)(R 0.204, F 0.208)]  [G loss: -4.442] \n",
                        "656 [D loss: (0.208)(R 0.206, F 0.210)]  [G loss: -4.475] \n",
                        "657 [D loss: (0.209)(R 0.207, F 0.211)]  [G loss: -4.509] \n",
                        "658 [D loss: (0.211)(R 0.209, F 0.213)]  [G loss: -4.541] \n",
                        "659 [D loss: (0.213)(R 0.210, F 0.215)]  [G loss: -4.575] \n",
                        "660 [D loss: (0.214)(R 0.212, F 0.216)]  [G loss: -4.609] \n",
                        "661 [D loss: (0.216)(R 0.214, F 0.218)]  [G loss: -4.642] \n",
                        "662 [D loss: (0.217)(R 0.215, F 0.220)]  [G loss: -4.676] \n",
                        "663 [D loss: (0.219)(R 0.217, F 0.221)]  [G loss: -4.710] \n",
                        "664 [D loss: (0.221)(R 0.219, F 0.223)]  [G loss: -4.744] \n",
                        "665 [D loss: (0.222)(R 0.220, F 0.225)]  [G loss: -4.777] \n",
                        "666 [D loss: (0.224)(R 0.222, F 0.226)]  [G loss: -4.810] \n",
                        "667 [D loss: (0.226)(R 0.223, F 0.228)]  [G loss: -4.843] \n",
                        "668 [D loss: (0.227)(R 0.225, F 0.229)]  [G loss: -4.875] \n",
                        "669 [D loss: (0.228)(R 0.226, F 0.230)]  [G loss: -4.908] \n",
                        "670 [D loss: (0.230)(R 0.228, F 0.232)]  [G loss: -4.940] \n",
                        "671 [D loss: (0.231)(R 0.229, F 0.233)]  [G loss: -4.974] \n",
                        "672 [D loss: (0.233)(R 0.231, F 0.235)]  [G loss: -5.007] \n",
                        "673 [D loss: (0.234)(R 0.232, F 0.236)]  [G loss: -5.040] \n",
                        "674 [D loss: (0.236)(R 0.233, F 0.238)]  [G loss: -5.072] \n",
                        "675 [D loss: (0.237)(R 0.235, F 0.239)]  [G loss: -5.105] \n",
                        "676 [D loss: (0.238)(R 0.236, F 0.240)]  [G loss: -5.138] \n",
                        "677 [D loss: (0.239)(R 0.237, F 0.242)]  [G loss: -5.172] \n",
                        "678 [D loss: (0.241)(R 0.239, F 0.243)]  [G loss: -5.205] \n",
                        "679 [D loss: (0.243)(R 0.241, F 0.245)]  [G loss: -5.240] \n",
                        "680 [D loss: (0.244)(R 0.242, F 0.247)]  [G loss: -5.273] \n",
                        "681 [D loss: (0.246)(R 0.244, F 0.248)]  [G loss: -5.307] \n",
                        "682 [D loss: (0.247)(R 0.245, F 0.249)]  [G loss: -5.341] \n",
                        "683 [D loss: (0.249)(R 0.247, F 0.251)]  [G loss: -5.375] \n",
                        "684 [D loss: (0.250)(R 0.248, F 0.253)]  [G loss: -5.410] \n",
                        "685 [D loss: (0.252)(R 0.250, F 0.254)]  [G loss: -5.446] \n",
                        "686 [D loss: (0.253)(R 0.251, F 0.256)]  [G loss: -5.482] \n",
                        "687 [D loss: (0.256)(R 0.253, F 0.258)]  [G loss: -5.517] \n",
                        "688 [D loss: (0.257)(R 0.255, F 0.260)]  [G loss: -5.553] \n",
                        "689 [D loss: (0.259)(R 0.257, F 0.262)]  [G loss: -5.588] \n",
                        "690 [D loss: (0.261)(R 0.258, F 0.263)]  [G loss: -5.624] \n",
                        "691 [D loss: (0.262)(R 0.260, F 0.265)]  [G loss: -5.660] \n",
                        "692 [D loss: (0.264)(R 0.262, F 0.266)]  [G loss: -5.697] \n",
                        "693 [D loss: (0.266)(R 0.263, F 0.268)]  [G loss: -5.735] \n",
                        "694 [D loss: (0.267)(R 0.265, F 0.270)]  [G loss: -5.772] \n",
                        "695 [D loss: (0.269)(R 0.266, F 0.271)]  [G loss: -5.811] \n",
                        "696 [D loss: (0.272)(R 0.269, F 0.274)]  [G loss: -5.849] \n",
                        "697 [D loss: (0.273)(R 0.271, F 0.276)]  [G loss: -5.888] \n",
                        "698 [D loss: (0.275)(R 0.273, F 0.278)]  [G loss: -5.927] \n",
                        "699 [D loss: (0.277)(R 0.275, F 0.280)]  [G loss: -5.966] \n",
                        "700 [D loss: (0.279)(R 0.277, F 0.282)]  [G loss: -6.005] \n",
                        "701 [D loss: (0.281)(R 0.279, F 0.284)]  [G loss: -6.044] \n",
                        "702 [D loss: (0.283)(R 0.281, F 0.286)]  [G loss: -6.083] \n",
                        "703 [D loss: (0.285)(R 0.283, F 0.288)]  [G loss: -6.124] \n",
                        "704 [D loss: (0.288)(R 0.285, F 0.290)]  [G loss: -6.163] \n",
                        "705 [D loss: (0.290)(R 0.287, F 0.293)]  [G loss: -6.204] \n",
                        "706 [D loss: (0.292)(R 0.289, F 0.295)]  [G loss: -6.244] \n",
                        "707 [D loss: (0.294)(R 0.292, F 0.297)]  [G loss: -6.285] \n",
                        "708 [D loss: (0.297)(R 0.294, F 0.299)]  [G loss: -6.325] \n",
                        "709 [D loss: (0.299)(R 0.296, F 0.301)]  [G loss: -6.364] \n",
                        "710 [D loss: (0.301)(R 0.298, F 0.303)]  [G loss: -6.402] \n",
                        "711 [D loss: (0.303)(R 0.301, F 0.306)]  [G loss: -6.439] \n",
                        "712 [D loss: (0.305)(R 0.302, F 0.308)]  [G loss: -6.477] \n",
                        "713 [D loss: (0.307)(R 0.304, F 0.309)]  [G loss: -6.514] \n",
                        "714 [D loss: (0.309)(R 0.306, F 0.311)]  [G loss: -6.551] \n",
                        "715 [D loss: (0.310)(R 0.308, F 0.313)]  [G loss: -6.588] \n",
                        "716 [D loss: (0.312)(R 0.309, F 0.314)]  [G loss: -6.627] \n",
                        "717 [D loss: (0.314)(R 0.311, F 0.316)]  [G loss: -6.664] \n",
                        "718 [D loss: (0.316)(R 0.313, F 0.318)]  [G loss: -6.701] \n",
                        "719 [D loss: (0.318)(R 0.315, F 0.320)]  [G loss: -6.736] \n",
                        "720 [D loss: (0.319)(R 0.317, F 0.322)]  [G loss: -6.770] \n",
                        "721 [D loss: (0.321)(R 0.318, F 0.323)]  [G loss: -6.803] \n",
                        "722 [D loss: (0.322)(R 0.320, F 0.324)]  [G loss: -6.836] \n",
                        "723 [D loss: (0.323)(R 0.321, F 0.325)]  [G loss: -6.870] \n",
                        "724 [D loss: (0.325)(R 0.322, F 0.327)]  [G loss: -6.903] \n",
                        "725 [D loss: (0.326)(R 0.323, F 0.328)]  [G loss: -6.938] \n",
                        "726 [D loss: (0.327)(R 0.325, F 0.329)]  [G loss: -6.974] \n",
                        "727 [D loss: (0.328)(R 0.326, F 0.331)]  [G loss: -7.011] \n",
                        "728 [D loss: (0.329)(R 0.327, F 0.332)]  [G loss: -7.051] \n",
                        "729 [D loss: (0.331)(R 0.328, F 0.333)]  [G loss: -7.093] \n",
                        "730 [D loss: (0.333)(R 0.330, F 0.336)]  [G loss: -7.138] \n",
                        "731 [D loss: (0.336)(R 0.333, F 0.339)]  [G loss: -7.181] \n",
                        "732 [D loss: (0.339)(R 0.336, F 0.342)]  [G loss: -7.222] \n",
                        "733 [D loss: (0.341)(R 0.339, F 0.344)]  [G loss: -7.260] \n",
                        "734 [D loss: (0.343)(R 0.340, F 0.345)]  [G loss: -7.297] \n",
                        "735 [D loss: (0.344)(R 0.341, F 0.346)]  [G loss: -7.333] \n",
                        "736 [D loss: (0.344)(R 0.342, F 0.347)]  [G loss: -7.369] \n",
                        "737 [D loss: (0.345)(R 0.343, F 0.348)]  [G loss: -7.406] \n",
                        "738 [D loss: (0.346)(R 0.344, F 0.348)]  [G loss: -7.444] \n",
                        "739 [D loss: (0.347)(R 0.345, F 0.350)]  [G loss: -7.483] \n",
                        "740 [D loss: (0.349)(R 0.346, F 0.352)]  [G loss: -7.525] \n",
                        "741 [D loss: (0.351)(R 0.348, F 0.354)]  [G loss: -7.568] \n",
                        "742 [D loss: (0.353)(R 0.350, F 0.356)]  [G loss: -7.613] \n",
                        "743 [D loss: (0.355)(R 0.352, F 0.358)]  [G loss: -7.661] \n",
                        "744 [D loss: (0.356)(R 0.353, F 0.359)]  [G loss: -7.711] \n",
                        "745 [D loss: (0.357)(R 0.354, F 0.360)]  [G loss: -7.761] \n",
                        "746 [D loss: (0.358)(R 0.354, F 0.361)]  [G loss: -7.813] \n",
                        "747 [D loss: (0.358)(R 0.355, F 0.361)]  [G loss: -7.867] \n",
                        "748 [D loss: (0.358)(R 0.355, F 0.362)]  [G loss: -7.924] \n",
                        "749 [D loss: (0.360)(R 0.357, F 0.364)]  [G loss: -7.983] \n",
                        "750 [D loss: (0.363)(R 0.360, F 0.367)]  [G loss: -8.043] \n",
                        "751 [D loss: (0.367)(R 0.364, F 0.370)]  [G loss: -8.101] \n",
                        "752 [D loss: (0.371)(R 0.368, F 0.375)]  [G loss: -8.158] \n",
                        "753 [D loss: (0.376)(R 0.372, F 0.379)]  [G loss: -8.213] \n",
                        "754 [D loss: (0.380)(R 0.377, F 0.384)]  [G loss: -8.268] \n",
                        "755 [D loss: (0.385)(R 0.382, F 0.389)]  [G loss: -8.322] \n",
                        "756 [D loss: (0.390)(R 0.387, F 0.394)]  [G loss: -8.374] \n",
                        "757 [D loss: (0.394)(R 0.391, F 0.397)]  [G loss: -8.423] \n",
                        "758 [D loss: (0.397)(R 0.394, F 0.400)]  [G loss: -8.471] \n",
                        "759 [D loss: (0.398)(R 0.395, F 0.401)]  [G loss: -8.516] \n",
                        "760 [D loss: (0.399)(R 0.396, F 0.402)]  [G loss: -8.560] \n",
                        "761 [D loss: (0.400)(R 0.397, F 0.402)]  [G loss: -8.604] \n",
                        "762 [D loss: (0.401)(R 0.398, F 0.404)]  [G loss: -8.647] \n",
                        "763 [D loss: (0.403)(R 0.400, F 0.406)]  [G loss: -8.691] \n",
                        "764 [D loss: (0.406)(R 0.403, F 0.409)]  [G loss: -8.738] \n",
                        "765 [D loss: (0.411)(R 0.407, F 0.414)]  [G loss: -8.785] \n",
                        "766 [D loss: (0.416)(R 0.412, F 0.419)]  [G loss: -8.833] \n",
                        "767 [D loss: (0.421)(R 0.417, F 0.424)]  [G loss: -8.877] \n",
                        "768 [D loss: (0.424)(R 0.421, F 0.427)]  [G loss: -8.920] \n",
                        "769 [D loss: (0.427)(R 0.424, F 0.430)]  [G loss: -8.960] \n",
                        "770 [D loss: (0.428)(R 0.425, F 0.431)]  [G loss: -8.996] \n",
                        "771 [D loss: (0.428)(R 0.425, F 0.430)]  [G loss: -9.029] \n",
                        "772 [D loss: (0.427)(R 0.424, F 0.429)]  [G loss: -9.061] \n",
                        "773 [D loss: (0.426)(R 0.423, F 0.428)]  [G loss: -9.093] \n",
                        "774 [D loss: (0.425)(R 0.422, F 0.427)]  [G loss: -9.126] \n",
                        "775 [D loss: (0.425)(R 0.422, F 0.427)]  [G loss: -9.159] \n",
                        "776 [D loss: (0.426)(R 0.423, F 0.428)]  [G loss: -9.193] \n",
                        "777 [D loss: (0.427)(R 0.425, F 0.430)]  [G loss: -9.228] \n",
                        "778 [D loss: (0.430)(R 0.427, F 0.432)]  [G loss: -9.263] \n",
                        "779 [D loss: (0.432)(R 0.429, F 0.435)]  [G loss: -9.298] \n",
                        "780 [D loss: (0.434)(R 0.431, F 0.436)]  [G loss: -9.331] \n",
                        "781 [D loss: (0.435)(R 0.432, F 0.437)]  [G loss: -9.363] \n",
                        "782 [D loss: (0.435)(R 0.432, F 0.437)]  [G loss: -9.394] \n",
                        "783 [D loss: (0.434)(R 0.432, F 0.436)]  [G loss: -9.422] \n",
                        "784 [D loss: (0.433)(R 0.431, F 0.435)]  [G loss: -9.450] \n",
                        "785 [D loss: (0.431)(R 0.429, F 0.434)]  [G loss: -9.477] \n",
                        "786 [D loss: (0.430)(R 0.428, F 0.433)]  [G loss: -9.504] \n",
                        "787 [D loss: (0.430)(R 0.427, F 0.432)]  [G loss: -9.536] \n",
                        "788 [D loss: (0.430)(R 0.428, F 0.433)]  [G loss: -9.568] \n",
                        "789 [D loss: (0.431)(R 0.428, F 0.433)]  [G loss: -9.601] \n",
                        "790 [D loss: (0.431)(R 0.428, F 0.433)]  [G loss: -9.637] \n",
                        "791 [D loss: (0.431)(R 0.429, F 0.434)]  [G loss: -9.675] \n",
                        "792 [D loss: (0.433)(R 0.430, F 0.435)]  [G loss: -9.712] \n",
                        "793 [D loss: (0.434)(R 0.431, F 0.436)]  [G loss: -9.749] \n",
                        "794 [D loss: (0.435)(R 0.432, F 0.438)]  [G loss: -9.785] \n",
                        "795 [D loss: (0.437)(R 0.434, F 0.439)]  [G loss: -9.821] \n",
                        "796 [D loss: (0.439)(R 0.436, F 0.442)]  [G loss: -9.857] \n",
                        "797 [D loss: (0.441)(R 0.438, F 0.444)]  [G loss: -9.895] \n",
                        "798 [D loss: (0.443)(R 0.440, F 0.446)]  [G loss: -9.932] \n",
                        "799 [D loss: (0.445)(R 0.442, F 0.448)]  [G loss: -9.967] \n",
                        "800 [D loss: (0.446)(R 0.444, F 0.449)]  [G loss: -10.003] \n",
                        "801 [D loss: (0.447)(R 0.445, F 0.450)]  [G loss: -10.037] \n",
                        "802 [D loss: (0.449)(R 0.446, F 0.451)]  [G loss: -10.072] \n",
                        "803 [D loss: (0.450)(R 0.447, F 0.452)]  [G loss: -10.106] \n",
                        "804 [D loss: (0.451)(R 0.448, F 0.454)]  [G loss: -10.143] \n",
                        "805 [D loss: (0.453)(R 0.450, F 0.455)]  [G loss: -10.182] \n",
                        "806 [D loss: (0.454)(R 0.451, F 0.457)]  [G loss: -10.221] \n",
                        "807 [D loss: (0.455)(R 0.452, F 0.458)]  [G loss: -10.262] \n",
                        "808 [D loss: (0.457)(R 0.454, F 0.459)]  [G loss: -10.305] \n",
                        "809 [D loss: (0.458)(R 0.455, F 0.461)]  [G loss: -10.348] \n",
                        "810 [D loss: (0.460)(R 0.457, F 0.463)]  [G loss: -10.392] \n",
                        "811 [D loss: (0.461)(R 0.458, F 0.465)]  [G loss: -10.437] \n",
                        "812 [D loss: (0.463)(R 0.460, F 0.467)]  [G loss: -10.483] \n",
                        "813 [D loss: (0.466)(R 0.462, F 0.469)]  [G loss: -10.529] \n",
                        "814 [D loss: (0.468)(R 0.464, F 0.471)]  [G loss: -10.576] \n",
                        "815 [D loss: (0.469)(R 0.466, F 0.473)]  [G loss: -10.624] \n",
                        "816 [D loss: (0.471)(R 0.468, F 0.475)]  [G loss: -10.673] \n",
                        "817 [D loss: (0.473)(R 0.470, F 0.477)]  [G loss: -10.724] \n",
                        "818 [D loss: (0.476)(R 0.472, F 0.479)]  [G loss: -10.777] \n",
                        "819 [D loss: (0.478)(R 0.475, F 0.482)]  [G loss: -10.830] \n",
                        "820 [D loss: (0.481)(R 0.477, F 0.484)]  [G loss: -10.885] \n",
                        "821 [D loss: (0.484)(R 0.480, F 0.487)]  [G loss: -10.941] \n",
                        "822 [D loss: (0.487)(R 0.483, F 0.490)]  [G loss: -10.998] \n",
                        "823 [D loss: (0.490)(R 0.486, F 0.494)]  [G loss: -11.052] \n",
                        "824 [D loss: (0.493)(R 0.489, F 0.497)]  [G loss: -11.106] \n",
                        "825 [D loss: (0.496)(R 0.493, F 0.500)]  [G loss: -11.161] \n",
                        "826 [D loss: (0.499)(R 0.496, F 0.503)]  [G loss: -11.213] \n",
                        "827 [D loss: (0.502)(R 0.499, F 0.506)]  [G loss: -11.263] \n",
                        "828 [D loss: (0.505)(R 0.501, F 0.508)]  [G loss: -11.312] \n",
                        "829 [D loss: (0.507)(R 0.503, F 0.510)]  [G loss: -11.361] \n",
                        "830 [D loss: (0.509)(R 0.506, F 0.512)]  [G loss: -11.408] \n",
                        "831 [D loss: (0.511)(R 0.507, F 0.514)]  [G loss: -11.454] \n",
                        "832 [D loss: (0.512)(R 0.509, F 0.515)]  [G loss: -11.501] \n",
                        "833 [D loss: (0.514)(R 0.511, F 0.517)]  [G loss: -11.546] \n",
                        "834 [D loss: (0.516)(R 0.513, F 0.519)]  [G loss: -11.592] \n",
                        "835 [D loss: (0.518)(R 0.515, F 0.521)]  [G loss: -11.637] \n",
                        "836 [D loss: (0.519)(R 0.516, F 0.523)]  [G loss: -11.681] \n",
                        "837 [D loss: (0.521)(R 0.518, F 0.524)]  [G loss: -11.726] \n",
                        "838 [D loss: (0.523)(R 0.520, F 0.526)]  [G loss: -11.771] \n",
                        "839 [D loss: (0.525)(R 0.522, F 0.528)]  [G loss: -11.817] \n",
                        "840 [D loss: (0.528)(R 0.524, F 0.531)]  [G loss: -11.863] \n",
                        "841 [D loss: (0.530)(R 0.527, F 0.533)]  [G loss: -11.909] \n",
                        "842 [D loss: (0.533)(R 0.530, F 0.536)]  [G loss: -11.954] \n",
                        "843 [D loss: (0.535)(R 0.532, F 0.539)]  [G loss: -12.000] \n",
                        "844 [D loss: (0.537)(R 0.534, F 0.541)]  [G loss: -12.046] \n",
                        "845 [D loss: (0.540)(R 0.536, F 0.543)]  [G loss: -12.091] \n",
                        "846 [D loss: (0.542)(R 0.538, F 0.545)]  [G loss: -12.137] \n",
                        "847 [D loss: (0.544)(R 0.540, F 0.547)]  [G loss: -12.183] \n",
                        "848 [D loss: (0.546)(R 0.542, F 0.549)]  [G loss: -12.229] \n",
                        "849 [D loss: (0.548)(R 0.545, F 0.551)]  [G loss: -12.274] \n",
                        "850 [D loss: (0.550)(R 0.546, F 0.553)]  [G loss: -12.317] \n",
                        "851 [D loss: (0.551)(R 0.548, F 0.554)]  [G loss: -12.357] \n",
                        "852 [D loss: (0.552)(R 0.549, F 0.555)]  [G loss: -12.396] \n",
                        "853 [D loss: (0.553)(R 0.550, F 0.556)]  [G loss: -12.435] \n",
                        "854 [D loss: (0.554)(R 0.551, F 0.556)]  [G loss: -12.473] \n",
                        "855 [D loss: (0.554)(R 0.551, F 0.557)]  [G loss: -12.511] \n",
                        "856 [D loss: (0.555)(R 0.552, F 0.558)]  [G loss: -12.549] \n",
                        "857 [D loss: (0.556)(R 0.553, F 0.558)]  [G loss: -12.587] \n",
                        "858 [D loss: (0.556)(R 0.553, F 0.559)]  [G loss: -12.625] \n",
                        "859 [D loss: (0.557)(R 0.554, F 0.560)]  [G loss: -12.664] \n",
                        "860 [D loss: (0.557)(R 0.555, F 0.560)]  [G loss: -12.703] \n",
                        "861 [D loss: (0.558)(R 0.555, F 0.561)]  [G loss: -12.743] \n",
                        "862 [D loss: (0.559)(R 0.556, F 0.562)]  [G loss: -12.784] \n",
                        "863 [D loss: (0.560)(R 0.557, F 0.563)]  [G loss: -12.826] \n",
                        "864 [D loss: (0.561)(R 0.558, F 0.564)]  [G loss: -12.868] \n",
                        "865 [D loss: (0.563)(R 0.560, F 0.566)]  [G loss: -12.911] \n",
                        "866 [D loss: (0.564)(R 0.561, F 0.567)]  [G loss: -12.954] \n",
                        "867 [D loss: (0.565)(R 0.562, F 0.568)]  [G loss: -12.997] \n",
                        "868 [D loss: (0.566)(R 0.563, F 0.570)]  [G loss: -13.042] \n",
                        "869 [D loss: (0.568)(R 0.565, F 0.571)]  [G loss: -13.087] \n",
                        "870 [D loss: (0.570)(R 0.566, F 0.573)]  [G loss: -13.133] \n",
                        "871 [D loss: (0.571)(R 0.568, F 0.575)]  [G loss: -13.181] \n",
                        "872 [D loss: (0.573)(R 0.570, F 0.576)]  [G loss: -13.228] \n",
                        "873 [D loss: (0.575)(R 0.571, F 0.578)]  [G loss: -13.277] \n",
                        "874 [D loss: (0.576)(R 0.573, F 0.580)]  [G loss: -13.327] \n",
                        "875 [D loss: (0.579)(R 0.575, F 0.582)]  [G loss: -13.377] \n",
                        "876 [D loss: (0.581)(R 0.578, F 0.585)]  [G loss: -13.428] \n",
                        "877 [D loss: (0.584)(R 0.580, F 0.587)]  [G loss: -13.479] \n",
                        "878 [D loss: (0.586)(R 0.582, F 0.589)]  [G loss: -13.529] \n",
                        "879 [D loss: (0.588)(R 0.584, F 0.591)]  [G loss: -13.579] \n",
                        "880 [D loss: (0.590)(R 0.586, F 0.593)]  [G loss: -13.628] \n",
                        "881 [D loss: (0.592)(R 0.588, F 0.595)]  [G loss: -13.677] \n",
                        "882 [D loss: (0.594)(R 0.590, F 0.597)]  [G loss: -13.726] \n",
                        "883 [D loss: (0.596)(R 0.592, F 0.599)]  [G loss: -13.776] \n",
                        "884 [D loss: (0.598)(R 0.595, F 0.602)]  [G loss: -13.826] \n",
                        "885 [D loss: (0.600)(R 0.597, F 0.604)]  [G loss: -13.877] \n",
                        "886 [D loss: (0.602)(R 0.599, F 0.606)]  [G loss: -13.929] \n",
                        "887 [D loss: (0.605)(R 0.601, F 0.608)]  [G loss: -13.981] \n",
                        "888 [D loss: (0.606)(R 0.603, F 0.610)]  [G loss: -14.036] \n",
                        "889 [D loss: (0.608)(R 0.605, F 0.612)]  [G loss: -14.092] \n",
                        "890 [D loss: (0.612)(R 0.608, F 0.615)]  [G loss: -14.147] \n",
                        "891 [D loss: (0.614)(R 0.611, F 0.618)]  [G loss: -14.201] \n",
                        "892 [D loss: (0.617)(R 0.613, F 0.621)]  [G loss: -14.253] \n",
                        "893 [D loss: (0.619)(R 0.616, F 0.623)]  [G loss: -14.305] \n",
                        "894 [D loss: (0.621)(R 0.618, F 0.625)]  [G loss: -14.355] \n",
                        "895 [D loss: (0.623)(R 0.620, F 0.627)]  [G loss: -14.405] \n",
                        "896 [D loss: (0.625)(R 0.621, F 0.629)]  [G loss: -14.456] \n",
                        "897 [D loss: (0.628)(R 0.624, F 0.631)]  [G loss: -14.506] \n",
                        "898 [D loss: (0.630)(R 0.626, F 0.633)]  [G loss: -14.560] \n",
                        "899 [D loss: (0.633)(R 0.629, F 0.636)]  [G loss: -14.613] \n",
                        "900 [D loss: (0.635)(R 0.632, F 0.639)]  [G loss: -14.666] \n",
                        "901 [D loss: (0.638)(R 0.634, F 0.641)]  [G loss: -14.717] \n",
                        "902 [D loss: (0.640)(R 0.636, F 0.643)]  [G loss: -14.767] \n",
                        "903 [D loss: (0.642)(R 0.638, F 0.645)]  [G loss: -14.815] \n",
                        "904 [D loss: (0.644)(R 0.640, F 0.647)]  [G loss: -14.864] \n",
                        "905 [D loss: (0.646)(R 0.643, F 0.650)]  [G loss: -14.912] \n",
                        "906 [D loss: (0.648)(R 0.645, F 0.652)]  [G loss: -14.959] \n",
                        "907 [D loss: (0.650)(R 0.647, F 0.653)]  [G loss: -15.005] \n",
                        "908 [D loss: (0.651)(R 0.648, F 0.655)]  [G loss: -15.050] \n",
                        "909 [D loss: (0.652)(R 0.649, F 0.656)]  [G loss: -15.095] \n",
                        "910 [D loss: (0.654)(R 0.650, F 0.657)]  [G loss: -15.142] \n",
                        "911 [D loss: (0.655)(R 0.652, F 0.659)]  [G loss: -15.185] \n",
                        "912 [D loss: (0.656)(R 0.653, F 0.660)]  [G loss: -15.231] \n",
                        "913 [D loss: (0.658)(R 0.654, F 0.661)]  [G loss: -15.276] \n",
                        "914 [D loss: (0.659)(R 0.656, F 0.662)]  [G loss: -15.321] \n",
                        "915 [D loss: (0.660)(R 0.657, F 0.663)]  [G loss: -15.368] \n",
                        "916 [D loss: (0.662)(R 0.659, F 0.665)]  [G loss: -15.413] \n",
                        "917 [D loss: (0.664)(R 0.660, F 0.667)]  [G loss: -15.460] \n",
                        "918 [D loss: (0.665)(R 0.662, F 0.669)]  [G loss: -15.505] \n",
                        "919 [D loss: (0.667)(R 0.664, F 0.670)]  [G loss: -15.550] \n",
                        "920 [D loss: (0.668)(R 0.665, F 0.672)]  [G loss: -15.593] \n",
                        "921 [D loss: (0.670)(R 0.666, F 0.673)]  [G loss: -15.637] \n",
                        "922 [D loss: (0.671)(R 0.668, F 0.674)]  [G loss: -15.681] \n",
                        "923 [D loss: (0.673)(R 0.669, F 0.676)]  [G loss: -15.726] \n",
                        "924 [D loss: (0.674)(R 0.671, F 0.677)]  [G loss: -15.771] \n",
                        "925 [D loss: (0.675)(R 0.672, F 0.679)]  [G loss: -15.818] \n",
                        "926 [D loss: (0.677)(R 0.673, F 0.680)]  [G loss: -15.865] \n",
                        "927 [D loss: (0.678)(R 0.675, F 0.682)]  [G loss: -15.915] \n",
                        "928 [D loss: (0.680)(R 0.677, F 0.684)]  [G loss: -15.967] \n",
                        "929 [D loss: (0.683)(R 0.680, F 0.687)]  [G loss: -16.019] \n",
                        "930 [D loss: (0.686)(R 0.683, F 0.690)]  [G loss: -16.070] \n",
                        "931 [D loss: (0.688)(R 0.685, F 0.692)]  [G loss: -16.118] \n",
                        "932 [D loss: (0.690)(R 0.686, F 0.693)]  [G loss: -16.166] \n",
                        "933 [D loss: (0.691)(R 0.688, F 0.695)]  [G loss: -16.213] \n",
                        "934 [D loss: (0.692)(R 0.689, F 0.696)]  [G loss: -16.262] \n",
                        "935 [D loss: (0.694)(R 0.690, F 0.697)]  [G loss: -16.313] \n",
                        "936 [D loss: (0.696)(R 0.692, F 0.700)]  [G loss: -16.364] \n",
                        "937 [D loss: (0.698)(R 0.694, F 0.702)]  [G loss: -16.417] \n",
                        "938 [D loss: (0.700)(R 0.697, F 0.704)]  [G loss: -16.468] \n",
                        "939 [D loss: (0.702)(R 0.698, F 0.706)]  [G loss: -16.521] \n",
                        "940 [D loss: (0.704)(R 0.701, F 0.708)]  [G loss: -16.573] \n",
                        "941 [D loss: (0.706)(R 0.703, F 0.710)]  [G loss: -16.624] \n",
                        "942 [D loss: (0.708)(R 0.705, F 0.712)]  [G loss: -16.675] \n",
                        "943 [D loss: (0.710)(R 0.706, F 0.714)]  [G loss: -16.726] \n",
                        "944 [D loss: (0.712)(R 0.708, F 0.716)]  [G loss: -16.776] \n",
                        "945 [D loss: (0.713)(R 0.709, F 0.717)]  [G loss: -16.827] \n",
                        "946 [D loss: (0.715)(R 0.711, F 0.718)]  [G loss: -16.878] \n",
                        "947 [D loss: (0.716)(R 0.712, F 0.720)]  [G loss: -16.931] \n",
                        "948 [D loss: (0.718)(R 0.715, F 0.722)]  [G loss: -16.982] \n",
                        "949 [D loss: (0.720)(R 0.716, F 0.724)]  [G loss: -17.035] \n",
                        "950 [D loss: (0.722)(R 0.718, F 0.726)]  [G loss: -17.085] \n",
                        "951 [D loss: (0.724)(R 0.720, F 0.727)]  [G loss: -17.139] \n",
                        "952 [D loss: (0.726)(R 0.722, F 0.729)]  [G loss: -17.189] \n",
                        "953 [D loss: (0.727)(R 0.723, F 0.731)]  [G loss: -17.243] \n",
                        "954 [D loss: (0.729)(R 0.725, F 0.733)]  [G loss: -17.296] \n",
                        "955 [D loss: (0.731)(R 0.728, F 0.735)]  [G loss: -17.349] \n",
                        "956 [D loss: (0.734)(R 0.730, F 0.737)]  [G loss: -17.403] \n",
                        "957 [D loss: (0.736)(R 0.732, F 0.740)]  [G loss: -17.457] \n",
                        "958 [D loss: (0.739)(R 0.735, F 0.743)]  [G loss: -17.511] \n",
                        "959 [D loss: (0.742)(R 0.738, F 0.745)]  [G loss: -17.565] \n",
                        "960 [D loss: (0.744)(R 0.740, F 0.748)]  [G loss: -17.619] \n",
                        "961 [D loss: (0.746)(R 0.743, F 0.750)]  [G loss: -17.671] \n",
                        "962 [D loss: (0.748)(R 0.745, F 0.752)]  [G loss: -17.723] \n",
                        "963 [D loss: (0.751)(R 0.747, F 0.754)]  [G loss: -17.774] \n",
                        "964 [D loss: (0.753)(R 0.749, F 0.757)]  [G loss: -17.824] \n",
                        "965 [D loss: (0.755)(R 0.751, F 0.759)]  [G loss: -17.875] \n",
                        "966 [D loss: (0.757)(R 0.754, F 0.761)]  [G loss: -17.926] \n",
                        "967 [D loss: (0.760)(R 0.756, F 0.764)]  [G loss: -17.977] \n",
                        "968 [D loss: (0.762)(R 0.759, F 0.766)]  [G loss: -18.026] \n",
                        "969 [D loss: (0.765)(R 0.761, F 0.768)]  [G loss: -18.075] \n",
                        "970 [D loss: (0.767)(R 0.763, F 0.770)]  [G loss: -18.123] \n",
                        "971 [D loss: (0.768)(R 0.765, F 0.772)]  [G loss: -18.172] \n",
                        "972 [D loss: (0.771)(R 0.767, F 0.774)]  [G loss: -18.217] \n",
                        "973 [D loss: (0.772)(R 0.769, F 0.776)]  [G loss: -18.264] \n",
                        "974 [D loss: (0.773)(R 0.770, F 0.777)]  [G loss: -18.311] \n",
                        "975 [D loss: (0.775)(R 0.772, F 0.779)]  [G loss: -18.356] \n",
                        "976 [D loss: (0.776)(R 0.773, F 0.780)]  [G loss: -18.403] \n",
                        "977 [D loss: (0.778)(R 0.775, F 0.782)]  [G loss: -18.449] \n",
                        "978 [D loss: (0.780)(R 0.776, F 0.783)]  [G loss: -18.496] \n",
                        "979 [D loss: (0.781)(R 0.778, F 0.785)]  [G loss: -18.544] \n",
                        "980 [D loss: (0.783)(R 0.779, F 0.787)]  [G loss: -18.591] \n",
                        "981 [D loss: (0.784)(R 0.781, F 0.788)]  [G loss: -18.641] \n",
                        "982 [D loss: (0.786)(R 0.783, F 0.790)]  [G loss: -18.689] \n",
                        "983 [D loss: (0.788)(R 0.785, F 0.792)]  [G loss: -18.740] \n",
                        "984 [D loss: (0.790)(R 0.787, F 0.794)]  [G loss: -18.793] \n",
                        "985 [D loss: (0.793)(R 0.789, F 0.797)]  [G loss: -18.843] \n",
                        "986 [D loss: (0.794)(R 0.790, F 0.798)]  [G loss: -18.895] \n",
                        "987 [D loss: (0.798)(R 0.794, F 0.801)]  [G loss: -18.946] \n",
                        "988 [D loss: (0.800)(R 0.796, F 0.803)]  [G loss: -18.994] \n",
                        "989 [D loss: (0.802)(R 0.798, F 0.805)]  [G loss: -19.047] \n",
                        "990 [D loss: (0.804)(R 0.800, F 0.808)]  [G loss: -19.101] \n",
                        "991 [D loss: (0.807)(R 0.804, F 0.811)]  [G loss: -19.152] \n",
                        "992 [D loss: (0.810)(R 0.806, F 0.813)]  [G loss: -19.205] \n",
                        "993 [D loss: (0.812)(R 0.808, F 0.815)]  [G loss: -19.257] \n",
                        "994 [D loss: (0.814)(R 0.810, F 0.817)]  [G loss: -19.310] \n",
                        "995 [D loss: (0.816)(R 0.812, F 0.820)]  [G loss: -19.365] \n",
                        "996 [D loss: (0.818)(R 0.814, F 0.822)]  [G loss: -19.415] \n",
                        "997 [D loss: (0.821)(R 0.817, F 0.825)]  [G loss: -19.469] \n",
                        "998 [D loss: (0.823)(R 0.819, F 0.827)]  [G loss: -19.525] \n",
                        "999 [D loss: (0.826)(R 0.822, F 0.830)]  [G loss: -19.578] \n",
                        "1000 [D loss: (0.828)(R 0.824, F 0.832)]  [G loss: -19.632] \n",
                        "1001 [D loss: (0.829)(R 0.825, F 0.833)]  [G loss: -19.688] \n",
                        "1002 [D loss: (0.832)(R 0.828, F 0.836)]  [G loss: -19.742] \n",
                        "1003 [D loss: (0.835)(R 0.831, F 0.839)]  [G loss: -19.796] \n",
                        "1004 [D loss: (0.837)(R 0.833, F 0.841)]  [G loss: -19.853] \n",
                        "1005 [D loss: (0.840)(R 0.836, F 0.844)]  [G loss: -19.907] \n",
                        "1006 [D loss: (0.842)(R 0.838, F 0.846)]  [G loss: -19.962] \n",
                        "1007 [D loss: (0.844)(R 0.840, F 0.848)]  [G loss: -20.018] \n",
                        "1008 [D loss: (0.846)(R 0.842, F 0.850)]  [G loss: -20.074] \n",
                        "1009 [D loss: (0.849)(R 0.845, F 0.853)]  [G loss: -20.130] \n",
                        "1010 [D loss: (0.851)(R 0.847, F 0.855)]  [G loss: -20.188] \n",
                        "1011 [D loss: (0.854)(R 0.850, F 0.858)]  [G loss: -20.243] \n",
                        "1012 [D loss: (0.857)(R 0.853, F 0.861)]  [G loss: -20.299] \n",
                        "1013 [D loss: (0.859)(R 0.855, F 0.863)]  [G loss: -20.355] \n",
                        "1014 [D loss: (0.862)(R 0.858, F 0.866)]  [G loss: -20.409] \n",
                        "1015 [D loss: (0.864)(R 0.860, F 0.868)]  [G loss: -20.462] \n",
                        "1016 [D loss: (0.866)(R 0.862, F 0.870)]  [G loss: -20.516] \n",
                        "1017 [D loss: (0.869)(R 0.865, F 0.873)]  [G loss: -20.568] \n",
                        "1018 [D loss: (0.871)(R 0.867, F 0.875)]  [G loss: -20.623] \n",
                        "1019 [D loss: (0.874)(R 0.870, F 0.878)]  [G loss: -20.675] \n",
                        "1020 [D loss: (0.876)(R 0.872, F 0.880)]  [G loss: -20.728] \n",
                        "1021 [D loss: (0.879)(R 0.875, F 0.883)]  [G loss: -20.780] \n",
                        "1022 [D loss: (0.881)(R 0.877, F 0.885)]  [G loss: -20.832] \n",
                        "1023 [D loss: (0.883)(R 0.879, F 0.887)]  [G loss: -20.886] \n",
                        "1024 [D loss: (0.886)(R 0.882, F 0.890)]  [G loss: -20.938] \n",
                        "1025 [D loss: (0.888)(R 0.884, F 0.892)]  [G loss: -20.991] \n",
                        "1026 [D loss: (0.890)(R 0.886, F 0.894)]  [G loss: -21.041] \n",
                        "1027 [D loss: (0.893)(R 0.889, F 0.896)]  [G loss: -21.090] \n",
                        "1028 [D loss: (0.894)(R 0.891, F 0.898)]  [G loss: -21.140] \n",
                        "1029 [D loss: (0.896)(R 0.892, F 0.900)]  [G loss: -21.190] \n",
                        "1030 [D loss: (0.898)(R 0.895, F 0.902)]  [G loss: -21.239] \n",
                        "1031 [D loss: (0.900)(R 0.896, F 0.904)]  [G loss: -21.289] \n",
                        "1032 [D loss: (0.902)(R 0.899, F 0.906)]  [G loss: -21.337] \n",
                        "1033 [D loss: (0.904)(R 0.900, F 0.908)]  [G loss: -21.387] \n",
                        "1034 [D loss: (0.906)(R 0.902, F 0.910)]  [G loss: -21.437] \n",
                        "1035 [D loss: (0.908)(R 0.904, F 0.912)]  [G loss: -21.486] \n",
                        "1036 [D loss: (0.909)(R 0.906, F 0.913)]  [G loss: -21.538] \n",
                        "1037 [D loss: (0.912)(R 0.908, F 0.915)]  [G loss: -21.587] \n",
                        "1038 [D loss: (0.914)(R 0.910, F 0.918)]  [G loss: -21.639] \n",
                        "1039 [D loss: (0.916)(R 0.912, F 0.920)]  [G loss: -21.691] \n",
                        "1040 [D loss: (0.918)(R 0.914, F 0.922)]  [G loss: -21.743] \n",
                        "1041 [D loss: (0.920)(R 0.916, F 0.924)]  [G loss: -21.796] \n",
                        "1042 [D loss: (0.922)(R 0.918, F 0.926)]  [G loss: -21.849] \n",
                        "1043 [D loss: (0.924)(R 0.920, F 0.928)]  [G loss: -21.903] \n",
                        "1044 [D loss: (0.927)(R 0.923, F 0.931)]  [G loss: -21.954] \n",
                        "1045 [D loss: (0.929)(R 0.925, F 0.933)]  [G loss: -22.008] \n",
                        "1046 [D loss: (0.931)(R 0.927, F 0.936)]  [G loss: -22.063] \n",
                        "1047 [D loss: (0.935)(R 0.931, F 0.938)]  [G loss: -22.115] \n",
                        "1048 [D loss: (0.937)(R 0.933, F 0.941)]  [G loss: -22.168] \n",
                        "1049 [D loss: (0.939)(R 0.935, F 0.943)]  [G loss: -22.222] \n",
                        "1050 [D loss: (0.943)(R 0.938, F 0.947)]  [G loss: -22.274] \n",
                        "1051 [D loss: (0.945)(R 0.941, F 0.949)]  [G loss: -22.326] \n",
                        "1052 [D loss: (0.947)(R 0.943, F 0.951)]  [G loss: -22.379] \n",
                        "1053 [D loss: (0.950)(R 0.946, F 0.954)]  [G loss: -22.430] \n",
                        "1054 [D loss: (0.952)(R 0.948, F 0.956)]  [G loss: -22.481] \n",
                        "1055 [D loss: (0.954)(R 0.950, F 0.958)]  [G loss: -22.532] \n",
                        "1056 [D loss: (0.956)(R 0.952, F 0.960)]  [G loss: -22.583] \n",
                        "1057 [D loss: (0.958)(R 0.954, F 0.962)]  [G loss: -22.633] \n",
                        "1058 [D loss: (0.960)(R 0.956, F 0.964)]  [G loss: -22.684] \n",
                        "1059 [D loss: (0.962)(R 0.958, F 0.966)]  [G loss: -22.733] \n",
                        "1060 [D loss: (0.964)(R 0.961, F 0.968)]  [G loss: -22.783] \n",
                        "1061 [D loss: (0.966)(R 0.962, F 0.970)]  [G loss: -22.833] \n",
                        "1062 [D loss: (0.968)(R 0.964, F 0.972)]  [G loss: -22.882] \n",
                        "1063 [D loss: (0.970)(R 0.967, F 0.974)]  [G loss: -22.931] \n",
                        "1064 [D loss: (0.972)(R 0.968, F 0.976)]  [G loss: -22.982] \n",
                        "1065 [D loss: (0.974)(R 0.970, F 0.978)]  [G loss: -23.033] \n",
                        "1066 [D loss: (0.976)(R 0.972, F 0.980)]  [G loss: -23.085] \n",
                        "1067 [D loss: (0.978)(R 0.975, F 0.982)]  [G loss: -23.135] \n",
                        "1068 [D loss: (0.980)(R 0.977, F 0.984)]  [G loss: -23.185] \n",
                        "1069 [D loss: (0.982)(R 0.979, F 0.986)]  [G loss: -23.234] \n",
                        "1070 [D loss: (0.984)(R 0.980, F 0.988)]  [G loss: -23.283] \n",
                        "1071 [D loss: (0.986)(R 0.983, F 0.990)]  [G loss: -23.332] \n",
                        "1072 [D loss: (0.988)(R 0.984, F 0.992)]  [G loss: -23.384] \n",
                        "1073 [D loss: (0.991)(R 0.987, F 0.995)]  [G loss: -23.433] \n",
                        "1074 [D loss: (0.992)(R 0.988, F 0.996)]  [G loss: -23.487] \n",
                        "1075 [D loss: (0.994)(R 0.990, F 0.998)]  [G loss: -23.543] \n",
                        "1076 [D loss: (0.997)(R 0.992, F 1.001)]  [G loss: -23.600] \n",
                        "1077 [D loss: (0.999)(R 0.995, F 1.004)]  [G loss: -23.658] \n",
                        "1078 [D loss: (1.003)(R 0.999, F 1.007)]  [G loss: -23.713] \n",
                        "1079 [D loss: (1.006)(R 1.002, F 1.010)]  [G loss: -23.762] \n",
                        "1080 [D loss: (1.007)(R 1.003, F 1.011)]  [G loss: -23.809] \n",
                        "1081 [D loss: (1.008)(R 1.005, F 1.012)]  [G loss: -23.858] \n",
                        "1082 [D loss: (1.010)(R 1.006, F 1.013)]  [G loss: -23.908] \n",
                        "1083 [D loss: (1.011)(R 1.008, F 1.015)]  [G loss: -23.957] \n",
                        "1084 [D loss: (1.013)(R 1.010, F 1.017)]  [G loss: -24.009] \n",
                        "1085 [D loss: (1.016)(R 1.012, F 1.020)]  [G loss: -24.061] \n",
                        "1086 [D loss: (1.018)(R 1.014, F 1.022)]  [G loss: -24.116] \n",
                        "1087 [D loss: (1.020)(R 1.016, F 1.024)]  [G loss: -24.172] \n",
                        "1088 [D loss: (1.021)(R 1.017, F 1.026)]  [G loss: -24.228] \n",
                        "1089 [D loss: (1.022)(R 1.018, F 1.026)]  [G loss: -24.288] \n",
                        "1090 [D loss: (1.024)(R 1.019, F 1.028)]  [G loss: -24.350] \n",
                        "1091 [D loss: (1.026)(R 1.022, F 1.031)]  [G loss: -24.418] \n",
                        "1092 [D loss: (1.032)(R 1.027, F 1.036)]  [G loss: -24.485] \n",
                        "1093 [D loss: (1.037)(R 1.032, F 1.042)]  [G loss: -24.551] \n",
                        "1094 [D loss: (1.042)(R 1.038, F 1.047)]  [G loss: -24.614] \n",
                        "1095 [D loss: (1.047)(R 1.043, F 1.051)]  [G loss: -24.673] \n",
                        "1096 [D loss: (1.051)(R 1.046, F 1.055)]  [G loss: -24.728] \n",
                        "1097 [D loss: (1.053)(R 1.049, F 1.057)]  [G loss: -24.779] \n",
                        "1098 [D loss: (1.054)(R 1.050, F 1.058)]  [G loss: -24.829] \n",
                        "1099 [D loss: (1.055)(R 1.052, F 1.059)]  [G loss: -24.879] \n",
                        "1100 [D loss: (1.057)(R 1.053, F 1.061)]  [G loss: -24.929] \n",
                        "1101 [D loss: (1.059)(R 1.055, F 1.063)]  [G loss: -24.978] \n",
                        "1102 [D loss: (1.061)(R 1.057, F 1.064)]  [G loss: -25.028] \n",
                        "1103 [D loss: (1.063)(R 1.059, F 1.067)]  [G loss: -25.079] \n",
                        "1104 [D loss: (1.066)(R 1.062, F 1.070)]  [G loss: -25.130] \n",
                        "1105 [D loss: (1.069)(R 1.065, F 1.073)]  [G loss: -25.181] \n",
                        "1106 [D loss: (1.072)(R 1.068, F 1.076)]  [G loss: -25.233] \n",
                        "1107 [D loss: (1.074)(R 1.070, F 1.078)]  [G loss: -25.284] \n",
                        "1108 [D loss: (1.076)(R 1.072, F 1.079)]  [G loss: -25.336] \n",
                        "1109 [D loss: (1.077)(R 1.073, F 1.081)]  [G loss: -25.387] \n",
                        "1110 [D loss: (1.079)(R 1.075, F 1.083)]  [G loss: -25.436] \n",
                        "1111 [D loss: (1.081)(R 1.077, F 1.085)]  [G loss: -25.480] \n",
                        "1112 [D loss: (1.083)(R 1.079, F 1.087)]  [G loss: -25.524] \n",
                        "1113 [D loss: (1.085)(R 1.081, F 1.088)]  [G loss: -25.566] \n",
                        "1114 [D loss: (1.086)(R 1.082, F 1.089)]  [G loss: -25.608] \n",
                        "1115 [D loss: (1.086)(R 1.083, F 1.089)]  [G loss: -25.649] \n",
                        "1116 [D loss: (1.086)(R 1.083, F 1.089)]  [G loss: -25.690] \n",
                        "1117 [D loss: (1.087)(R 1.083, F 1.090)]  [G loss: -25.732] \n",
                        "1118 [D loss: (1.087)(R 1.084, F 1.090)]  [G loss: -25.773] \n",
                        "1119 [D loss: (1.088)(R 1.085, F 1.091)]  [G loss: -25.815] \n",
                        "1120 [D loss: (1.090)(R 1.086, F 1.093)]  [G loss: -25.858] \n",
                        "1121 [D loss: (1.092)(R 1.088, F 1.095)]  [G loss: -25.901] \n",
                        "1122 [D loss: (1.092)(R 1.088, F 1.096)]  [G loss: -25.946] \n",
                        "1123 [D loss: (1.092)(R 1.088, F 1.096)]  [G loss: -25.993] \n",
                        "1124 [D loss: (1.092)(R 1.088, F 1.096)]  [G loss: -26.041] \n",
                        "1125 [D loss: (1.093)(R 1.089, F 1.097)]  [G loss: -26.094] \n",
                        "1126 [D loss: (1.096)(R 1.092, F 1.100)]  [G loss: -26.150] \n",
                        "1127 [D loss: (1.101)(R 1.096, F 1.105)]  [G loss: -26.205] \n",
                        "1128 [D loss: (1.105)(R 1.101, F 1.110)]  [G loss: -26.259] \n",
                        "1129 [D loss: (1.109)(R 1.105, F 1.113)]  [G loss: -26.308] \n",
                        "1130 [D loss: (1.111)(R 1.107, F 1.115)]  [G loss: -26.356] \n",
                        "1131 [D loss: (1.112)(R 1.109, F 1.116)]  [G loss: -26.400] \n",
                        "1132 [D loss: (1.113)(R 1.109, F 1.117)]  [G loss: -26.446] \n",
                        "1133 [D loss: (1.114)(R 1.111, F 1.118)]  [G loss: -26.494] \n",
                        "1134 [D loss: (1.117)(R 1.113, F 1.121)]  [G loss: -26.545] \n",
                        "1135 [D loss: (1.119)(R 1.115, F 1.123)]  [G loss: -26.597] \n",
                        "1136 [D loss: (1.121)(R 1.117, F 1.125)]  [G loss: -26.651] \n",
                        "1137 [D loss: (1.123)(R 1.119, F 1.127)]  [G loss: -26.704] \n",
                        "1138 [D loss: (1.125)(R 1.121, F 1.129)]  [G loss: -26.757] \n",
                        "1139 [D loss: (1.128)(R 1.124, F 1.132)]  [G loss: -26.809] \n",
                        "1140 [D loss: (1.130)(R 1.126, F 1.134)]  [G loss: -26.858] \n",
                        "1141 [D loss: (1.132)(R 1.128, F 1.136)]  [G loss: -26.909] \n",
                        "1142 [D loss: (1.135)(R 1.131, F 1.138)]  [G loss: -26.958] \n",
                        "1143 [D loss: (1.136)(R 1.132, F 1.140)]  [G loss: -27.007] \n",
                        "1144 [D loss: (1.138)(R 1.134, F 1.142)]  [G loss: -27.060] \n",
                        "1145 [D loss: (1.140)(R 1.136, F 1.144)]  [G loss: -27.110] \n",
                        "1146 [D loss: (1.142)(R 1.138, F 1.146)]  [G loss: -27.161] \n",
                        "1147 [D loss: (1.144)(R 1.140, F 1.148)]  [G loss: -27.212] \n",
                        "1148 [D loss: (1.147)(R 1.143, F 1.151)]  [G loss: -27.262] \n",
                        "1149 [D loss: (1.149)(R 1.145, F 1.153)]  [G loss: -27.312] \n",
                        "1150 [D loss: (1.151)(R 1.147, F 1.155)]  [G loss: -27.361] \n",
                        "1151 [D loss: (1.153)(R 1.149, F 1.157)]  [G loss: -27.409] \n",
                        "1152 [D loss: (1.155)(R 1.151, F 1.159)]  [G loss: -27.457] \n",
                        "1153 [D loss: (1.157)(R 1.153, F 1.160)]  [G loss: -27.504] \n",
                        "1154 [D loss: (1.158)(R 1.154, F 1.162)]  [G loss: -27.551] \n",
                        "1155 [D loss: (1.160)(R 1.156, F 1.164)]  [G loss: -27.599] \n",
                        "1156 [D loss: (1.161)(R 1.158, F 1.165)]  [G loss: -27.645] \n",
                        "1157 [D loss: (1.163)(R 1.160, F 1.167)]  [G loss: -27.693] \n",
                        "1158 [D loss: (1.165)(R 1.161, F 1.169)]  [G loss: -27.740] \n",
                        "1159 [D loss: (1.167)(R 1.163, F 1.171)]  [G loss: -27.787] \n",
                        "1160 [D loss: (1.169)(R 1.165, F 1.172)]  [G loss: -27.835] \n",
                        "1161 [D loss: (1.170)(R 1.166, F 1.174)]  [G loss: -27.882] \n",
                        "1162 [D loss: (1.172)(R 1.168, F 1.176)]  [G loss: -27.930] \n",
                        "1163 [D loss: (1.174)(R 1.170, F 1.177)]  [G loss: -27.978] \n",
                        "1164 [D loss: (1.175)(R 1.171, F 1.179)]  [G loss: -28.025] \n",
                        "1165 [D loss: (1.177)(R 1.173, F 1.181)]  [G loss: -28.072] \n",
                        "1166 [D loss: (1.178)(R 1.175, F 1.182)]  [G loss: -28.119] \n",
                        "1167 [D loss: (1.180)(R 1.176, F 1.184)]  [G loss: -28.166] \n",
                        "1168 [D loss: (1.181)(R 1.178, F 1.185)]  [G loss: -28.214] \n",
                        "1169 [D loss: (1.183)(R 1.179, F 1.187)]  [G loss: -28.261] \n",
                        "1170 [D loss: (1.184)(R 1.181, F 1.188)]  [G loss: -28.309] \n",
                        "1171 [D loss: (1.186)(R 1.182, F 1.190)]  [G loss: -28.357] \n",
                        "1172 [D loss: (1.188)(R 1.184, F 1.192)]  [G loss: -28.408] \n",
                        "1173 [D loss: (1.189)(R 1.185, F 1.194)]  [G loss: -28.460] \n",
                        "1174 [D loss: (1.192)(R 1.188, F 1.196)]  [G loss: -28.514] \n",
                        "1175 [D loss: (1.194)(R 1.190, F 1.199)]  [G loss: -28.567] \n",
                        "1176 [D loss: (1.197)(R 1.193, F 1.201)]  [G loss: -28.619] \n",
                        "1177 [D loss: (1.199)(R 1.195, F 1.203)]  [G loss: -28.668] \n",
                        "1178 [D loss: (1.201)(R 1.197, F 1.205)]  [G loss: -28.716] \n",
                        "1179 [D loss: (1.202)(R 1.198, F 1.206)]  [G loss: -28.764] \n",
                        "1180 [D loss: (1.204)(R 1.200, F 1.208)]  [G loss: -28.811] \n",
                        "1181 [D loss: (1.205)(R 1.201, F 1.209)]  [G loss: -28.857] \n",
                        "1182 [D loss: (1.206)(R 1.202, F 1.210)]  [G loss: -28.904] \n",
                        "1183 [D loss: (1.207)(R 1.203, F 1.211)]  [G loss: -28.952] \n",
                        "1184 [D loss: (1.208)(R 1.205, F 1.212)]  [G loss: -29.003] \n",
                        "1185 [D loss: (1.211)(R 1.207, F 1.215)]  [G loss: -29.052] \n",
                        "1186 [D loss: (1.213)(R 1.209, F 1.217)]  [G loss: -29.104] \n",
                        "1187 [D loss: (1.216)(R 1.212, F 1.220)]  [G loss: -29.154] \n",
                        "1188 [D loss: (1.217)(R 1.213, F 1.221)]  [G loss: -29.205] \n",
                        "1189 [D loss: (1.220)(R 1.216, F 1.224)]  [G loss: -29.253] \n",
                        "1190 [D loss: (1.221)(R 1.217, F 1.225)]  [G loss: -29.303] \n",
                        "1191 [D loss: (1.223)(R 1.219, F 1.227)]  [G loss: -29.354] \n",
                        "1192 [D loss: (1.225)(R 1.221, F 1.229)]  [G loss: -29.404] \n",
                        "1193 [D loss: (1.227)(R 1.223, F 1.231)]  [G loss: -29.455] \n",
                        "1194 [D loss: (1.229)(R 1.225, F 1.233)]  [G loss: -29.506] \n",
                        "1195 [D loss: (1.230)(R 1.226, F 1.235)]  [G loss: -29.558] \n",
                        "1196 [D loss: (1.233)(R 1.229, F 1.237)]  [G loss: -29.609] \n",
                        "1197 [D loss: (1.235)(R 1.231, F 1.239)]  [G loss: -29.661] \n",
                        "1198 [D loss: (1.237)(R 1.233, F 1.241)]  [G loss: -29.713] \n",
                        "1199 [D loss: (1.239)(R 1.235, F 1.243)]  [G loss: -29.763] \n",
                        "1200 [D loss: (1.241)(R 1.237, F 1.245)]  [G loss: -29.817] \n",
                        "1201 [D loss: (1.243)(R 1.239, F 1.247)]  [G loss: -29.870] \n",
                        "1202 [D loss: (1.246)(R 1.241, F 1.250)]  [G loss: -29.924] \n",
                        "1203 [D loss: (1.248)(R 1.244, F 1.252)]  [G loss: -29.977] \n",
                        "1204 [D loss: (1.250)(R 1.246, F 1.254)]  [G loss: -30.028] \n",
                        "1205 [D loss: (1.253)(R 1.248, F 1.257)]  [G loss: -30.079] \n",
                        "1206 [D loss: (1.255)(R 1.251, F 1.259)]  [G loss: -30.131] \n",
                        "1207 [D loss: (1.258)(R 1.254, F 1.262)]  [G loss: -30.180] \n",
                        "1208 [D loss: (1.260)(R 1.255, F 1.264)]  [G loss: -30.232] \n",
                        "1209 [D loss: (1.262)(R 1.258, F 1.266)]  [G loss: -30.281] \n",
                        "1210 [D loss: (1.264)(R 1.260, F 1.268)]  [G loss: -30.330] \n",
                        "1211 [D loss: (1.266)(R 1.262, F 1.270)]  [G loss: -30.377] \n",
                        "1212 [D loss: (1.268)(R 1.264, F 1.272)]  [G loss: -30.426] \n",
                        "1213 [D loss: (1.270)(R 1.266, F 1.274)]  [G loss: -30.472] \n",
                        "1214 [D loss: (1.271)(R 1.267, F 1.275)]  [G loss: -30.520] \n",
                        "1215 [D loss: (1.273)(R 1.269, F 1.277)]  [G loss: -30.569] \n",
                        "1216 [D loss: (1.275)(R 1.271, F 1.278)]  [G loss: -30.618] \n",
                        "1217 [D loss: (1.276)(R 1.272, F 1.280)]  [G loss: -30.669] \n",
                        "1218 [D loss: (1.278)(R 1.274, F 1.282)]  [G loss: -30.721] \n",
                        "1219 [D loss: (1.280)(R 1.276, F 1.284)]  [G loss: -30.770] \n",
                        "1220 [D loss: (1.282)(R 1.278, F 1.286)]  [G loss: -30.820] \n",
                        "1221 [D loss: (1.284)(R 1.280, F 1.288)]  [G loss: -30.870] \n",
                        "1222 [D loss: (1.286)(R 1.282, F 1.290)]  [G loss: -30.920] \n",
                        "1223 [D loss: (1.288)(R 1.284, F 1.292)]  [G loss: -30.972] \n",
                        "1224 [D loss: (1.290)(R 1.286, F 1.294)]  [G loss: -31.022] \n",
                        "1225 [D loss: (1.292)(R 1.288, F 1.296)]  [G loss: -31.072] \n",
                        "1226 [D loss: (1.294)(R 1.290, F 1.298)]  [G loss: -31.123] \n",
                        "1227 [D loss: (1.296)(R 1.292, F 1.300)]  [G loss: -31.173] \n",
                        "1228 [D loss: (1.298)(R 1.294, F 1.302)]  [G loss: -31.227] \n",
                        "1229 [D loss: (1.301)(R 1.297, F 1.305)]  [G loss: -31.280] \n",
                        "1230 [D loss: (1.304)(R 1.300, F 1.308)]  [G loss: -31.332] \n",
                        "1231 [D loss: (1.306)(R 1.302, F 1.310)]  [G loss: -31.382] \n",
                        "1232 [D loss: (1.308)(R 1.304, F 1.312)]  [G loss: -31.435] \n",
                        "1233 [D loss: (1.311)(R 1.307, F 1.315)]  [G loss: -31.483] \n",
                        "1234 [D loss: (1.313)(R 1.309, F 1.317)]  [G loss: -31.533] \n",
                        "1235 [D loss: (1.316)(R 1.312, F 1.320)]  [G loss: -31.582] \n",
                        "1236 [D loss: (1.318)(R 1.314, F 1.322)]  [G loss: -31.629] \n",
                        "1237 [D loss: (1.320)(R 1.316, F 1.324)]  [G loss: -31.676] \n",
                        "1238 [D loss: (1.321)(R 1.318, F 1.325)]  [G loss: -31.722] \n",
                        "1239 [D loss: (1.323)(R 1.320, F 1.327)]  [G loss: -31.767] \n",
                        "1240 [D loss: (1.325)(R 1.321, F 1.329)]  [G loss: -31.814] \n",
                        "1241 [D loss: (1.327)(R 1.323, F 1.330)]  [G loss: -31.861] \n",
                        "1242 [D loss: (1.329)(R 1.325, F 1.333)]  [G loss: -31.907] \n",
                        "1243 [D loss: (1.331)(R 1.327, F 1.335)]  [G loss: -31.953] \n",
                        "1244 [D loss: (1.332)(R 1.328, F 1.336)]  [G loss: -32.000] \n",
                        "1245 [D loss: (1.334)(R 1.330, F 1.338)]  [G loss: -32.045] \n",
                        "1246 [D loss: (1.336)(R 1.332, F 1.339)]  [G loss: -32.090] \n",
                        "1247 [D loss: (1.337)(R 1.333, F 1.341)]  [G loss: -32.136] \n",
                        "1248 [D loss: (1.338)(R 1.335, F 1.342)]  [G loss: -32.182] \n",
                        "1249 [D loss: (1.340)(R 1.337, F 1.344)]  [G loss: -32.228] \n",
                        "1250 [D loss: (1.342)(R 1.338, F 1.346)]  [G loss: -32.275] \n",
                        "1251 [D loss: (1.344)(R 1.340, F 1.348)]  [G loss: -32.322] \n",
                        "1252 [D loss: (1.345)(R 1.341, F 1.349)]  [G loss: -32.371] \n",
                        "1253 [D loss: (1.347)(R 1.343, F 1.351)]  [G loss: -32.421] \n",
                        "1254 [D loss: (1.350)(R 1.346, F 1.354)]  [G loss: -32.469] \n",
                        "1255 [D loss: (1.352)(R 1.348, F 1.356)]  [G loss: -32.516] \n",
                        "1256 [D loss: (1.354)(R 1.350, F 1.358)]  [G loss: -32.562] \n",
                        "1257 [D loss: (1.355)(R 1.352, F 1.359)]  [G loss: -32.609] \n",
                        "1258 [D loss: (1.358)(R 1.354, F 1.362)]  [G loss: -32.655] \n",
                        "1259 [D loss: (1.359)(R 1.355, F 1.363)]  [G loss: -32.703] \n",
                        "1260 [D loss: (1.361)(R 1.357, F 1.365)]  [G loss: -32.750] \n",
                        "1261 [D loss: (1.362)(R 1.358, F 1.367)]  [G loss: -32.800] \n",
                        "1262 [D loss: (1.365)(R 1.360, F 1.369)]  [G loss: -32.851] \n",
                        "1263 [D loss: (1.367)(R 1.362, F 1.371)]  [G loss: -32.903] \n",
                        "1264 [D loss: (1.369)(R 1.365, F 1.373)]  [G loss: -32.954] \n",
                        "1265 [D loss: (1.371)(R 1.367, F 1.375)]  [G loss: -33.005] \n",
                        "1266 [D loss: (1.374)(R 1.370, F 1.378)]  [G loss: -33.056] \n",
                        "1267 [D loss: (1.376)(R 1.372, F 1.380)]  [G loss: -33.107] \n",
                        "1268 [D loss: (1.378)(R 1.374, F 1.383)]  [G loss: -33.157] \n",
                        "1269 [D loss: (1.381)(R 1.377, F 1.385)]  [G loss: -33.207] \n",
                        "1270 [D loss: (1.383)(R 1.379, F 1.387)]  [G loss: -33.256] \n",
                        "1271 [D loss: (1.385)(R 1.381, F 1.389)]  [G loss: -33.305] \n",
                        "1272 [D loss: (1.387)(R 1.383, F 1.391)]  [G loss: -33.355] \n",
                        "1273 [D loss: (1.389)(R 1.385, F 1.393)]  [G loss: -33.404] \n",
                        "1274 [D loss: (1.391)(R 1.387, F 1.395)]  [G loss: -33.454] \n",
                        "1275 [D loss: (1.393)(R 1.389, F 1.397)]  [G loss: -33.504] \n",
                        "1276 [D loss: (1.396)(R 1.392, F 1.400)]  [G loss: -33.554] \n",
                        "1277 [D loss: (1.398)(R 1.394, F 1.402)]  [G loss: -33.606] \n",
                        "1278 [D loss: (1.401)(R 1.397, F 1.405)]  [G loss: -33.654] \n",
                        "1279 [D loss: (1.403)(R 1.399, F 1.407)]  [G loss: -33.704] \n",
                        "1280 [D loss: (1.406)(R 1.402, F 1.410)]  [G loss: -33.753] \n",
                        "1281 [D loss: (1.408)(R 1.404, F 1.412)]  [G loss: -33.802] \n",
                        "1282 [D loss: (1.410)(R 1.406, F 1.414)]  [G loss: -33.851] \n",
                        "1283 [D loss: (1.413)(R 1.409, F 1.416)]  [G loss: -33.899] \n",
                        "1284 [D loss: (1.415)(R 1.411, F 1.419)]  [G loss: -33.948] \n",
                        "1285 [D loss: (1.417)(R 1.413, F 1.421)]  [G loss: -33.996] \n",
                        "1286 [D loss: (1.419)(R 1.415, F 1.423)]  [G loss: -34.047] \n",
                        "1287 [D loss: (1.421)(R 1.417, F 1.426)]  [G loss: -34.097] \n",
                        "1288 [D loss: (1.424)(R 1.420, F 1.428)]  [G loss: -34.147] \n",
                        "1289 [D loss: (1.426)(R 1.422, F 1.430)]  [G loss: -34.200] \n",
                        "1290 [D loss: (1.430)(R 1.426, F 1.434)]  [G loss: -34.248] \n",
                        "1291 [D loss: (1.432)(R 1.428, F 1.437)]  [G loss: -34.299] \n",
                        "1292 [D loss: (1.435)(R 1.431, F 1.439)]  [G loss: -34.350] \n",
                        "1293 [D loss: (1.438)(R 1.434, F 1.442)]  [G loss: -34.398] \n",
                        "1294 [D loss: (1.441)(R 1.437, F 1.445)]  [G loss: -34.446] \n",
                        "1295 [D loss: (1.443)(R 1.439, F 1.447)]  [G loss: -34.493] \n",
                        "1296 [D loss: (1.445)(R 1.442, F 1.449)]  [G loss: -34.539] \n",
                        "1297 [D loss: (1.448)(R 1.444, F 1.452)]  [G loss: -34.585] \n",
                        "1298 [D loss: (1.450)(R 1.446, F 1.454)]  [G loss: -34.629] \n",
                        "1299 [D loss: (1.452)(R 1.448, F 1.456)]  [G loss: -34.675] \n",
                        "1300 [D loss: (1.454)(R 1.450, F 1.458)]  [G loss: -34.719] \n",
                        "1301 [D loss: (1.456)(R 1.452, F 1.460)]  [G loss: -34.764] \n",
                        "1302 [D loss: (1.458)(R 1.454, F 1.462)]  [G loss: -34.810] \n",
                        "1303 [D loss: (1.460)(R 1.456, F 1.464)]  [G loss: -34.857] \n",
                        "1304 [D loss: (1.462)(R 1.458, F 1.466)]  [G loss: -34.904] \n",
                        "1305 [D loss: (1.464)(R 1.461, F 1.468)]  [G loss: -34.950] \n",
                        "1306 [D loss: (1.467)(R 1.463, F 1.471)]  [G loss: -34.998] \n",
                        "1307 [D loss: (1.469)(R 1.465, F 1.473)]  [G loss: -35.046] \n",
                        "1308 [D loss: (1.471)(R 1.467, F 1.475)]  [G loss: -35.093] \n",
                        "1309 [D loss: (1.473)(R 1.469, F 1.477)]  [G loss: -35.142] \n",
                        "1310 [D loss: (1.476)(R 1.473, F 1.480)]  [G loss: -35.188] \n",
                        "1311 [D loss: (1.479)(R 1.475, F 1.483)]  [G loss: -35.236] \n",
                        "1312 [D loss: (1.481)(R 1.477, F 1.485)]  [G loss: -35.285] \n",
                        "1313 [D loss: (1.484)(R 1.480, F 1.488)]  [G loss: -35.333] \n",
                        "1314 [D loss: (1.486)(R 1.482, F 1.490)]  [G loss: -35.381] \n",
                        "1315 [D loss: (1.488)(R 1.484, F 1.492)]  [G loss: -35.431] \n",
                        "1316 [D loss: (1.492)(R 1.487, F 1.496)]  [G loss: -35.480] \n",
                        "1317 [D loss: (1.494)(R 1.490, F 1.498)]  [G loss: -35.528] \n",
                        "1318 [D loss: (1.497)(R 1.493, F 1.500)]  [G loss: -35.574] \n",
                        "1319 [D loss: (1.499)(R 1.495, F 1.503)]  [G loss: -35.619] \n",
                        "1320 [D loss: (1.501)(R 1.497, F 1.505)]  [G loss: -35.662] \n",
                        "1321 [D loss: (1.502)(R 1.499, F 1.506)]  [G loss: -35.707] \n",
                        "1322 [D loss: (1.505)(R 1.501, F 1.509)]  [G loss: -35.752] \n",
                        "1323 [D loss: (1.507)(R 1.503, F 1.511)]  [G loss: -35.797] \n",
                        "1324 [D loss: (1.509)(R 1.506, F 1.513)]  [G loss: -35.839] \n",
                        "1325 [D loss: (1.511)(R 1.507, F 1.514)]  [G loss: -35.882] \n",
                        "1326 [D loss: (1.512)(R 1.509, F 1.516)]  [G loss: -35.924] \n",
                        "1327 [D loss: (1.514)(R 1.510, F 1.517)]  [G loss: -35.967] \n",
                        "1328 [D loss: (1.515)(R 1.512, F 1.519)]  [G loss: -36.012] \n",
                        "1329 [D loss: (1.518)(R 1.514, F 1.521)]  [G loss: -36.056] \n",
                        "1330 [D loss: (1.520)(R 1.516, F 1.524)]  [G loss: -36.100] \n",
                        "1331 [D loss: (1.522)(R 1.518, F 1.525)]  [G loss: -36.144] \n",
                        "1332 [D loss: (1.523)(R 1.519, F 1.527)]  [G loss: -36.188] \n",
                        "1333 [D loss: (1.525)(R 1.521, F 1.529)]  [G loss: -36.232] \n",
                        "1334 [D loss: (1.527)(R 1.523, F 1.530)]  [G loss: -36.278] \n",
                        "1335 [D loss: (1.529)(R 1.525, F 1.533)]  [G loss: -36.322] \n",
                        "1336 [D loss: (1.530)(R 1.527, F 1.534)]  [G loss: -36.367] \n",
                        "1337 [D loss: (1.532)(R 1.528, F 1.536)]  [G loss: -36.413] \n",
                        "1338 [D loss: (1.534)(R 1.530, F 1.538)]  [G loss: -36.458] \n",
                        "1339 [D loss: (1.536)(R 1.532, F 1.539)]  [G loss: -36.504] \n",
                        "1340 [D loss: (1.538)(R 1.534, F 1.541)]  [G loss: -36.550] \n",
                        "1341 [D loss: (1.539)(R 1.535, F 1.543)]  [G loss: -36.598] \n",
                        "1342 [D loss: (1.542)(R 1.538, F 1.546)]  [G loss: -36.646] \n",
                        "1343 [D loss: (1.544)(R 1.540, F 1.548)]  [G loss: -36.695] \n",
                        "1344 [D loss: (1.546)(R 1.542, F 1.550)]  [G loss: -36.744] \n",
                        "1345 [D loss: (1.549)(R 1.545, F 1.553)]  [G loss: -36.791] \n",
                        "1346 [D loss: (1.551)(R 1.547, F 1.555)]  [G loss: -36.838] \n",
                        "1347 [D loss: (1.553)(R 1.549, F 1.557)]  [G loss: -36.884] \n",
                        "1348 [D loss: (1.555)(R 1.551, F 1.559)]  [G loss: -36.928] \n",
                        "1349 [D loss: (1.557)(R 1.553, F 1.561)]  [G loss: -36.972] \n",
                        "1350 [D loss: (1.558)(R 1.555, F 1.562)]  [G loss: -37.017] \n",
                        "1351 [D loss: (1.560)(R 1.557, F 1.564)]  [G loss: -37.061] \n",
                        "1352 [D loss: (1.562)(R 1.558, F 1.566)]  [G loss: -37.105] \n",
                        "1353 [D loss: (1.564)(R 1.560, F 1.568)]  [G loss: -37.149] \n",
                        "1354 [D loss: (1.566)(R 1.562, F 1.569)]  [G loss: -37.192] \n",
                        "1355 [D loss: (1.567)(R 1.563, F 1.571)]  [G loss: -37.236] \n",
                        "1356 [D loss: (1.569)(R 1.565, F 1.573)]  [G loss: -37.280] \n",
                        "1357 [D loss: (1.571)(R 1.567, F 1.574)]  [G loss: -37.326] \n",
                        "1358 [D loss: (1.573)(R 1.569, F 1.577)]  [G loss: -37.372] \n",
                        "1359 [D loss: (1.575)(R 1.572, F 1.579)]  [G loss: -37.417] \n",
                        "1360 [D loss: (1.577)(R 1.574, F 1.581)]  [G loss: -37.461] \n",
                        "1361 [D loss: (1.579)(R 1.575, F 1.583)]  [G loss: -37.507] \n",
                        "1362 [D loss: (1.581)(R 1.577, F 1.585)]  [G loss: -37.552] \n",
                        "1363 [D loss: (1.583)(R 1.579, F 1.587)]  [G loss: -37.597] \n",
                        "1364 [D loss: (1.585)(R 1.581, F 1.589)]  [G loss: -37.642] \n",
                        "1365 [D loss: (1.586)(R 1.583, F 1.590)]  [G loss: -37.687] \n",
                        "1366 [D loss: (1.589)(R 1.585, F 1.592)]  [G loss: -37.730] \n",
                        "1367 [D loss: (1.590)(R 1.586, F 1.594)]  [G loss: -37.774] \n",
                        "1368 [D loss: (1.592)(R 1.588, F 1.596)]  [G loss: -37.817] \n",
                        "1369 [D loss: (1.594)(R 1.590, F 1.597)]  [G loss: -37.858] \n",
                        "1370 [D loss: (1.595)(R 1.591, F 1.599)]  [G loss: -37.901] \n",
                        "1371 [D loss: (1.597)(R 1.594, F 1.601)]  [G loss: -37.942] \n",
                        "1372 [D loss: (1.599)(R 1.595, F 1.603)]  [G loss: -37.984] \n",
                        "1373 [D loss: (1.601)(R 1.597, F 1.604)]  [G loss: -38.027] \n",
                        "1374 [D loss: (1.603)(R 1.600, F 1.607)]  [G loss: -38.066] \n",
                        "1375 [D loss: (1.605)(R 1.601, F 1.608)]  [G loss: -38.104] \n",
                        "1376 [D loss: (1.606)(R 1.602, F 1.609)]  [G loss: -38.142] \n",
                        "1377 [D loss: (1.607)(R 1.603, F 1.610)]  [G loss: -38.181] \n",
                        "1378 [D loss: (1.608)(R 1.605, F 1.612)]  [G loss: -38.218] \n",
                        "1379 [D loss: (1.609)(R 1.606, F 1.613)]  [G loss: -38.257] \n",
                        "1380 [D loss: (1.610)(R 1.607, F 1.614)]  [G loss: -38.296] \n",
                        "1381 [D loss: (1.612)(R 1.608, F 1.615)]  [G loss: -38.336] \n",
                        "1382 [D loss: (1.613)(R 1.609, F 1.616)]  [G loss: -38.377] \n",
                        "1383 [D loss: (1.614)(R 1.610, F 1.618)]  [G loss: -38.419] \n",
                        "1384 [D loss: (1.615)(R 1.611, F 1.619)]  [G loss: -38.462] \n",
                        "1385 [D loss: (1.616)(R 1.613, F 1.620)]  [G loss: -38.507] \n",
                        "1386 [D loss: (1.619)(R 1.615, F 1.622)]  [G loss: -38.553] \n",
                        "1387 [D loss: (1.621)(R 1.617, F 1.625)]  [G loss: -38.601] \n",
                        "1388 [D loss: (1.623)(R 1.619, F 1.627)]  [G loss: -38.648] \n",
                        "1389 [D loss: (1.625)(R 1.621, F 1.629)]  [G loss: -38.696] \n",
                        "1390 [D loss: (1.626)(R 1.622, F 1.630)]  [G loss: -38.744] \n",
                        "1391 [D loss: (1.629)(R 1.624, F 1.633)]  [G loss: -38.793] \n",
                        "1392 [D loss: (1.631)(R 1.627, F 1.635)]  [G loss: -38.841] \n",
                        "1393 [D loss: (1.633)(R 1.629, F 1.637)]  [G loss: -38.891] \n",
                        "1394 [D loss: (1.635)(R 1.631, F 1.639)]  [G loss: -38.939] \n",
                        "1395 [D loss: (1.637)(R 1.633, F 1.642)]  [G loss: -38.989] \n",
                        "1396 [D loss: (1.640)(R 1.636, F 1.644)]  [G loss: -39.038] \n",
                        "1397 [D loss: (1.642)(R 1.638, F 1.647)]  [G loss: -39.087] \n",
                        "1398 [D loss: (1.645)(R 1.641, F 1.649)]  [G loss: -39.136] \n",
                        "1399 [D loss: (1.647)(R 1.643, F 1.651)]  [G loss: -39.185] \n",
                        "1400 [D loss: (1.650)(R 1.646, F 1.654)]  [G loss: -39.232] \n",
                        "1401 [D loss: (1.653)(R 1.649, F 1.657)]  [G loss: -39.279] \n",
                        "1402 [D loss: (1.655)(R 1.651, F 1.659)]  [G loss: -39.325] \n",
                        "1403 [D loss: (1.657)(R 1.653, F 1.661)]  [G loss: -39.369] \n",
                        "1404 [D loss: (1.659)(R 1.655, F 1.663)]  [G loss: -39.413] \n",
                        "1405 [D loss: (1.661)(R 1.657, F 1.665)]  [G loss: -39.454] \n",
                        "1406 [D loss: (1.663)(R 1.659, F 1.666)]  [G loss: -39.495] \n",
                        "1407 [D loss: (1.664)(R 1.660, F 1.668)]  [G loss: -39.534] \n",
                        "1408 [D loss: (1.665)(R 1.662, F 1.669)]  [G loss: -39.574] \n",
                        "1409 [D loss: (1.667)(R 1.663, F 1.670)]  [G loss: -39.614] \n",
                        "1410 [D loss: (1.668)(R 1.664, F 1.672)]  [G loss: -39.653] \n",
                        "1411 [D loss: (1.669)(R 1.666, F 1.673)]  [G loss: -39.693] \n",
                        "1412 [D loss: (1.671)(R 1.667, F 1.674)]  [G loss: -39.732] \n",
                        "1413 [D loss: (1.672)(R 1.669, F 1.676)]  [G loss: -39.773] \n",
                        "1414 [D loss: (1.673)(R 1.670, F 1.677)]  [G loss: -39.814] \n",
                        "1415 [D loss: (1.675)(R 1.672, F 1.679)]  [G loss: -39.855] \n",
                        "1416 [D loss: (1.677)(R 1.673, F 1.680)]  [G loss: -39.898] \n",
                        "1417 [D loss: (1.678)(R 1.675, F 1.682)]  [G loss: -39.941] \n",
                        "1418 [D loss: (1.680)(R 1.676, F 1.684)]  [G loss: -39.984] \n",
                        "1419 [D loss: (1.681)(R 1.677, F 1.685)]  [G loss: -40.027] \n",
                        "1420 [D loss: (1.683)(R 1.679, F 1.687)]  [G loss: -40.071] \n",
                        "1421 [D loss: (1.684)(R 1.681, F 1.688)]  [G loss: -40.116] \n",
                        "1422 [D loss: (1.687)(R 1.683, F 1.691)]  [G loss: -40.160] \n",
                        "1423 [D loss: (1.689)(R 1.685, F 1.693)]  [G loss: -40.206] \n",
                        "1424 [D loss: (1.691)(R 1.687, F 1.695)]  [G loss: -40.251] \n",
                        "1425 [D loss: (1.693)(R 1.689, F 1.697)]  [G loss: -40.296] \n",
                        "1426 [D loss: (1.695)(R 1.691, F 1.699)]  [G loss: -40.342] \n",
                        "1427 [D loss: (1.697)(R 1.694, F 1.701)]  [G loss: -40.385] \n",
                        "1428 [D loss: (1.699)(R 1.695, F 1.703)]  [G loss: -40.430] \n",
                        "1429 [D loss: (1.701)(R 1.697, F 1.704)]  [G loss: -40.476] \n",
                        "1430 [D loss: (1.703)(R 1.700, F 1.707)]  [G loss: -40.518] \n",
                        "1431 [D loss: (1.705)(R 1.701, F 1.709)]  [G loss: -40.563] \n",
                        "1432 [D loss: (1.707)(R 1.703, F 1.710)]  [G loss: -40.608] \n",
                        "1433 [D loss: (1.709)(R 1.705, F 1.713)]  [G loss: -40.652] \n",
                        "1434 [D loss: (1.710)(R 1.707, F 1.714)]  [G loss: -40.696] \n",
                        "1435 [D loss: (1.712)(R 1.708, F 1.716)]  [G loss: -40.740] \n",
                        "1436 [D loss: (1.714)(R 1.710, F 1.718)]  [G loss: -40.786] \n",
                        "1437 [D loss: (1.716)(R 1.712, F 1.720)]  [G loss: -40.828] \n",
                        "1438 [D loss: (1.718)(R 1.714, F 1.722)]  [G loss: -40.873] \n",
                        "1439 [D loss: (1.719)(R 1.716, F 1.723)]  [G loss: -40.918] \n",
                        "1440 [D loss: (1.722)(R 1.718, F 1.726)]  [G loss: -40.961] \n",
                        "1441 [D loss: (1.723)(R 1.720, F 1.727)]  [G loss: -41.006] \n",
                        "1442 [D loss: (1.725)(R 1.721, F 1.729)]  [G loss: -41.051] \n",
                        "1443 [D loss: (1.727)(R 1.723, F 1.731)]  [G loss: -41.095] \n",
                        "1444 [D loss: (1.729)(R 1.725, F 1.733)]  [G loss: -41.140] \n",
                        "1445 [D loss: (1.730)(R 1.726, F 1.734)]  [G loss: -41.185] \n",
                        "1446 [D loss: (1.732)(R 1.728, F 1.736)]  [G loss: -41.230] \n",
                        "1447 [D loss: (1.734)(R 1.730, F 1.738)]  [G loss: -41.276] \n",
                        "1448 [D loss: (1.736)(R 1.732, F 1.740)]  [G loss: -41.322] \n",
                        "1449 [D loss: (1.739)(R 1.735, F 1.743)]  [G loss: -41.365] \n",
                        "1450 [D loss: (1.741)(R 1.737, F 1.745)]  [G loss: -41.412] \n",
                        "1451 [D loss: (1.744)(R 1.740, F 1.748)]  [G loss: -41.457] \n",
                        "1452 [D loss: (1.746)(R 1.742, F 1.750)]  [G loss: -41.499] \n",
                        "1453 [D loss: (1.748)(R 1.744, F 1.752)]  [G loss: -41.545] \n",
                        "1454 [D loss: (1.751)(R 1.747, F 1.755)]  [G loss: -41.586] \n",
                        "1455 [D loss: (1.753)(R 1.749, F 1.757)]  [G loss: -41.628] \n",
                        "1456 [D loss: (1.755)(R 1.751, F 1.758)]  [G loss: -41.672] \n",
                        "1457 [D loss: (1.757)(R 1.753, F 1.761)]  [G loss: -41.713] \n",
                        "1458 [D loss: (1.758)(R 1.755, F 1.762)]  [G loss: -41.755] \n",
                        "1459 [D loss: (1.760)(R 1.756, F 1.764)]  [G loss: -41.799] \n",
                        "1460 [D loss: (1.762)(R 1.758, F 1.766)]  [G loss: -41.843] \n",
                        "1461 [D loss: (1.764)(R 1.760, F 1.768)]  [G loss: -41.885] \n",
                        "1462 [D loss: (1.766)(R 1.762, F 1.770)]  [G loss: -41.927] \n",
                        "1463 [D loss: (1.767)(R 1.763, F 1.771)]  [G loss: -41.969] \n",
                        "1464 [D loss: (1.769)(R 1.765, F 1.773)]  [G loss: -42.011] \n",
                        "1465 [D loss: (1.770)(R 1.767, F 1.774)]  [G loss: -42.054] \n",
                        "1466 [D loss: (1.772)(R 1.768, F 1.776)]  [G loss: -42.095] \n",
                        "1467 [D loss: (1.774)(R 1.770, F 1.777)]  [G loss: -42.138] \n",
                        "1468 [D loss: (1.776)(R 1.772, F 1.779)]  [G loss: -42.181] \n",
                        "1469 [D loss: (1.777)(R 1.773, F 1.781)]  [G loss: -42.224] \n",
                        "1470 [D loss: (1.779)(R 1.775, F 1.783)]  [G loss: -42.268] \n",
                        "1471 [D loss: (1.781)(R 1.777, F 1.785)]  [G loss: -42.311] \n",
                        "1472 [D loss: (1.783)(R 1.779, F 1.787)]  [G loss: -42.355] \n",
                        "1473 [D loss: (1.785)(R 1.781, F 1.789)]  [G loss: -42.398] \n",
                        "1474 [D loss: (1.787)(R 1.783, F 1.790)]  [G loss: -42.441] \n",
                        "1475 [D loss: (1.788)(R 1.784, F 1.792)]  [G loss: -42.485] \n",
                        "1476 [D loss: (1.790)(R 1.786, F 1.794)]  [G loss: -42.529] \n",
                        "1477 [D loss: (1.792)(R 1.788, F 1.796)]  [G loss: -42.573] \n",
                        "1478 [D loss: (1.794)(R 1.790, F 1.798)]  [G loss: -42.616] \n",
                        "1479 [D loss: (1.796)(R 1.792, F 1.799)]  [G loss: -42.659] \n",
                        "1480 [D loss: (1.797)(R 1.793, F 1.801)]  [G loss: -42.703] \n",
                        "1481 [D loss: (1.800)(R 1.796, F 1.804)]  [G loss: -42.744] \n",
                        "1482 [D loss: (1.802)(R 1.798, F 1.805)]  [G loss: -42.786] \n",
                        "1483 [D loss: (1.804)(R 1.800, F 1.808)]  [G loss: -42.827] \n",
                        "1484 [D loss: (1.806)(R 1.802, F 1.809)]  [G loss: -42.868] \n",
                        "1485 [D loss: (1.807)(R 1.804, F 1.811)]  [G loss: -42.907] \n",
                        "1486 [D loss: (1.809)(R 1.806, F 1.813)]  [G loss: -42.947] \n",
                        "1487 [D loss: (1.811)(R 1.807, F 1.815)]  [G loss: -42.988] \n",
                        "1488 [D loss: (1.813)(R 1.809, F 1.816)]  [G loss: -43.029] \n",
                        "1489 [D loss: (1.814)(R 1.811, F 1.818)]  [G loss: -43.070] \n",
                        "1490 [D loss: (1.816)(R 1.812, F 1.820)]  [G loss: -43.112] \n",
                        "1491 [D loss: (1.818)(R 1.814, F 1.822)]  [G loss: -43.154] \n",
                        "1492 [D loss: (1.819)(R 1.816, F 1.823)]  [G loss: -43.195] \n",
                        "1493 [D loss: (1.821)(R 1.817, F 1.825)]  [G loss: -43.237] \n",
                        "1494 [D loss: (1.823)(R 1.819, F 1.826)]  [G loss: -43.278] \n",
                        "1495 [D loss: (1.825)(R 1.821, F 1.828)]  [G loss: -43.320] \n",
                        "1496 [D loss: (1.826)(R 1.822, F 1.830)]  [G loss: -43.362] \n",
                        "1497 [D loss: (1.828)(R 1.824, F 1.831)]  [G loss: -43.404] \n",
                        "1498 [D loss: (1.830)(R 1.826, F 1.833)]  [G loss: -43.447] \n",
                        "1499 [D loss: (1.831)(R 1.827, F 1.835)]  [G loss: -43.489] \n",
                        "1500 [D loss: (1.833)(R 1.829, F 1.837)]  [G loss: -43.532] \n",
                        "1501 [D loss: (1.835)(R 1.831, F 1.839)]  [G loss: -43.575] \n",
                        "1502 [D loss: (1.837)(R 1.833, F 1.841)]  [G loss: -43.618] \n",
                        "1503 [D loss: (1.839)(R 1.835, F 1.842)]  [G loss: -43.661] \n",
                        "1504 [D loss: (1.841)(R 1.837, F 1.845)]  [G loss: -43.704] \n",
                        "1505 [D loss: (1.843)(R 1.839, F 1.847)]  [G loss: -43.748] \n",
                        "1506 [D loss: (1.845)(R 1.841, F 1.848)]  [G loss: -43.792] \n",
                        "1507 [D loss: (1.847)(R 1.843, F 1.851)]  [G loss: -43.837] \n",
                        "1508 [D loss: (1.849)(R 1.845, F 1.853)]  [G loss: -43.881] \n",
                        "1509 [D loss: (1.851)(R 1.847, F 1.855)]  [G loss: -43.926] \n",
                        "1510 [D loss: (1.853)(R 1.849, F 1.857)]  [G loss: -43.970] \n",
                        "1511 [D loss: (1.855)(R 1.852, F 1.859)]  [G loss: -44.012] \n",
                        "1512 [D loss: (1.857)(R 1.853, F 1.861)]  [G loss: -44.053] \n",
                        "1513 [D loss: (1.859)(R 1.855, F 1.863)]  [G loss: -44.094] \n",
                        "1514 [D loss: (1.861)(R 1.857, F 1.865)]  [G loss: -44.133] \n",
                        "1515 [D loss: (1.862)(R 1.859, F 1.866)]  [G loss: -44.173] \n",
                        "1516 [D loss: (1.864)(R 1.860, F 1.868)]  [G loss: -44.212] \n",
                        "1517 [D loss: (1.865)(R 1.862, F 1.869)]  [G loss: -44.251] \n",
                        "1518 [D loss: (1.867)(R 1.863, F 1.870)]  [G loss: -44.290] \n",
                        "1519 [D loss: (1.868)(R 1.865, F 1.872)]  [G loss: -44.330] \n",
                        "1520 [D loss: (1.870)(R 1.866, F 1.873)]  [G loss: -44.370] \n",
                        "1521 [D loss: (1.871)(R 1.868, F 1.875)]  [G loss: -44.410] \n",
                        "1522 [D loss: (1.873)(R 1.869, F 1.876)]  [G loss: -44.452] \n",
                        "1523 [D loss: (1.875)(R 1.871, F 1.879)]  [G loss: -44.493] \n",
                        "1524 [D loss: (1.876)(R 1.873, F 1.880)]  [G loss: -44.535] \n",
                        "1525 [D loss: (1.878)(R 1.874, F 1.881)]  [G loss: -44.578] \n",
                        "1526 [D loss: (1.880)(R 1.876, F 1.884)]  [G loss: -44.622] \n",
                        "1527 [D loss: (1.882)(R 1.878, F 1.886)]  [G loss: -44.666] \n",
                        "1528 [D loss: (1.884)(R 1.880, F 1.888)]  [G loss: -44.710] \n",
                        "1529 [D loss: (1.886)(R 1.882, F 1.890)]  [G loss: -44.757] \n",
                        "1530 [D loss: (1.890)(R 1.886, F 1.894)]  [G loss: -44.801] \n",
                        "1531 [D loss: (1.892)(R 1.888, F 1.896)]  [G loss: -44.845] \n",
                        "1532 [D loss: (1.895)(R 1.891, F 1.899)]  [G loss: -44.888] \n",
                        "1533 [D loss: (1.897)(R 1.893, F 1.901)]  [G loss: -44.931] \n",
                        "1534 [D loss: (1.899)(R 1.896, F 1.903)]  [G loss: -44.972] \n",
                        "1535 [D loss: (1.901)(R 1.898, F 1.905)]  [G loss: -45.011] \n",
                        "1536 [D loss: (1.903)(R 1.899, F 1.906)]  [G loss: -45.050] \n",
                        "1537 [D loss: (1.904)(R 1.900, F 1.908)]  [G loss: -45.088] \n",
                        "1538 [D loss: (1.905)(R 1.902, F 1.909)]  [G loss: -45.126] \n",
                        "1539 [D loss: (1.906)(R 1.903, F 1.910)]  [G loss: -45.165] \n",
                        "1540 [D loss: (1.908)(R 1.904, F 1.911)]  [G loss: -45.202] \n",
                        "1541 [D loss: (1.909)(R 1.905, F 1.913)]  [G loss: -45.243] \n",
                        "1542 [D loss: (1.911)(R 1.907, F 1.914)]  [G loss: -45.284] \n",
                        "1543 [D loss: (1.912)(R 1.909, F 1.916)]  [G loss: -45.324] \n",
                        "1544 [D loss: (1.914)(R 1.910, F 1.918)]  [G loss: -45.367] \n",
                        "1545 [D loss: (1.916)(R 1.912, F 1.920)]  [G loss: -45.408] \n",
                        "1546 [D loss: (1.918)(R 1.914, F 1.922)]  [G loss: -45.452] \n",
                        "1547 [D loss: (1.919)(R 1.916, F 1.923)]  [G loss: -45.496] \n",
                        "1548 [D loss: (1.922)(R 1.918, F 1.926)]  [G loss: -45.539] \n",
                        "1549 [D loss: (1.924)(R 1.920, F 1.928)]  [G loss: -45.582] \n",
                        "1550 [D loss: (1.926)(R 1.922, F 1.930)]  [G loss: -45.625] \n",
                        "1551 [D loss: (1.928)(R 1.924, F 1.932)]  [G loss: -45.669] \n",
                        "1552 [D loss: (1.930)(R 1.926, F 1.934)]  [G loss: -45.712] \n",
                        "1553 [D loss: (1.932)(R 1.928, F 1.936)]  [G loss: -45.754] \n",
                        "1554 [D loss: (1.934)(R 1.930, F 1.938)]  [G loss: -45.798] \n",
                        "1555 [D loss: (1.937)(R 1.933, F 1.940)]  [G loss: -45.839] \n",
                        "1556 [D loss: (1.939)(R 1.935, F 1.942)]  [G loss: -45.881] \n",
                        "1557 [D loss: (1.941)(R 1.937, F 1.944)]  [G loss: -45.923] \n",
                        "1558 [D loss: (1.943)(R 1.939, F 1.946)]  [G loss: -45.963] \n",
                        "1559 [D loss: (1.944)(R 1.941, F 1.948)]  [G loss: -46.004] \n",
                        "1560 [D loss: (1.946)(R 1.943, F 1.950)]  [G loss: -46.045] \n",
                        "1561 [D loss: (1.948)(R 1.944, F 1.952)]  [G loss: -46.084] \n",
                        "1562 [D loss: (1.950)(R 1.946, F 1.953)]  [G loss: -46.122] \n",
                        "1563 [D loss: (1.951)(R 1.947, F 1.955)]  [G loss: -46.160] \n",
                        "1564 [D loss: (1.953)(R 1.949, F 1.956)]  [G loss: -46.198] \n",
                        "1565 [D loss: (1.954)(R 1.950, F 1.958)]  [G loss: -46.235] \n",
                        "1566 [D loss: (1.955)(R 1.952, F 1.959)]  [G loss: -46.273] \n",
                        "1567 [D loss: (1.957)(R 1.953, F 1.960)]  [G loss: -46.310] \n",
                        "1568 [D loss: (1.959)(R 1.955, F 1.962)]  [G loss: -46.348] \n",
                        "1569 [D loss: (1.960)(R 1.956, F 1.964)]  [G loss: -46.386] \n",
                        "1570 [D loss: (1.961)(R 1.958, F 1.965)]  [G loss: -46.424] \n",
                        "1571 [D loss: (1.963)(R 1.959, F 1.967)]  [G loss: -46.463] \n",
                        "1572 [D loss: (1.964)(R 1.961, F 1.968)]  [G loss: -46.503] \n",
                        "1573 [D loss: (1.966)(R 1.962, F 1.969)]  [G loss: -46.542] \n",
                        "1574 [D loss: (1.968)(R 1.964, F 1.971)]  [G loss: -46.584] \n",
                        "1575 [D loss: (1.969)(R 1.966, F 1.973)]  [G loss: -46.625] \n",
                        "1576 [D loss: (1.971)(R 1.967, F 1.975)]  [G loss: -46.667] \n",
                        "1577 [D loss: (1.973)(R 1.969, F 1.977)]  [G loss: -46.709] \n",
                        "1578 [D loss: (1.975)(R 1.971, F 1.979)]  [G loss: -46.752] \n",
                        "1579 [D loss: (1.977)(R 1.973, F 1.981)]  [G loss: -46.794] \n",
                        "1580 [D loss: (1.979)(R 1.975, F 1.983)]  [G loss: -46.837] \n",
                        "1581 [D loss: (1.982)(R 1.978, F 1.986)]  [G loss: -46.879] \n",
                        "1582 [D loss: (1.984)(R 1.980, F 1.988)]  [G loss: -46.920] \n",
                        "1583 [D loss: (1.986)(R 1.982, F 1.990)]  [G loss: -46.962] \n",
                        "1584 [D loss: (1.988)(R 1.984, F 1.992)]  [G loss: -47.002] \n",
                        "1585 [D loss: (1.990)(R 1.986, F 1.994)]  [G loss: -47.042] \n",
                        "1586 [D loss: (1.992)(R 1.988, F 1.996)]  [G loss: -47.082] \n",
                        "1587 [D loss: (1.993)(R 1.990, F 1.997)]  [G loss: -47.122] \n",
                        "1588 [D loss: (1.996)(R 1.992, F 1.999)]  [G loss: -47.160] \n",
                        "1589 [D loss: (1.997)(R 1.994, F 2.001)]  [G loss: -47.198] \n",
                        "1590 [D loss: (1.999)(R 1.995, F 2.002)]  [G loss: -47.236] \n",
                        "1591 [D loss: (2.001)(R 1.997, F 2.004)]  [G loss: -47.273] \n",
                        "1592 [D loss: (2.002)(R 1.998, F 2.005)]  [G loss: -47.311] \n",
                        "1593 [D loss: (2.004)(R 2.000, F 2.007)]  [G loss: -47.349] \n",
                        "1594 [D loss: (2.005)(R 2.001, F 2.009)]  [G loss: -47.387] \n",
                        "1595 [D loss: (2.007)(R 2.003, F 2.010)]  [G loss: -47.426] \n",
                        "1596 [D loss: (2.008)(R 2.005, F 2.012)]  [G loss: -47.464] \n",
                        "1597 [D loss: (2.010)(R 2.006, F 2.013)]  [G loss: -47.503] \n",
                        "1598 [D loss: (2.011)(R 2.008, F 2.015)]  [G loss: -47.543] \n",
                        "1599 [D loss: (2.013)(R 2.009, F 2.017)]  [G loss: -47.584] \n",
                        "1600 [D loss: (2.015)(R 2.011, F 2.019)]  [G loss: -47.625] \n",
                        "1601 [D loss: (2.017)(R 2.013, F 2.021)]  [G loss: -47.667] \n",
                        "1602 [D loss: (2.018)(R 2.014, F 2.022)]  [G loss: -47.709] \n",
                        "1603 [D loss: (2.021)(R 2.017, F 2.024)]  [G loss: -47.751] \n",
                        "1604 [D loss: (2.023)(R 2.019, F 2.026)]  [G loss: -47.795] \n",
                        "1605 [D loss: (2.025)(R 2.021, F 2.029)]  [G loss: -47.839] \n",
                        "1606 [D loss: (2.028)(R 2.024, F 2.031)]  [G loss: -47.882] \n",
                        "1607 [D loss: (2.030)(R 2.026, F 2.034)]  [G loss: -47.925] \n",
                        "1608 [D loss: (2.032)(R 2.028, F 2.036)]  [G loss: -47.969] \n",
                        "1609 [D loss: (2.035)(R 2.031, F 2.039)]  [G loss: -48.012] \n",
                        "1610 [D loss: (2.037)(R 2.033, F 2.041)]  [G loss: -48.053] \n",
                        "1611 [D loss: (2.039)(R 2.036, F 2.043)]  [G loss: -48.095] \n",
                        "1612 [D loss: (2.042)(R 2.038, F 2.045)]  [G loss: -48.135] \n",
                        "1613 [D loss: (2.043)(R 2.039, F 2.047)]  [G loss: -48.174] \n",
                        "1614 [D loss: (2.045)(R 2.042, F 2.049)]  [G loss: -48.211] \n",
                        "1615 [D loss: (2.047)(R 2.043, F 2.050)]  [G loss: -48.249] \n",
                        "1616 [D loss: (2.048)(R 2.045, F 2.052)]  [G loss: -48.287] \n",
                        "1617 [D loss: (2.050)(R 2.046, F 2.053)]  [G loss: -48.325] \n",
                        "1618 [D loss: (2.051)(R 2.048, F 2.055)]  [G loss: -48.364] \n",
                        "1619 [D loss: (2.053)(R 2.050, F 2.057)]  [G loss: -48.402] \n",
                        "1620 [D loss: (2.055)(R 2.051, F 2.058)]  [G loss: -48.442] \n",
                        "1621 [D loss: (2.057)(R 2.053, F 2.061)]  [G loss: -48.481] \n",
                        "1622 [D loss: (2.059)(R 2.055, F 2.062)]  [G loss: -48.520] \n",
                        "1623 [D loss: (2.060)(R 2.057, F 2.064)]  [G loss: -48.561] \n",
                        "1624 [D loss: (2.062)(R 2.058, F 2.066)]  [G loss: -48.602] \n",
                        "1625 [D loss: (2.064)(R 2.060, F 2.068)]  [G loss: -48.642] \n",
                        "1626 [D loss: (2.066)(R 2.062, F 2.069)]  [G loss: -48.683] \n",
                        "1627 [D loss: (2.068)(R 2.064, F 2.071)]  [G loss: -48.723] \n",
                        "1628 [D loss: (2.069)(R 2.066, F 2.073)]  [G loss: -48.764] \n",
                        "1629 [D loss: (2.071)(R 2.068, F 2.075)]  [G loss: -48.805] \n",
                        "1630 [D loss: (2.073)(R 2.070, F 2.077)]  [G loss: -48.846] \n",
                        "1631 [D loss: (2.076)(R 2.072, F 2.079)]  [G loss: -48.886] \n",
                        "1632 [D loss: (2.078)(R 2.074, F 2.081)]  [G loss: -48.927] \n",
                        "1633 [D loss: (2.080)(R 2.076, F 2.083)]  [G loss: -48.963] \n",
                        "1634 [D loss: (2.082)(R 2.078, F 2.086)]  [G loss: -49.004] \n",
                        "1635 [D loss: (2.084)(R 2.081, F 2.088)]  [G loss: -49.043] \n",
                        "1636 [D loss: (2.086)(R 2.083, F 2.090)]  [G loss: -49.082] \n",
                        "1637 [D loss: (2.088)(R 2.085, F 2.092)]  [G loss: -49.121] \n",
                        "1638 [D loss: (2.090)(R 2.086, F 2.094)]  [G loss: -49.160] \n",
                        "1639 [D loss: (2.092)(R 2.088, F 2.095)]  [G loss: -49.198] \n",
                        "1640 [D loss: (2.093)(R 2.090, F 2.097)]  [G loss: -49.235] \n",
                        "1641 [D loss: (2.095)(R 2.092, F 2.099)]  [G loss: -49.270] \n",
                        "1642 [D loss: (2.096)(R 2.093, F 2.100)]  [G loss: -49.307] \n",
                        "1643 [D loss: (2.098)(R 2.095, F 2.102)]  [G loss: -49.343] \n",
                        "1644 [D loss: (2.100)(R 2.096, F 2.103)]  [G loss: -49.379] \n",
                        "1645 [D loss: (2.101)(R 2.098, F 2.105)]  [G loss: -49.415] \n",
                        "1646 [D loss: (2.103)(R 2.099, F 2.106)]  [G loss: -49.451] \n",
                        "1647 [D loss: (2.104)(R 2.100, F 2.107)]  [G loss: -49.487] \n",
                        "1648 [D loss: (2.106)(R 2.102, F 2.109)]  [G loss: -49.523] \n",
                        "1649 [D loss: (2.107)(R 2.104, F 2.111)]  [G loss: -49.560] \n",
                        "1650 [D loss: (2.109)(R 2.105, F 2.112)]  [G loss: -49.598] \n",
                        "1651 [D loss: (2.110)(R 2.107, F 2.114)]  [G loss: -49.636] \n",
                        "1652 [D loss: (2.112)(R 2.108, F 2.115)]  [G loss: -49.675] \n",
                        "1653 [D loss: (2.113)(R 2.110, F 2.117)]  [G loss: -49.714] \n",
                        "1654 [D loss: (2.114)(R 2.111, F 2.118)]  [G loss: -49.755] \n",
                        "1655 [D loss: (2.118)(R 2.114, F 2.121)]  [G loss: -49.795] \n",
                        "1656 [D loss: (2.120)(R 2.116, F 2.123)]  [G loss: -49.833] \n",
                        "1657 [D loss: (2.121)(R 2.118, F 2.125)]  [G loss: -49.873] \n",
                        "1658 [D loss: (2.123)(R 2.119, F 2.127)]  [G loss: -49.914] \n",
                        "1659 [D loss: (2.126)(R 2.122, F 2.130)]  [G loss: -49.953] \n",
                        "1660 [D loss: (2.128)(R 2.124, F 2.131)]  [G loss: -49.991] \n",
                        "1661 [D loss: (2.129)(R 2.126, F 2.133)]  [G loss: -50.031] \n",
                        "1662 [D loss: (2.132)(R 2.128, F 2.135)]  [G loss: -50.070] \n",
                        "1663 [D loss: (2.134)(R 2.130, F 2.137)]  [G loss: -50.109] \n",
                        "1664 [D loss: (2.136)(R 2.132, F 2.139)]  [G loss: -50.148] \n",
                        "1665 [D loss: (2.137)(R 2.134, F 2.141)]  [G loss: -50.188] \n",
                        "1666 [D loss: (2.139)(R 2.136, F 2.143)]  [G loss: -50.227] \n",
                        "1667 [D loss: (2.141)(R 2.137, F 2.145)]  [G loss: -50.265] \n",
                        "1668 [D loss: (2.143)(R 2.140, F 2.147)]  [G loss: -50.303] \n",
                        "1669 [D loss: (2.145)(R 2.141, F 2.148)]  [G loss: -50.340] \n",
                        "1670 [D loss: (2.147)(R 2.143, F 2.150)]  [G loss: -50.377] \n",
                        "1671 [D loss: (2.148)(R 2.145, F 2.152)]  [G loss: -50.413] \n",
                        "1672 [D loss: (2.150)(R 2.146, F 2.153)]  [G loss: -50.449] \n",
                        "1673 [D loss: (2.151)(R 2.148, F 2.155)]  [G loss: -50.485] \n",
                        "1674 [D loss: (2.153)(R 2.149, F 2.156)]  [G loss: -50.520] \n",
                        "1675 [D loss: (2.154)(R 2.151, F 2.158)]  [G loss: -50.557] \n",
                        "1676 [D loss: (2.156)(R 2.152, F 2.159)]  [G loss: -50.593] \n",
                        "1677 [D loss: (2.157)(R 2.154, F 2.161)]  [G loss: -50.631] \n",
                        "1678 [D loss: (2.159)(R 2.155, F 2.163)]  [G loss: -50.669] \n",
                        "1679 [D loss: (2.161)(R 2.157, F 2.165)]  [G loss: -50.709] \n",
                        "1680 [D loss: (2.163)(R 2.159, F 2.166)]  [G loss: -50.749] \n",
                        "1681 [D loss: (2.165)(R 2.161, F 2.168)]  [G loss: -50.790] \n",
                        "1682 [D loss: (2.166)(R 2.163, F 2.170)]  [G loss: -50.830] \n",
                        "1683 [D loss: (2.168)(R 2.165, F 2.172)]  [G loss: -50.872] \n",
                        "1684 [D loss: (2.170)(R 2.167, F 2.174)]  [G loss: -50.912] \n",
                        "1685 [D loss: (2.172)(R 2.168, F 2.176)]  [G loss: -50.955] \n",
                        "1686 [D loss: (2.175)(R 2.171, F 2.179)]  [G loss: -50.997] \n",
                        "1687 [D loss: (2.177)(R 2.173, F 2.181)]  [G loss: -51.038] \n",
                        "1688 [D loss: (2.179)(R 2.176, F 2.183)]  [G loss: -51.079] \n",
                        "1689 [D loss: (2.182)(R 2.178, F 2.185)]  [G loss: -51.121] \n",
                        "1690 [D loss: (2.184)(R 2.180, F 2.188)]  [G loss: -51.161] \n",
                        "1691 [D loss: (2.186)(R 2.182, F 2.190)]  [G loss: -51.202] \n",
                        "1692 [D loss: (2.188)(R 2.185, F 2.192)]  [G loss: -51.243] \n",
                        "1693 [D loss: (2.191)(R 2.187, F 2.194)]  [G loss: -51.284] \n",
                        "1694 [D loss: (2.193)(R 2.189, F 2.197)]  [G loss: -51.323] \n",
                        "1695 [D loss: (2.195)(R 2.191, F 2.199)]  [G loss: -51.362] \n",
                        "1696 [D loss: (2.197)(R 2.193, F 2.200)]  [G loss: -51.399] \n",
                        "1697 [D loss: (2.198)(R 2.195, F 2.202)]  [G loss: -51.435] \n",
                        "1698 [D loss: (2.200)(R 2.197, F 2.204)]  [G loss: -51.470] \n",
                        "1699 [D loss: (2.202)(R 2.198, F 2.205)]  [G loss: -51.506] \n",
                        "1700 [D loss: (2.204)(R 2.200, F 2.207)]  [G loss: -51.539] \n",
                        "1701 [D loss: (2.205)(R 2.201, F 2.208)]  [G loss: -51.573] \n",
                        "1702 [D loss: (2.206)(R 2.203, F 2.210)]  [G loss: -51.607] \n",
                        "1703 [D loss: (2.208)(R 2.204, F 2.211)]  [G loss: -51.641] \n",
                        "1704 [D loss: (2.209)(R 2.206, F 2.213)]  [G loss: -51.676] \n",
                        "1705 [D loss: (2.211)(R 2.208, F 2.215)]  [G loss: -51.713] \n",
                        "1706 [D loss: (2.213)(R 2.209, F 2.216)]  [G loss: -51.749] \n",
                        "1707 [D loss: (2.214)(R 2.211, F 2.218)]  [G loss: -51.786] \n",
                        "1708 [D loss: (2.216)(R 2.212, F 2.219)]  [G loss: -51.825] \n",
                        "1709 [D loss: (2.218)(R 2.214, F 2.222)]  [G loss: -51.863] \n",
                        "1710 [D loss: (2.220)(R 2.216, F 2.223)]  [G loss: -51.901] \n",
                        "1711 [D loss: (2.221)(R 2.218, F 2.225)]  [G loss: -51.939] \n",
                        "1712 [D loss: (2.223)(R 2.219, F 2.227)]  [G loss: -51.979] \n",
                        "1713 [D loss: (2.226)(R 2.222, F 2.229)]  [G loss: -52.016] \n",
                        "1714 [D loss: (2.227)(R 2.224, F 2.231)]  [G loss: -52.055] \n",
                        "1715 [D loss: (2.229)(R 2.226, F 2.233)]  [G loss: -52.093] \n",
                        "1716 [D loss: (2.231)(R 2.228, F 2.235)]  [G loss: -52.130] \n",
                        "1717 [D loss: (2.233)(R 2.229, F 2.237)]  [G loss: -52.168] \n",
                        "1718 [D loss: (2.235)(R 2.232, F 2.239)]  [G loss: -52.204] \n",
                        "1719 [D loss: (2.237)(R 2.233, F 2.240)]  [G loss: -52.243] \n",
                        "1720 [D loss: (2.239)(R 2.236, F 2.243)]  [G loss: -52.279] \n",
                        "1721 [D loss: (2.241)(R 2.237, F 2.245)]  [G loss: -52.317] \n",
                        "1722 [D loss: (2.243)(R 2.239, F 2.246)]  [G loss: -52.355] \n",
                        "1723 [D loss: (2.245)(R 2.241, F 2.248)]  [G loss: -52.393] \n",
                        "1724 [D loss: (2.247)(R 2.243, F 2.250)]  [G loss: -52.430] \n",
                        "1725 [D loss: (2.248)(R 2.245, F 2.252)]  [G loss: -52.467] \n",
                        "1726 [D loss: (2.250)(R 2.247, F 2.254)]  [G loss: -52.502] \n",
                        "1727 [D loss: (2.252)(R 2.248, F 2.255)]  [G loss: -52.538] \n",
                        "1728 [D loss: (2.254)(R 2.250, F 2.257)]  [G loss: -52.572] \n",
                        "1729 [D loss: (2.255)(R 2.252, F 2.259)]  [G loss: -52.607] \n",
                        "1730 [D loss: (2.257)(R 2.253, F 2.260)]  [G loss: -52.641] \n",
                        "1731 [D loss: (2.258)(R 2.255, F 2.262)]  [G loss: -52.675] \n",
                        "1732 [D loss: (2.260)(R 2.256, F 2.263)]  [G loss: -52.711] \n",
                        "1733 [D loss: (2.261)(R 2.258, F 2.265)]  [G loss: -52.746] \n",
                        "1734 [D loss: (2.263)(R 2.259, F 2.266)]  [G loss: -52.783] \n",
                        "1735 [D loss: (2.265)(R 2.261, F 2.268)]  [G loss: -52.819] \n",
                        "1736 [D loss: (2.266)(R 2.263, F 2.270)]  [G loss: -52.858] \n",
                        "1737 [D loss: (2.268)(R 2.265, F 2.272)]  [G loss: -52.895] \n",
                        "1738 [D loss: (2.270)(R 2.266, F 2.274)]  [G loss: -52.933] \n",
                        "1739 [D loss: (2.272)(R 2.268, F 2.275)]  [G loss: -52.971] \n",
                        "1740 [D loss: (2.273)(R 2.270, F 2.277)]  [G loss: -53.009] \n",
                        "1741 [D loss: (2.275)(R 2.271, F 2.279)]  [G loss: -53.048] \n",
                        "1742 [D loss: (2.277)(R 2.273, F 2.281)]  [G loss: -53.084] \n",
                        "1743 [D loss: (2.278)(R 2.275, F 2.282)]  [G loss: -53.124] \n",
                        "1744 [D loss: (2.281)(R 2.277, F 2.285)]  [G loss: -53.162] \n",
                        "1745 [D loss: (2.283)(R 2.279, F 2.287)]  [G loss: -53.201] \n",
                        "1746 [D loss: (2.285)(R 2.281, F 2.288)]  [G loss: -53.240] \n",
                        "1747 [D loss: (2.287)(R 2.284, F 2.291)]  [G loss: -53.279] \n",
                        "1748 [D loss: (2.289)(R 2.286, F 2.293)]  [G loss: -53.317] \n",
                        "1749 [D loss: (2.291)(R 2.288, F 2.295)]  [G loss: -53.356] \n",
                        "1750 [D loss: (2.293)(R 2.290, F 2.297)]  [G loss: -53.396] \n",
                        "1751 [D loss: (2.296)(R 2.292, F 2.299)]  [G loss: -53.434] \n",
                        "1752 [D loss: (2.298)(R 2.294, F 2.301)]  [G loss: -53.472] \n",
                        "1753 [D loss: (2.299)(R 2.296, F 2.303)]  [G loss: -53.510] \n",
                        "1754 [D loss: (2.302)(R 2.298, F 2.306)]  [G loss: -53.547] \n",
                        "1755 [D loss: (2.304)(R 2.300, F 2.307)]  [G loss: -53.582] \n",
                        "1756 [D loss: (2.306)(R 2.302, F 2.309)]  [G loss: -53.618] \n",
                        "1757 [D loss: (2.307)(R 2.304, F 2.311)]  [G loss: -53.654] \n",
                        "1758 [D loss: (2.309)(R 2.306, F 2.313)]  [G loss: -53.688] \n",
                        "1759 [D loss: (2.311)(R 2.307, F 2.314)]  [G loss: -53.722] \n",
                        "1760 [D loss: (2.312)(R 2.309, F 2.316)]  [G loss: -53.756] \n",
                        "1761 [D loss: (2.314)(R 2.310, F 2.317)]  [G loss: -53.790] \n",
                        "1762 [D loss: (2.315)(R 2.312, F 2.318)]  [G loss: -53.824] \n",
                        "1763 [D loss: (2.316)(R 2.313, F 2.320)]  [G loss: -53.859] \n",
                        "1764 [D loss: (2.318)(R 2.315, F 2.321)]  [G loss: -53.894] \n",
                        "1765 [D loss: (2.319)(R 2.316, F 2.323)]  [G loss: -53.930] \n",
                        "1766 [D loss: (2.321)(R 2.318, F 2.325)]  [G loss: -53.966] \n",
                        "1767 [D loss: (2.323)(R 2.319, F 2.326)]  [G loss: -54.001] \n",
                        "1768 [D loss: (2.324)(R 2.321, F 2.328)]  [G loss: -54.037] \n",
                        "1769 [D loss: (2.326)(R 2.323, F 2.330)]  [G loss: -54.073] \n",
                        "1770 [D loss: (2.328)(R 2.324, F 2.331)]  [G loss: -54.110] \n",
                        "1771 [D loss: (2.329)(R 2.326, F 2.333)]  [G loss: -54.147] \n",
                        "1772 [D loss: (2.331)(R 2.328, F 2.335)]  [G loss: -54.183] \n",
                        "1773 [D loss: (2.333)(R 2.329, F 2.337)]  [G loss: -54.220] \n",
                        "1774 [D loss: (2.335)(R 2.331, F 2.339)]  [G loss: -54.256] \n",
                        "1775 [D loss: (2.337)(R 2.333, F 2.340)]  [G loss: -54.292] \n",
                        "1776 [D loss: (2.339)(R 2.335, F 2.342)]  [G loss: -54.327] \n",
                        "1777 [D loss: (2.340)(R 2.337, F 2.344)]  [G loss: -54.364] \n",
                        "1778 [D loss: (2.342)(R 2.339, F 2.346)]  [G loss: -54.398] \n",
                        "1779 [D loss: (2.344)(R 2.341, F 2.348)]  [G loss: -54.434] \n",
                        "1780 [D loss: (2.345)(R 2.342, F 2.349)]  [G loss: -54.471] \n",
                        "1781 [D loss: (2.348)(R 2.344, F 2.351)]  [G loss: -54.506] \n",
                        "1782 [D loss: (2.349)(R 2.346, F 2.353)]  [G loss: -54.540] \n",
                        "1783 [D loss: (2.351)(R 2.347, F 2.354)]  [G loss: -54.576] \n",
                        "1784 [D loss: (2.353)(R 2.349, F 2.356)]  [G loss: -54.611] \n",
                        "1785 [D loss: (2.354)(R 2.351, F 2.358)]  [G loss: -54.645] \n",
                        "1786 [D loss: (2.356)(R 2.352, F 2.359)]  [G loss: -54.680] \n",
                        "1787 [D loss: (2.357)(R 2.354, F 2.361)]  [G loss: -54.715] \n",
                        "1788 [D loss: (2.359)(R 2.355, F 2.362)]  [G loss: -54.750] \n",
                        "1789 [D loss: (2.360)(R 2.357, F 2.363)]  [G loss: -54.786] \n",
                        "1790 [D loss: (2.362)(R 2.359, F 2.366)]  [G loss: -54.821] \n",
                        "1791 [D loss: (2.364)(R 2.360, F 2.367)]  [G loss: -54.858] \n",
                        "1792 [D loss: (2.365)(R 2.361, F 2.369)]  [G loss: -54.895] \n",
                        "1793 [D loss: (2.367)(R 2.363, F 2.371)]  [G loss: -54.932] \n",
                        "1794 [D loss: (2.369)(R 2.365, F 2.372)]  [G loss: -54.970] \n",
                        "1795 [D loss: (2.370)(R 2.367, F 2.374)]  [G loss: -55.008] \n",
                        "1796 [D loss: (2.373)(R 2.369, F 2.376)]  [G loss: -55.046] \n",
                        "1797 [D loss: (2.374)(R 2.371, F 2.378)]  [G loss: -55.083] \n",
                        "1798 [D loss: (2.376)(R 2.372, F 2.380)]  [G loss: -55.122] \n",
                        "1799 [D loss: (2.379)(R 2.375, F 2.382)]  [G loss: -55.160] \n",
                        "1800 [D loss: (2.381)(R 2.377, F 2.384)]  [G loss: -55.196] \n",
                        "1801 [D loss: (2.382)(R 2.379, F 2.386)]  [G loss: -55.234] \n",
                        "1802 [D loss: (2.384)(R 2.381, F 2.388)]  [G loss: -55.270] \n",
                        "1803 [D loss: (2.386)(R 2.382, F 2.390)]  [G loss: -55.308] \n",
                        "1804 [D loss: (2.389)(R 2.385, F 2.392)]  [G loss: -55.344] \n",
                        "1805 [D loss: (2.390)(R 2.387, F 2.394)]  [G loss: -55.379] \n",
                        "1806 [D loss: (2.392)(R 2.388, F 2.396)]  [G loss: -55.415] \n",
                        "1807 [D loss: (2.394)(R 2.390, F 2.397)]  [G loss: -55.451] \n",
                        "1808 [D loss: (2.396)(R 2.392, F 2.399)]  [G loss: -55.486] \n",
                        "1809 [D loss: (2.398)(R 2.394, F 2.401)]  [G loss: -55.520] \n",
                        "1810 [D loss: (2.399)(R 2.396, F 2.403)]  [G loss: -55.554] \n",
                        "1811 [D loss: (2.401)(R 2.397, F 2.404)]  [G loss: -55.589] \n",
                        "1812 [D loss: (2.403)(R 2.399, F 2.406)]  [G loss: -55.621] \n",
                        "1813 [D loss: (2.404)(R 2.401, F 2.407)]  [G loss: -55.654] \n",
                        "1814 [D loss: (2.405)(R 2.402, F 2.409)]  [G loss: -55.687] \n",
                        "1815 [D loss: (2.407)(R 2.403, F 2.410)]  [G loss: -55.721] \n",
                        "1816 [D loss: (2.408)(R 2.405, F 2.412)]  [G loss: -55.753] \n",
                        "1817 [D loss: (2.410)(R 2.406, F 2.413)]  [G loss: -55.787] \n",
                        "1818 [D loss: (2.411)(R 2.408, F 2.414)]  [G loss: -55.822] \n",
                        "1819 [D loss: (2.413)(R 2.410, F 2.416)]  [G loss: -55.856] \n",
                        "1820 [D loss: (2.414)(R 2.411, F 2.418)]  [G loss: -55.891] \n",
                        "1821 [D loss: (2.416)(R 2.413, F 2.420)]  [G loss: -55.926] \n",
                        "1822 [D loss: (2.418)(R 2.414, F 2.421)]  [G loss: -55.962] \n",
                        "1823 [D loss: (2.420)(R 2.416, F 2.423)]  [G loss: -55.996] \n",
                        "1824 [D loss: (2.421)(R 2.418, F 2.425)]  [G loss: -56.032] \n",
                        "1825 [D loss: (2.423)(R 2.420, F 2.427)]  [G loss: -56.066] \n",
                        "1826 [D loss: (2.425)(R 2.421, F 2.428)]  [G loss: -56.101] \n",
                        "1827 [D loss: (2.427)(R 2.423, F 2.430)]  [G loss: -56.135] \n",
                        "1828 [D loss: (2.428)(R 2.425, F 2.432)]  [G loss: -56.169] \n",
                        "1829 [D loss: (2.430)(R 2.426, F 2.433)]  [G loss: -56.204] \n",
                        "1830 [D loss: (2.432)(R 2.428, F 2.435)]  [G loss: -56.237] \n",
                        "1831 [D loss: (2.433)(R 2.430, F 2.437)]  [G loss: -56.271] \n",
                        "1832 [D loss: (2.435)(R 2.432, F 2.438)]  [G loss: -56.305] \n",
                        "1833 [D loss: (2.437)(R 2.433, F 2.440)]  [G loss: -56.338] \n",
                        "1834 [D loss: (2.438)(R 2.435, F 2.441)]  [G loss: -56.371] \n",
                        "1835 [D loss: (2.439)(R 2.436, F 2.443)]  [G loss: -56.405] \n",
                        "1836 [D loss: (2.441)(R 2.438, F 2.445)]  [G loss: -56.439] \n",
                        "1837 [D loss: (2.443)(R 2.439, F 2.446)]  [G loss: -56.472] \n",
                        "1838 [D loss: (2.444)(R 2.441, F 2.447)]  [G loss: -56.506] \n",
                        "1839 [D loss: (2.446)(R 2.442, F 2.449)]  [G loss: -56.541] \n",
                        "1840 [D loss: (2.447)(R 2.444, F 2.451)]  [G loss: -56.574] \n",
                        "1841 [D loss: (2.449)(R 2.445, F 2.452)]  [G loss: -56.609] \n",
                        "1842 [D loss: (2.450)(R 2.447, F 2.454)]  [G loss: -56.644] \n",
                        "1843 [D loss: (2.452)(R 2.449, F 2.456)]  [G loss: -56.679] \n",
                        "1844 [D loss: (2.454)(R 2.450, F 2.457)]  [G loss: -56.715] \n",
                        "1845 [D loss: (2.455)(R 2.452, F 2.459)]  [G loss: -56.750] \n",
                        "1846 [D loss: (2.457)(R 2.453, F 2.460)]  [G loss: -56.786] \n",
                        "1847 [D loss: (2.459)(R 2.456, F 2.462)]  [G loss: -56.821] \n",
                        "1848 [D loss: (2.461)(R 2.457, F 2.464)]  [G loss: -56.857] \n",
                        "1849 [D loss: (2.462)(R 2.459, F 2.466)]  [G loss: -56.893] \n",
                        "1850 [D loss: (2.465)(R 2.461, F 2.468)]  [G loss: -56.929] \n",
                        "1851 [D loss: (2.466)(R 2.463, F 2.470)]  [G loss: -56.965] \n",
                        "1852 [D loss: (2.468)(R 2.465, F 2.472)]  [G loss: -56.999] \n",
                        "1853 [D loss: (2.470)(R 2.467, F 2.474)]  [G loss: -57.034] \n",
                        "1854 [D loss: (2.472)(R 2.468, F 2.475)]  [G loss: -57.069] \n",
                        "1855 [D loss: (2.473)(R 2.470, F 2.477)]  [G loss: -57.104] \n",
                        "1856 [D loss: (2.475)(R 2.472, F 2.479)]  [G loss: -57.138] \n",
                        "1857 [D loss: (2.477)(R 2.474, F 2.480)]  [G loss: -57.174] \n",
                        "1858 [D loss: (2.478)(R 2.475, F 2.482)]  [G loss: -57.209] \n",
                        "1859 [D loss: (2.481)(R 2.477, F 2.484)]  [G loss: -57.243] \n",
                        "1860 [D loss: (2.482)(R 2.479, F 2.486)]  [G loss: -57.278] \n",
                        "1861 [D loss: (2.484)(R 2.481, F 2.487)]  [G loss: -57.312] \n",
                        "1862 [D loss: (2.486)(R 2.482, F 2.489)]  [G loss: -57.347] \n",
                        "1863 [D loss: (2.488)(R 2.484, F 2.491)]  [G loss: -57.381] \n",
                        "1864 [D loss: (2.489)(R 2.486, F 2.493)]  [G loss: -57.415] \n",
                        "1865 [D loss: (2.491)(R 2.487, F 2.494)]  [G loss: -57.450] \n",
                        "1866 [D loss: (2.493)(R 2.489, F 2.496)]  [G loss: -57.483] \n",
                        "1867 [D loss: (2.494)(R 2.491, F 2.498)]  [G loss: -57.517] \n",
                        "1868 [D loss: (2.496)(R 2.493, F 2.499)]  [G loss: -57.551] \n",
                        "1869 [D loss: (2.498)(R 2.495, F 2.501)]  [G loss: -57.584] \n",
                        "1870 [D loss: (2.499)(R 2.496, F 2.503)]  [G loss: -57.618] \n",
                        "1871 [D loss: (2.501)(R 2.498, F 2.505)]  [G loss: -57.651] \n",
                        "1872 [D loss: (2.503)(R 2.499, F 2.506)]  [G loss: -57.686] \n",
                        "1873 [D loss: (2.505)(R 2.501, F 2.508)]  [G loss: -57.720] \n",
                        "1874 [D loss: (2.506)(R 2.503, F 2.510)]  [G loss: -57.754] \n",
                        "1875 [D loss: (2.508)(R 2.505, F 2.511)]  [G loss: -57.788] \n",
                        "1876 [D loss: (2.510)(R 2.507, F 2.513)]  [G loss: -57.822] \n",
                        "1877 [D loss: (2.512)(R 2.508, F 2.515)]  [G loss: -57.857] \n",
                        "1878 [D loss: (2.514)(R 2.510, F 2.517)]  [G loss: -57.889] \n",
                        "1879 [D loss: (2.515)(R 2.512, F 2.519)]  [G loss: -57.923] \n",
                        "1880 [D loss: (2.517)(R 2.514, F 2.520)]  [G loss: -57.955] \n",
                        "1881 [D loss: (2.519)(R 2.515, F 2.522)]  [G loss: -57.988] \n",
                        "1882 [D loss: (2.520)(R 2.517, F 2.523)]  [G loss: -58.021] \n",
                        "1883 [D loss: (2.521)(R 2.518, F 2.525)]  [G loss: -58.054] \n",
                        "1884 [D loss: (2.523)(R 2.520, F 2.527)]  [G loss: -58.086] \n",
                        "1885 [D loss: (2.525)(R 2.522, F 2.528)]  [G loss: -58.119] \n",
                        "1886 [D loss: (2.526)(R 2.523, F 2.530)]  [G loss: -58.153] \n",
                        "1887 [D loss: (2.528)(R 2.525, F 2.532)]  [G loss: -58.186] \n",
                        "1888 [D loss: (2.530)(R 2.526, F 2.533)]  [G loss: -58.220] \n",
                        "1889 [D loss: (2.531)(R 2.528, F 2.535)]  [G loss: -58.254] \n",
                        "1890 [D loss: (2.533)(R 2.530, F 2.536)]  [G loss: -58.289] \n",
                        "1891 [D loss: (2.535)(R 2.531, F 2.538)]  [G loss: -58.324] \n",
                        "1892 [D loss: (2.536)(R 2.533, F 2.540)]  [G loss: -58.359] \n",
                        "1893 [D loss: (2.539)(R 2.535, F 2.542)]  [G loss: -58.394] \n",
                        "1894 [D loss: (2.540)(R 2.537, F 2.544)]  [G loss: -58.430] \n",
                        "1895 [D loss: (2.542)(R 2.539, F 2.546)]  [G loss: -58.465] \n",
                        "1896 [D loss: (2.544)(R 2.541, F 2.548)]  [G loss: -58.500] \n",
                        "1897 [D loss: (2.546)(R 2.543, F 2.550)]  [G loss: -58.534] \n",
                        "1898 [D loss: (2.548)(R 2.545, F 2.552)]  [G loss: -58.569] \n",
                        "1899 [D loss: (2.550)(R 2.547, F 2.554)]  [G loss: -58.604] \n",
                        "1900 [D loss: (2.552)(R 2.549, F 2.556)]  [G loss: -58.637] \n",
                        "1901 [D loss: (2.554)(R 2.550, F 2.557)]  [G loss: -58.671] \n",
                        "1902 [D loss: (2.555)(R 2.552, F 2.559)]  [G loss: -58.704] \n",
                        "1903 [D loss: (2.557)(R 2.554, F 2.560)]  [G loss: -58.736] \n",
                        "1904 [D loss: (2.559)(R 2.555, F 2.562)]  [G loss: -58.770] \n",
                        "1905 [D loss: (2.561)(R 2.557, F 2.564)]  [G loss: -58.803] \n",
                        "1906 [D loss: (2.562)(R 2.559, F 2.566)]  [G loss: -58.836] \n",
                        "1907 [D loss: (2.564)(R 2.560, F 2.567)]  [G loss: -58.870] \n",
                        "1908 [D loss: (2.566)(R 2.562, F 2.569)]  [G loss: -58.902] \n",
                        "1909 [D loss: (2.567)(R 2.564, F 2.571)]  [G loss: -58.935] \n",
                        "1910 [D loss: (2.569)(R 2.566, F 2.572)]  [G loss: -58.968] \n",
                        "1911 [D loss: (2.570)(R 2.567, F 2.574)]  [G loss: -59.001] \n",
                        "1912 [D loss: (2.572)(R 2.569, F 2.576)]  [G loss: -59.034] \n",
                        "1913 [D loss: (2.574)(R 2.570, F 2.577)]  [G loss: -59.067] \n",
                        "1914 [D loss: (2.576)(R 2.572, F 2.579)]  [G loss: -59.100] \n",
                        "1915 [D loss: (2.577)(R 2.574, F 2.580)]  [G loss: -59.134] \n",
                        "1916 [D loss: (2.579)(R 2.576, F 2.582)]  [G loss: -59.167] \n",
                        "1917 [D loss: (2.581)(R 2.577, F 2.584)]  [G loss: -59.201] \n",
                        "1918 [D loss: (2.583)(R 2.579, F 2.586)]  [G loss: -59.234] \n",
                        "1919 [D loss: (2.584)(R 2.581, F 2.588)]  [G loss: -59.269] \n",
                        "1920 [D loss: (2.586)(R 2.583, F 2.590)]  [G loss: -59.301] \n",
                        "1921 [D loss: (2.588)(R 2.585, F 2.591)]  [G loss: -59.334] \n",
                        "1922 [D loss: (2.590)(R 2.587, F 2.593)]  [G loss: -59.367] \n",
                        "1923 [D loss: (2.592)(R 2.589, F 2.595)]  [G loss: -59.397] \n",
                        "1924 [D loss: (2.593)(R 2.590, F 2.597)]  [G loss: -59.428] \n",
                        "1925 [D loss: (2.595)(R 2.592, F 2.598)]  [G loss: -59.459] \n",
                        "1926 [D loss: (2.596)(R 2.593, F 2.600)]  [G loss: -59.490] \n",
                        "1927 [D loss: (2.598)(R 2.595, F 2.601)]  [G loss: -59.520] \n",
                        "1928 [D loss: (2.599)(R 2.596, F 2.602)]  [G loss: -59.552] \n",
                        "1929 [D loss: (2.601)(R 2.597, F 2.604)]  [G loss: -59.584] \n",
                        "1930 [D loss: (2.602)(R 2.599, F 2.606)]  [G loss: -59.615] \n",
                        "1931 [D loss: (2.604)(R 2.600, F 2.607)]  [G loss: -59.648] \n",
                        "1932 [D loss: (2.605)(R 2.602, F 2.609)]  [G loss: -59.680] \n",
                        "1933 [D loss: (2.607)(R 2.603, F 2.610)]  [G loss: -59.713] \n",
                        "1934 [D loss: (2.608)(R 2.605, F 2.612)]  [G loss: -59.746] \n",
                        "1935 [D loss: (2.610)(R 2.607, F 2.613)]  [G loss: -59.779] \n",
                        "1936 [D loss: (2.611)(R 2.608, F 2.615)]  [G loss: -59.814] \n",
                        "1937 [D loss: (2.614)(R 2.610, F 2.617)]  [G loss: -59.844] \n",
                        "1938 [D loss: (2.615)(R 2.612, F 2.619)]  [G loss: -59.878] \n",
                        "1939 [D loss: (2.617)(R 2.614, F 2.620)]  [G loss: -59.913] \n",
                        "1940 [D loss: (2.619)(R 2.616, F 2.623)]  [G loss: -59.945] \n",
                        "1941 [D loss: (2.621)(R 2.618, F 2.625)]  [G loss: -59.978] \n",
                        "1942 [D loss: (2.623)(R 2.619, F 2.626)]  [G loss: -60.013] \n",
                        "1943 [D loss: (2.625)(R 2.622, F 2.628)]  [G loss: -60.045] \n",
                        "1944 [D loss: (2.627)(R 2.623, F 2.630)]  [G loss: -60.077] \n",
                        "1945 [D loss: (2.628)(R 2.625, F 2.631)]  [G loss: -60.109] \n",
                        "1946 [D loss: (2.630)(R 2.627, F 2.633)]  [G loss: -60.140] \n",
                        "1947 [D loss: (2.631)(R 2.628, F 2.635)]  [G loss: -60.171] \n",
                        "1948 [D loss: (2.633)(R 2.629, F 2.636)]  [G loss: -60.203] \n",
                        "1949 [D loss: (2.635)(R 2.631, F 2.638)]  [G loss: -60.234] \n",
                        "1950 [D loss: (2.636)(R 2.633, F 2.639)]  [G loss: -60.264] \n",
                        "1951 [D loss: (2.637)(R 2.634, F 2.641)]  [G loss: -60.297] \n",
                        "1952 [D loss: (2.640)(R 2.636, F 2.643)]  [G loss: -60.327] \n",
                        "1953 [D loss: (2.641)(R 2.638, F 2.644)]  [G loss: -60.358] \n",
                        "1954 [D loss: (2.642)(R 2.639, F 2.646)]  [G loss: -60.390] \n",
                        "1955 [D loss: (2.644)(R 2.640, F 2.647)]  [G loss: -60.423] \n",
                        "1956 [D loss: (2.645)(R 2.642, F 2.649)]  [G loss: -60.455] \n",
                        "1957 [D loss: (2.647)(R 2.644, F 2.650)]  [G loss: -60.486] \n",
                        "1958 [D loss: (2.648)(R 2.645, F 2.651)]  [G loss: -60.520] \n",
                        "1959 [D loss: (2.650)(R 2.647, F 2.654)]  [G loss: -60.552] \n",
                        "1960 [D loss: (2.652)(R 2.649, F 2.655)]  [G loss: -60.584] \n",
                        "1961 [D loss: (2.653)(R 2.650, F 2.657)]  [G loss: -60.616] \n",
                        "1962 [D loss: (2.655)(R 2.652, F 2.658)]  [G loss: -60.649] \n",
                        "1963 [D loss: (2.657)(R 2.653, F 2.660)]  [G loss: -60.680] \n",
                        "1964 [D loss: (2.658)(R 2.655, F 2.662)]  [G loss: -60.712] \n",
                        "1965 [D loss: (2.660)(R 2.657, F 2.663)]  [G loss: -60.743] \n",
                        "1966 [D loss: (2.661)(R 2.658, F 2.664)]  [G loss: -60.776] \n",
                        "1967 [D loss: (2.664)(R 2.660, F 2.667)]  [G loss: -60.808] \n",
                        "1968 [D loss: (2.665)(R 2.662, F 2.668)]  [G loss: -60.838] \n",
                        "1969 [D loss: (2.667)(R 2.664, F 2.670)]  [G loss: -60.869] \n",
                        "1970 [D loss: (2.668)(R 2.665, F 2.672)]  [G loss: -60.900] \n",
                        "1971 [D loss: (2.670)(R 2.667, F 2.673)]  [G loss: -60.931] \n",
                        "1972 [D loss: (2.671)(R 2.668, F 2.675)]  [G loss: -60.961] \n",
                        "1973 [D loss: (2.673)(R 2.670, F 2.676)]  [G loss: -60.989] \n",
                        "1974 [D loss: (2.674)(R 2.671, F 2.677)]  [G loss: -61.018] \n",
                        "1975 [D loss: (2.675)(R 2.672, F 2.678)]  [G loss: -61.047] \n",
                        "1976 [D loss: (2.677)(R 2.673, F 2.680)]  [G loss: -61.076] \n",
                        "1977 [D loss: (2.678)(R 2.675, F 2.681)]  [G loss: -61.105] \n",
                        "1978 [D loss: (2.679)(R 2.676, F 2.682)]  [G loss: -61.135] \n",
                        "1979 [D loss: (2.680)(R 2.677, F 2.684)]  [G loss: -61.164] \n",
                        "1980 [D loss: (2.682)(R 2.678, F 2.685)]  [G loss: -61.194] \n",
                        "1981 [D loss: (2.683)(R 2.680, F 2.686)]  [G loss: -61.224] \n",
                        "1982 [D loss: (2.684)(R 2.681, F 2.688)]  [G loss: -61.255] \n",
                        "1983 [D loss: (2.686)(R 2.683, F 2.689)]  [G loss: -61.285] \n",
                        "1984 [D loss: (2.687)(R 2.684, F 2.690)]  [G loss: -61.316] \n",
                        "1985 [D loss: (2.689)(R 2.685, F 2.692)]  [G loss: -61.346] \n",
                        "1986 [D loss: (2.690)(R 2.687, F 2.693)]  [G loss: -61.378] \n",
                        "1987 [D loss: (2.691)(R 2.688, F 2.695)]  [G loss: -61.409] \n",
                        "1988 [D loss: (2.693)(R 2.690, F 2.696)]  [G loss: -61.439] \n",
                        "1989 [D loss: (2.694)(R 2.691, F 2.698)]  [G loss: -61.469] \n",
                        "1990 [D loss: (2.696)(R 2.692, F 2.699)]  [G loss: -61.500] \n",
                        "1991 [D loss: (2.698)(R 2.694, F 2.701)]  [G loss: -61.529] \n",
                        "1992 [D loss: (2.699)(R 2.696, F 2.702)]  [G loss: -61.560] \n",
                        "1993 [D loss: (2.700)(R 2.697, F 2.704)]  [G loss: -61.590] \n",
                        "1994 [D loss: (2.702)(R 2.699, F 2.705)]  [G loss: -61.621] \n",
                        "1995 [D loss: (2.703)(R 2.700, F 2.706)]  [G loss: -61.652] \n",
                        "1996 [D loss: (2.705)(R 2.701, F 2.708)]  [G loss: -61.685] \n",
                        "1997 [D loss: (2.707)(R 2.704, F 2.710)]  [G loss: -61.716] \n",
                        "1998 [D loss: (2.709)(R 2.705, F 2.712)]  [G loss: -61.746] \n",
                        "1999 [D loss: (2.710)(R 2.707, F 2.713)]  [G loss: -61.778] \n",
                        "2000 [D loss: (2.712)(R 2.708, F 2.715)]  [G loss: -61.808] \n",
                        "2001 [D loss: (2.713)(R 2.710, F 2.716)]  [G loss: -61.838] \n",
                        "2002 [D loss: (2.714)(R 2.711, F 2.717)]  [G loss: -61.868] \n",
                        "2003 [D loss: (2.716)(R 2.713, F 2.719)]  [G loss: -61.897] \n",
                        "2004 [D loss: (2.718)(R 2.714, F 2.721)]  [G loss: -61.925] \n",
                        "2005 [D loss: (2.719)(R 2.716, F 2.722)]  [G loss: -61.954] \n",
                        "2006 [D loss: (2.720)(R 2.717, F 2.723)]  [G loss: -61.983] \n",
                        "2007 [D loss: (2.721)(R 2.718, F 2.725)]  [G loss: -62.013] \n",
                        "2008 [D loss: (2.723)(R 2.720, F 2.726)]  [G loss: -62.042] \n",
                        "2009 [D loss: (2.724)(R 2.721, F 2.727)]  [G loss: -62.073] \n",
                        "2010 [D loss: (2.726)(R 2.722, F 2.729)]  [G loss: -62.102] \n",
                        "2011 [D loss: (2.727)(R 2.724, F 2.730)]  [G loss: -62.133] \n",
                        "2012 [D loss: (2.728)(R 2.725, F 2.731)]  [G loss: -62.162] \n",
                        "2013 [D loss: (2.729)(R 2.726, F 2.732)]  [G loss: -62.195] \n",
                        "2014 [D loss: (2.731)(R 2.728, F 2.735)]  [G loss: -62.227] \n",
                        "2015 [D loss: (2.733)(R 2.730, F 2.736)]  [G loss: -62.256] \n",
                        "2016 [D loss: (2.735)(R 2.731, F 2.738)]  [G loss: -62.287] \n",
                        "2017 [D loss: (2.736)(R 2.733, F 2.739)]  [G loss: -62.320] \n",
                        "2018 [D loss: (2.738)(R 2.734, F 2.741)]  [G loss: -62.352] \n",
                        "2019 [D loss: (2.740)(R 2.737, F 2.743)]  [G loss: -62.382] \n",
                        "2020 [D loss: (2.741)(R 2.738, F 2.745)]  [G loss: -62.413] \n",
                        "2021 [D loss: (2.743)(R 2.739, F 2.746)]  [G loss: -62.444] \n",
                        "2022 [D loss: (2.745)(R 2.741, F 2.748)]  [G loss: -62.474] \n",
                        "2023 [D loss: (2.746)(R 2.743, F 2.749)]  [G loss: -62.504] \n",
                        "2024 [D loss: (2.748)(R 2.744, F 2.751)]  [G loss: -62.535] \n",
                        "2025 [D loss: (2.749)(R 2.746, F 2.753)]  [G loss: -62.565] \n",
                        "2026 [D loss: (2.751)(R 2.748, F 2.754)]  [G loss: -62.594] \n",
                        "2027 [D loss: (2.752)(R 2.749, F 2.755)]  [G loss: -62.624] \n",
                        "2028 [D loss: (2.754)(R 2.751, F 2.757)]  [G loss: -62.653] \n",
                        "2029 [D loss: (2.755)(R 2.752, F 2.758)]  [G loss: -62.682] \n",
                        "2030 [D loss: (2.756)(R 2.753, F 2.760)]  [G loss: -62.710] \n",
                        "2031 [D loss: (2.758)(R 2.755, F 2.761)]  [G loss: -62.738] \n",
                        "2032 [D loss: (2.759)(R 2.756, F 2.762)]  [G loss: -62.766] \n",
                        "2033 [D loss: (2.760)(R 2.757, F 2.763)]  [G loss: -62.794] \n",
                        "2034 [D loss: (2.761)(R 2.758, F 2.764)]  [G loss: -62.824] \n",
                        "2035 [D loss: (2.763)(R 2.760, F 2.766)]  [G loss: -62.852] \n",
                        "2036 [D loss: (2.764)(R 2.761, F 2.767)]  [G loss: -62.880] \n",
                        "2037 [D loss: (2.765)(R 2.762, F 2.768)]  [G loss: -62.911] \n",
                        "2038 [D loss: (2.767)(R 2.764, F 2.770)]  [G loss: -62.940] \n",
                        "2039 [D loss: (2.768)(R 2.765, F 2.771)]  [G loss: -62.969] \n",
                        "2040 [D loss: (2.769)(R 2.766, F 2.773)]  [G loss: -63.000] \n",
                        "2041 [D loss: (2.771)(R 2.768, F 2.774)]  [G loss: -63.031] \n",
                        "2042 [D loss: (2.773)(R 2.769, F 2.776)]  [G loss: -63.061] \n",
                        "2043 [D loss: (2.774)(R 2.771, F 2.777)]  [G loss: -63.090] \n",
                        "2044 [D loss: (2.775)(R 2.772, F 2.779)]  [G loss: -63.121] \n",
                        "2045 [D loss: (2.777)(R 2.774, F 2.780)]  [G loss: -63.151] \n",
                        "2046 [D loss: (2.779)(R 2.775, F 2.782)]  [G loss: -63.180] \n",
                        "2047 [D loss: (2.780)(R 2.776, F 2.783)]  [G loss: -63.210] \n",
                        "2048 [D loss: (2.782)(R 2.779, F 2.785)]  [G loss: -63.239] \n",
                        "2049 [D loss: (2.783)(R 2.780, F 2.786)]  [G loss: -63.269] \n",
                        "2050 [D loss: (2.784)(R 2.781, F 2.788)]  [G loss: -63.299] \n",
                        "2051 [D loss: (2.786)(R 2.783, F 2.789)]  [G loss: -63.329] \n",
                        "2052 [D loss: (2.788)(R 2.784, F 2.791)]  [G loss: -63.359] \n",
                        "2053 [D loss: (2.789)(R 2.786, F 2.792)]  [G loss: -63.389] \n",
                        "2054 [D loss: (2.790)(R 2.787, F 2.794)]  [G loss: -63.419] \n",
                        "2055 [D loss: (2.792)(R 2.789, F 2.795)]  [G loss: -63.448] \n",
                        "2056 [D loss: (2.794)(R 2.790, F 2.797)]  [G loss: -63.477] \n",
                        "2057 [D loss: (2.795)(R 2.792, F 2.798)]  [G loss: -63.506] \n",
                        "2058 [D loss: (2.796)(R 2.793, F 2.799)]  [G loss: -63.535] \n",
                        "2059 [D loss: (2.798)(R 2.795, F 2.801)]  [G loss: -63.562] \n",
                        "2060 [D loss: (2.799)(R 2.796, F 2.802)]  [G loss: -63.589] \n",
                        "2061 [D loss: (2.800)(R 2.797, F 2.803)]  [G loss: -63.617] \n",
                        "2062 [D loss: (2.801)(R 2.798, F 2.805)]  [G loss: -63.646] \n",
                        "2063 [D loss: (2.803)(R 2.800, F 2.806)]  [G loss: -63.674] \n",
                        "2064 [D loss: (2.804)(R 2.801, F 2.807)]  [G loss: -63.703] \n",
                        "2065 [D loss: (2.806)(R 2.802, F 2.809)]  [G loss: -63.731] \n",
                        "2066 [D loss: (2.807)(R 2.804, F 2.810)]  [G loss: -63.761] \n",
                        "2067 [D loss: (2.808)(R 2.805, F 2.811)]  [G loss: -63.790] \n",
                        "2068 [D loss: (2.809)(R 2.806, F 2.812)]  [G loss: -63.820] \n",
                        "2069 [D loss: (2.811)(R 2.808, F 2.814)]  [G loss: -63.849] \n",
                        "2070 [D loss: (2.813)(R 2.809, F 2.816)]  [G loss: -63.878] \n",
                        "2071 [D loss: (2.814)(R 2.811, F 2.817)]  [G loss: -63.907] \n",
                        "2072 [D loss: (2.815)(R 2.812, F 2.819)]  [G loss: -63.936] \n",
                        "2073 [D loss: (2.817)(R 2.814, F 2.820)]  [G loss: -63.965] \n",
                        "2074 [D loss: (2.818)(R 2.815, F 2.821)]  [G loss: -63.992] \n",
                        "2075 [D loss: (2.820)(R 2.816, F 2.823)]  [G loss: -64.021] \n",
                        "2076 [D loss: (2.821)(R 2.818, F 2.824)]  [G loss: -64.051] \n",
                        "2077 [D loss: (2.823)(R 2.820, F 2.826)]  [G loss: -64.079] \n",
                        "2078 [D loss: (2.824)(R 2.821, F 2.827)]  [G loss: -64.108] \n",
                        "2079 [D loss: (2.825)(R 2.822, F 2.828)]  [G loss: -64.138] \n",
                        "2080 [D loss: (2.827)(R 2.824, F 2.830)]  [G loss: -64.168] \n",
                        "2081 [D loss: (2.829)(R 2.826, F 2.832)]  [G loss: -64.196] \n",
                        "2082 [D loss: (2.830)(R 2.827, F 2.833)]  [G loss: -64.224] \n",
                        "2083 [D loss: (2.831)(R 2.828, F 2.834)]  [G loss: -64.253] \n",
                        "2084 [D loss: (2.833)(R 2.830, F 2.836)]  [G loss: -64.280] \n",
                        "2085 [D loss: (2.834)(R 2.831, F 2.837)]  [G loss: -64.308] \n",
                        "2086 [D loss: (2.835)(R 2.832, F 2.838)]  [G loss: -64.336] \n",
                        "2087 [D loss: (2.836)(R 2.833, F 2.839)]  [G loss: -64.364] \n",
                        "2088 [D loss: (2.838)(R 2.834, F 2.841)]  [G loss: -64.393] \n",
                        "2089 [D loss: (2.839)(R 2.836, F 2.842)]  [G loss: -64.422] \n",
                        "2090 [D loss: (2.840)(R 2.837, F 2.843)]  [G loss: -64.451] \n",
                        "2091 [D loss: (2.842)(R 2.838, F 2.845)]  [G loss: -64.481] \n",
                        "2092 [D loss: (2.843)(R 2.840, F 2.846)]  [G loss: -64.511] \n",
                        "2093 [D loss: (2.844)(R 2.841, F 2.848)]  [G loss: -64.541] \n",
                        "2094 [D loss: (2.846)(R 2.843, F 2.849)]  [G loss: -64.571] \n",
                        "2095 [D loss: (2.848)(R 2.845, F 2.851)]  [G loss: -64.599] \n",
                        "2096 [D loss: (2.849)(R 2.846, F 2.852)]  [G loss: -64.630] \n",
                        "2097 [D loss: (2.851)(R 2.848, F 2.854)]  [G loss: -64.658] \n",
                        "2098 [D loss: (2.852)(R 2.849, F 2.855)]  [G loss: -64.687] \n",
                        "2099 [D loss: (2.854)(R 2.850, F 2.857)]  [G loss: -64.716] \n",
                        "2100 [D loss: (2.855)(R 2.852, F 2.858)]  [G loss: -64.746] \n",
                        "2101 [D loss: (2.857)(R 2.853, F 2.860)]  [G loss: -64.775] \n",
                        "2102 [D loss: (2.858)(R 2.855, F 2.861)]  [G loss: -64.804] \n",
                        "2103 [D loss: (2.860)(R 2.856, F 2.863)]  [G loss: -64.833] \n",
                        "2104 [D loss: (2.861)(R 2.858, F 2.864)]  [G loss: -64.862] \n",
                        "2105 [D loss: (2.863)(R 2.860, F 2.866)]  [G loss: -64.890] \n",
                        "2106 [D loss: (2.864)(R 2.861, F 2.867)]  [G loss: -64.918] \n",
                        "2107 [D loss: (2.865)(R 2.862, F 2.868)]  [G loss: -64.946] \n",
                        "2108 [D loss: (2.867)(R 2.864, F 2.870)]  [G loss: -64.973] \n",
                        "2109 [D loss: (2.868)(R 2.865, F 2.871)]  [G loss: -65.000] \n",
                        "2110 [D loss: (2.869)(R 2.866, F 2.872)]  [G loss: -65.028] \n",
                        "2111 [D loss: (2.871)(R 2.868, F 2.874)]  [G loss: -65.055] \n",
                        "2112 [D loss: (2.872)(R 2.869, F 2.875)]  [G loss: -65.082] \n",
                        "2113 [D loss: (2.873)(R 2.870, F 2.876)]  [G loss: -65.110] \n",
                        "2114 [D loss: (2.875)(R 2.872, F 2.878)]  [G loss: -65.137] \n",
                        "2115 [D loss: (2.876)(R 2.873, F 2.879)]  [G loss: -65.164] \n",
                        "2116 [D loss: (2.877)(R 2.874, F 2.880)]  [G loss: -65.193] \n",
                        "2117 [D loss: (2.878)(R 2.875, F 2.881)]  [G loss: -65.220] \n",
                        "2118 [D loss: (2.880)(R 2.877, F 2.883)]  [G loss: -65.248] \n",
                        "2119 [D loss: (2.881)(R 2.878, F 2.884)]  [G loss: -65.277] \n",
                        "2120 [D loss: (2.882)(R 2.879, F 2.886)]  [G loss: -65.304] \n",
                        "2121 [D loss: (2.884)(R 2.881, F 2.887)]  [G loss: -65.332] \n",
                        "2122 [D loss: (2.885)(R 2.882, F 2.888)]  [G loss: -65.359] \n",
                        "2123 [D loss: (2.886)(R 2.883, F 2.890)]  [G loss: -65.388] \n",
                        "2124 [D loss: (2.888)(R 2.885, F 2.891)]  [G loss: -65.416] \n",
                        "2125 [D loss: (2.889)(R 2.886, F 2.892)]  [G loss: -65.445] \n",
                        "2126 [D loss: (2.891)(R 2.888, F 2.894)]  [G loss: -65.471] \n",
                        "2127 [D loss: (2.892)(R 2.889, F 2.895)]  [G loss: -65.499] \n",
                        "2128 [D loss: (2.894)(R 2.890, F 2.897)]  [G loss: -65.527] \n",
                        "2129 [D loss: (2.895)(R 2.892, F 2.898)]  [G loss: -65.555] \n",
                        "2130 [D loss: (2.897)(R 2.894, F 2.900)]  [G loss: -65.582] \n",
                        "2131 [D loss: (2.898)(R 2.895, F 2.901)]  [G loss: -65.609] \n",
                        "2132 [D loss: (2.899)(R 2.896, F 2.902)]  [G loss: -65.636] \n",
                        "2133 [D loss: (2.900)(R 2.897, F 2.904)]  [G loss: -65.663] \n",
                        "2134 [D loss: (2.902)(R 2.898, F 2.905)]  [G loss: -65.691] \n",
                        "2135 [D loss: (2.903)(R 2.900, F 2.906)]  [G loss: -65.719] \n",
                        "2136 [D loss: (2.904)(R 2.901, F 2.907)]  [G loss: -65.746] \n",
                        "2137 [D loss: (2.905)(R 2.902, F 2.909)]  [G loss: -65.774] \n",
                        "2138 [D loss: (2.907)(R 2.904, F 2.910)]  [G loss: -65.802] \n",
                        "2139 [D loss: (2.908)(R 2.905, F 2.911)]  [G loss: -65.831] \n",
                        "2140 [D loss: (2.910)(R 2.907, F 2.913)]  [G loss: -65.858] \n",
                        "2141 [D loss: (2.911)(R 2.908, F 2.914)]  [G loss: -65.887] \n",
                        "2142 [D loss: (2.913)(R 2.910, F 2.916)]  [G loss: -65.914] \n",
                        "2143 [D loss: (2.914)(R 2.911, F 2.917)]  [G loss: -65.943] \n",
                        "2144 [D loss: (2.915)(R 2.912, F 2.919)]  [G loss: -65.972] \n",
                        "2145 [D loss: (2.917)(R 2.914, F 2.920)]  [G loss: -66.000] \n",
                        "2146 [D loss: (2.918)(R 2.915, F 2.922)]  [G loss: -66.029] \n",
                        "2147 [D loss: (2.920)(R 2.917, F 2.923)]  [G loss: -66.057] \n",
                        "2148 [D loss: (2.921)(R 2.918, F 2.924)]  [G loss: -66.086] \n",
                        "2149 [D loss: (2.923)(R 2.920, F 2.926)]  [G loss: -66.113] \n",
                        "2150 [D loss: (2.924)(R 2.921, F 2.928)]  [G loss: -66.141] \n",
                        "2151 [D loss: (2.926)(R 2.923, F 2.929)]  [G loss: -66.169] \n",
                        "2152 [D loss: (2.927)(R 2.924, F 2.930)]  [G loss: -66.196] \n",
                        "2153 [D loss: (2.929)(R 2.925, F 2.932)]  [G loss: -66.224] \n",
                        "2154 [D loss: (2.930)(R 2.926, F 2.933)]  [G loss: -66.252] \n",
                        "2155 [D loss: (2.932)(R 2.928, F 2.935)]  [G loss: -66.280] \n",
                        "2156 [D loss: (2.933)(R 2.930, F 2.936)]  [G loss: -66.307] \n",
                        "2157 [D loss: (2.934)(R 2.931, F 2.937)]  [G loss: -66.336] \n",
                        "2158 [D loss: (2.936)(R 2.933, F 2.939)]  [G loss: -66.362] \n",
                        "2159 [D loss: (2.937)(R 2.934, F 2.940)]  [G loss: -66.391] \n",
                        "2160 [D loss: (2.939)(R 2.936, F 2.942)]  [G loss: -66.418] \n",
                        "2161 [D loss: (2.940)(R 2.937, F 2.943)]  [G loss: -66.445] \n",
                        "2162 [D loss: (2.941)(R 2.938, F 2.944)]  [G loss: -66.474] \n",
                        "2163 [D loss: (2.943)(R 2.940, F 2.946)]  [G loss: -66.500] \n",
                        "2164 [D loss: (2.944)(R 2.941, F 2.947)]  [G loss: -66.529] \n",
                        "2165 [D loss: (2.946)(R 2.943, F 2.949)]  [G loss: -66.556] \n",
                        "2166 [D loss: (2.947)(R 2.944, F 2.950)]  [G loss: -66.583] \n",
                        "2167 [D loss: (2.948)(R 2.945, F 2.951)]  [G loss: -66.611] \n",
                        "2168 [D loss: (2.950)(R 2.947, F 2.953)]  [G loss: -66.638] \n",
                        "2169 [D loss: (2.951)(R 2.948, F 2.954)]  [G loss: -66.664] \n",
                        "2170 [D loss: (2.952)(R 2.949, F 2.956)]  [G loss: -66.691] \n",
                        "2171 [D loss: (2.954)(R 2.951, F 2.957)]  [G loss: -66.718] \n",
                        "2172 [D loss: (2.955)(R 2.952, F 2.958)]  [G loss: -66.746] \n",
                        "2173 [D loss: (2.957)(R 2.954, F 2.960)]  [G loss: -66.772] \n",
                        "2174 [D loss: (2.958)(R 2.955, F 2.961)]  [G loss: -66.798] \n",
                        "2175 [D loss: (2.959)(R 2.956, F 2.962)]  [G loss: -66.826] \n",
                        "2176 [D loss: (2.961)(R 2.957, F 2.964)]  [G loss: -66.854] \n",
                        "2177 [D loss: (2.962)(R 2.959, F 2.965)]  [G loss: -66.881] \n",
                        "2178 [D loss: (2.963)(R 2.960, F 2.966)]  [G loss: -66.909] \n",
                        "2179 [D loss: (2.965)(R 2.961, F 2.968)]  [G loss: -66.936] \n",
                        "2180 [D loss: (2.966)(R 2.963, F 2.969)]  [G loss: -66.963] \n",
                        "2181 [D loss: (2.967)(R 2.964, F 2.970)]  [G loss: -66.991] \n",
                        "2182 [D loss: (2.969)(R 2.966, F 2.972)]  [G loss: -67.017] \n",
                        "2183 [D loss: (2.970)(R 2.967, F 2.973)]  [G loss: -67.045] \n",
                        "2184 [D loss: (2.971)(R 2.968, F 2.974)]  [G loss: -67.072] \n",
                        "2185 [D loss: (2.973)(R 2.970, F 2.976)]  [G loss: -67.099] \n",
                        "2186 [D loss: (2.974)(R 2.971, F 2.977)]  [G loss: -67.127] \n",
                        "2187 [D loss: (2.975)(R 2.972, F 2.978)]  [G loss: -67.155] \n",
                        "2188 [D loss: (2.977)(R 2.974, F 2.980)]  [G loss: -67.182] \n",
                        "2189 [D loss: (2.978)(R 2.975, F 2.981)]  [G loss: -67.210] \n",
                        "2190 [D loss: (2.980)(R 2.977, F 2.983)]  [G loss: -67.237] \n",
                        "2191 [D loss: (2.981)(R 2.978, F 2.984)]  [G loss: -67.265] \n",
                        "2192 [D loss: (2.982)(R 2.979, F 2.985)]  [G loss: -67.293] \n",
                        "2193 [D loss: (2.984)(R 2.981, F 2.987)]  [G loss: -67.320] \n",
                        "2194 [D loss: (2.985)(R 2.982, F 2.988)]  [G loss: -67.348] \n",
                        "2195 [D loss: (2.987)(R 2.984, F 2.990)]  [G loss: -67.374] \n",
                        "2196 [D loss: (2.988)(R 2.985, F 2.991)]  [G loss: -67.402] \n",
                        "2197 [D loss: (2.989)(R 2.986, F 2.992)]  [G loss: -67.430] \n",
                        "2198 [D loss: (2.991)(R 2.988, F 2.994)]  [G loss: -67.457] \n",
                        "2199 [D loss: (2.992)(R 2.989, F 2.995)]  [G loss: -67.484] \n",
                        "2200 [D loss: (2.993)(R 2.990, F 2.997)]  [G loss: -67.512] \n",
                        "2201 [D loss: (2.995)(R 2.992, F 2.998)]  [G loss: -67.540] \n",
                        "2202 [D loss: (2.996)(R 2.993, F 2.999)]  [G loss: -67.566] \n",
                        "2203 [D loss: (2.997)(R 2.994, F 3.001)]  [G loss: -67.595] \n",
                        "2204 [D loss: (2.999)(R 2.996, F 3.002)]  [G loss: -67.622] \n",
                        "2205 [D loss: (3.000)(R 2.997, F 3.003)]  [G loss: -67.650] \n",
                        "2206 [D loss: (3.002)(R 2.999, F 3.005)]  [G loss: -67.677] \n",
                        "2207 [D loss: (3.003)(R 3.000, F 3.006)]  [G loss: -67.705] \n",
                        "2208 [D loss: (3.005)(R 3.002, F 3.008)]  [G loss: -67.732] \n",
                        "2209 [D loss: (3.006)(R 3.003, F 3.009)]  [G loss: -67.760] \n",
                        "2210 [D loss: (3.008)(R 3.005, F 3.011)]  [G loss: -67.787] \n",
                        "2211 [D loss: (3.009)(R 3.006, F 3.012)]  [G loss: -67.814] \n",
                        "2212 [D loss: (3.010)(R 3.007, F 3.013)]  [G loss: -67.842] \n",
                        "2213 [D loss: (3.012)(R 3.009, F 3.015)]  [G loss: -67.868] \n",
                        "2214 [D loss: (3.013)(R 3.010, F 3.016)]  [G loss: -67.896] \n",
                        "2215 [D loss: (3.014)(R 3.011, F 3.017)]  [G loss: -67.922] \n",
                        "2216 [D loss: (3.016)(R 3.013, F 3.019)]  [G loss: -67.948] \n",
                        "2217 [D loss: (3.017)(R 3.014, F 3.020)]  [G loss: -67.975] \n",
                        "2218 [D loss: (3.018)(R 3.015, F 3.021)]  [G loss: -68.002] \n",
                        "2219 [D loss: (3.019)(R 3.016, F 3.023)]  [G loss: -68.030] \n",
                        "2220 [D loss: (3.021)(R 3.018, F 3.024)]  [G loss: -68.056] \n",
                        "2221 [D loss: (3.022)(R 3.019, F 3.025)]  [G loss: -68.083] \n",
                        "2222 [D loss: (3.024)(R 3.021, F 3.027)]  [G loss: -68.110] \n",
                        "2223 [D loss: (3.025)(R 3.022, F 3.028)]  [G loss: -68.136] \n",
                        "2224 [D loss: (3.026)(R 3.023, F 3.029)]  [G loss: -68.162] \n",
                        "2225 [D loss: (3.028)(R 3.025, F 3.031)]  [G loss: -68.188] \n",
                        "2226 [D loss: (3.029)(R 3.026, F 3.032)]  [G loss: -68.215] \n",
                        "2227 [D loss: (3.030)(R 3.027, F 3.033)]  [G loss: -68.241] \n",
                        "2228 [D loss: (3.031)(R 3.028, F 3.034)]  [G loss: -68.268] \n",
                        "2229 [D loss: (3.033)(R 3.030, F 3.036)]  [G loss: -68.294] \n",
                        "2230 [D loss: (3.034)(R 3.031, F 3.037)]  [G loss: -68.321] \n",
                        "2231 [D loss: (3.035)(R 3.032, F 3.038)]  [G loss: -68.347] \n",
                        "2232 [D loss: (3.036)(R 3.033, F 3.039)]  [G loss: -68.375] \n",
                        "2233 [D loss: (3.038)(R 3.035, F 3.041)]  [G loss: -68.401] \n",
                        "2234 [D loss: (3.039)(R 3.036, F 3.042)]  [G loss: -68.428] \n",
                        "2235 [D loss: (3.040)(R 3.037, F 3.043)]  [G loss: -68.456] \n",
                        "2236 [D loss: (3.042)(R 3.039, F 3.045)]  [G loss: -68.482] \n",
                        "2237 [D loss: (3.043)(R 3.040, F 3.046)]  [G loss: -68.509] \n",
                        "2238 [D loss: (3.044)(R 3.041, F 3.047)]  [G loss: -68.535] \n",
                        "2239 [D loss: (3.045)(R 3.042, F 3.048)]  [G loss: -68.562] \n",
                        "2240 [D loss: (3.047)(R 3.044, F 3.050)]  [G loss: -68.588] \n",
                        "2241 [D loss: (3.048)(R 3.045, F 3.051)]  [G loss: -68.615] \n",
                        "2242 [D loss: (3.049)(R 3.046, F 3.052)]  [G loss: -68.642] \n",
                        "2243 [D loss: (3.051)(R 3.048, F 3.054)]  [G loss: -68.670] \n",
                        "2244 [D loss: (3.052)(R 3.049, F 3.055)]  [G loss: -68.697] \n",
                        "2245 [D loss: (3.054)(R 3.051, F 3.057)]  [G loss: -68.725] \n",
                        "2246 [D loss: (3.055)(R 3.052, F 3.058)]  [G loss: -68.752] \n",
                        "2247 [D loss: (3.057)(R 3.054, F 3.060)]  [G loss: -68.779] \n",
                        "2248 [D loss: (3.058)(R 3.055, F 3.061)]  [G loss: -68.805] \n",
                        "2249 [D loss: (3.059)(R 3.056, F 3.062)]  [G loss: -68.832] \n",
                        "2250 [D loss: (3.061)(R 3.058, F 3.064)]  [G loss: -68.858] \n",
                        "2251 [D loss: (3.062)(R 3.059, F 3.065)]  [G loss: -68.884] \n",
                        "2252 [D loss: (3.064)(R 3.061, F 3.067)]  [G loss: -68.910] \n",
                        "2253 [D loss: (3.065)(R 3.062, F 3.068)]  [G loss: -68.937] \n",
                        "2254 [D loss: (3.066)(R 3.063, F 3.069)]  [G loss: -68.963] \n",
                        "2255 [D loss: (3.067)(R 3.064, F 3.070)]  [G loss: -68.989] \n",
                        "2256 [D loss: (3.068)(R 3.065, F 3.072)]  [G loss: -69.016] \n",
                        "2257 [D loss: (3.070)(R 3.067, F 3.073)]  [G loss: -69.042] \n",
                        "2258 [D loss: (3.071)(R 3.068, F 3.074)]  [G loss: -69.069] \n",
                        "2259 [D loss: (3.073)(R 3.069, F 3.076)]  [G loss: -69.096] \n",
                        "2260 [D loss: (3.074)(R 3.071, F 3.077)]  [G loss: -69.122] \n",
                        "2261 [D loss: (3.075)(R 3.072, F 3.078)]  [G loss: -69.149] \n",
                        "2262 [D loss: (3.077)(R 3.074, F 3.080)]  [G loss: -69.174] \n",
                        "2263 [D loss: (3.078)(R 3.075, F 3.081)]  [G loss: -69.200] \n",
                        "2264 [D loss: (3.079)(R 3.077, F 3.082)]  [G loss: -69.224] \n",
                        "2265 [D loss: (3.080)(R 3.078, F 3.083)]  [G loss: -69.249] \n",
                        "2266 [D loss: (3.082)(R 3.079, F 3.085)]  [G loss: -69.273] \n",
                        "2267 [D loss: (3.083)(R 3.080, F 3.086)]  [G loss: -69.298] \n",
                        "2268 [D loss: (3.084)(R 3.081, F 3.087)]  [G loss: -69.323] \n",
                        "2269 [D loss: (3.085)(R 3.082, F 3.088)]  [G loss: -69.348] \n",
                        "2270 [D loss: (3.086)(R 3.083, F 3.089)]  [G loss: -69.374] \n",
                        "2271 [D loss: (3.087)(R 3.084, F 3.090)]  [G loss: -69.401] \n",
                        "2272 [D loss: (3.089)(R 3.086, F 3.092)]  [G loss: -69.427] \n",
                        "2273 [D loss: (3.090)(R 3.087, F 3.093)]  [G loss: -69.454] \n",
                        "2274 [D loss: (3.091)(R 3.088, F 3.094)]  [G loss: -69.481] \n",
                        "2275 [D loss: (3.093)(R 3.090, F 3.096)]  [G loss: -69.506] \n",
                        "2276 [D loss: (3.094)(R 3.091, F 3.097)]  [G loss: -69.532] \n",
                        "2277 [D loss: (3.095)(R 3.092, F 3.098)]  [G loss: -69.557] \n",
                        "2278 [D loss: (3.097)(R 3.094, F 3.100)]  [G loss: -69.581] \n",
                        "2279 [D loss: (3.098)(R 3.095, F 3.100)]  [G loss: -69.606] \n",
                        "2280 [D loss: (3.099)(R 3.096, F 3.102)]  [G loss: -69.630] \n",
                        "2281 [D loss: (3.100)(R 3.097, F 3.103)]  [G loss: -69.654] \n",
                        "2282 [D loss: (3.101)(R 3.098, F 3.104)]  [G loss: -69.680] \n",
                        "2283 [D loss: (3.102)(R 3.099, F 3.105)]  [G loss: -69.704] \n",
                        "2284 [D loss: (3.103)(R 3.100, F 3.106)]  [G loss: -69.730] \n",
                        "2285 [D loss: (3.104)(R 3.101, F 3.107)]  [G loss: -69.757] \n",
                        "2286 [D loss: (3.106)(R 3.103, F 3.109)]  [G loss: -69.783] \n",
                        "2287 [D loss: (3.107)(R 3.104, F 3.110)]  [G loss: -69.810] \n",
                        "2288 [D loss: (3.108)(R 3.105, F 3.111)]  [G loss: -69.837] \n",
                        "2289 [D loss: (3.110)(R 3.107, F 3.113)]  [G loss: -69.864] \n",
                        "2290 [D loss: (3.111)(R 3.108, F 3.114)]  [G loss: -69.890] \n",
                        "2291 [D loss: (3.112)(R 3.109, F 3.115)]  [G loss: -69.917] \n",
                        "2292 [D loss: (3.114)(R 3.111, F 3.117)]  [G loss: -69.943] \n",
                        "2293 [D loss: (3.115)(R 3.112, F 3.118)]  [G loss: -69.968] \n",
                        "2294 [D loss: (3.116)(R 3.113, F 3.119)]  [G loss: -69.993] \n",
                        "2295 [D loss: (3.117)(R 3.114, F 3.120)]  [G loss: -70.020] \n",
                        "2296 [D loss: (3.119)(R 3.116, F 3.122)]  [G loss: -70.045] \n",
                        "2297 [D loss: (3.120)(R 3.117, F 3.123)]  [G loss: -70.069] \n",
                        "2298 [D loss: (3.121)(R 3.118, F 3.124)]  [G loss: -70.095] \n",
                        "2299 [D loss: (3.123)(R 3.120, F 3.126)]  [G loss: -70.121] \n",
                        "2300 [D loss: (3.124)(R 3.121, F 3.127)]  [G loss: -70.147] \n",
                        "2301 [D loss: (3.125)(R 3.122, F 3.128)]  [G loss: -70.172] \n",
                        "2302 [D loss: (3.127)(R 3.124, F 3.129)]  [G loss: -70.198] \n",
                        "2303 [D loss: (3.128)(R 3.125, F 3.131)]  [G loss: -70.223] \n",
                        "2304 [D loss: (3.129)(R 3.126, F 3.132)]  [G loss: -70.248] \n",
                        "2305 [D loss: (3.130)(R 3.127, F 3.133)]  [G loss: -70.274] \n",
                        "2306 [D loss: (3.132)(R 3.129, F 3.135)]  [G loss: -70.298] \n",
                        "2307 [D loss: (3.133)(R 3.130, F 3.136)]  [G loss: -70.324] \n",
                        "2308 [D loss: (3.134)(R 3.131, F 3.137)]  [G loss: -70.350] \n",
                        "2309 [D loss: (3.135)(R 3.132, F 3.138)]  [G loss: -70.375] \n",
                        "2310 [D loss: (3.136)(R 3.133, F 3.139)]  [G loss: -70.401] \n",
                        "2311 [D loss: (3.138)(R 3.135, F 3.141)]  [G loss: -70.426] \n",
                        "2312 [D loss: (3.139)(R 3.136, F 3.142)]  [G loss: -70.451] \n",
                        "2313 [D loss: (3.140)(R 3.137, F 3.143)]  [G loss: -70.477] \n",
                        "2314 [D loss: (3.141)(R 3.138, F 3.144)]  [G loss: -70.503] \n",
                        "2315 [D loss: (3.142)(R 3.139, F 3.145)]  [G loss: -70.529] \n",
                        "2316 [D loss: (3.144)(R 3.141, F 3.147)]  [G loss: -70.553] \n",
                        "2317 [D loss: (3.145)(R 3.142, F 3.148)]  [G loss: -70.578] \n",
                        "2318 [D loss: (3.146)(R 3.143, F 3.149)]  [G loss: -70.604] \n",
                        "2319 [D loss: (3.148)(R 3.145, F 3.150)]  [G loss: -70.629] \n",
                        "2320 [D loss: (3.149)(R 3.146, F 3.152)]  [G loss: -70.654] \n",
                        "2321 [D loss: (3.150)(R 3.147, F 3.153)]  [G loss: -70.678] \n",
                        "2322 [D loss: (3.151)(R 3.148, F 3.154)]  [G loss: -70.704] \n",
                        "2323 [D loss: (3.152)(R 3.149, F 3.155)]  [G loss: -70.729] \n",
                        "2324 [D loss: (3.154)(R 3.151, F 3.157)]  [G loss: -70.755] \n",
                        "2325 [D loss: (3.155)(R 3.152, F 3.158)]  [G loss: -70.781] \n",
                        "2326 [D loss: (3.156)(R 3.153, F 3.159)]  [G loss: -70.806] \n",
                        "2327 [D loss: (3.157)(R 3.154, F 3.160)]  [G loss: -70.831] \n",
                        "2328 [D loss: (3.159)(R 3.156, F 3.162)]  [G loss: -70.856] \n",
                        "2329 [D loss: (3.160)(R 3.157, F 3.163)]  [G loss: -70.882] \n",
                        "2330 [D loss: (3.161)(R 3.158, F 3.164)]  [G loss: -70.906] \n",
                        "2331 [D loss: (3.162)(R 3.159, F 3.165)]  [G loss: -70.931] \n",
                        "2332 [D loss: (3.164)(R 3.161, F 3.167)]  [G loss: -70.956] \n",
                        "2333 [D loss: (3.165)(R 3.162, F 3.168)]  [G loss: -70.981] \n",
                        "2334 [D loss: (3.166)(R 3.163, F 3.169)]  [G loss: -71.006] \n",
                        "2335 [D loss: (3.167)(R 3.164, F 3.170)]  [G loss: -71.031] \n",
                        "2336 [D loss: (3.168)(R 3.166, F 3.171)]  [G loss: -71.054] \n",
                        "2337 [D loss: (3.169)(R 3.166, F 3.172)]  [G loss: -71.080] \n",
                        "2338 [D loss: (3.171)(R 3.168, F 3.174)]  [G loss: -71.105] \n",
                        "2339 [D loss: (3.172)(R 3.169, F 3.175)]  [G loss: -71.130] \n",
                        "2340 [D loss: (3.173)(R 3.170, F 3.176)]  [G loss: -71.155] \n",
                        "2341 [D loss: (3.175)(R 3.172, F 3.178)]  [G loss: -71.181] \n",
                        "2342 [D loss: (3.176)(R 3.173, F 3.179)]  [G loss: -71.205] \n",
                        "2343 [D loss: (3.177)(R 3.174, F 3.180)]  [G loss: -71.230] \n",
                        "2344 [D loss: (3.178)(R 3.176, F 3.181)]  [G loss: -71.254] \n",
                        "2345 [D loss: (3.179)(R 3.177, F 3.182)]  [G loss: -71.279] \n",
                        "2346 [D loss: (3.181)(R 3.178, F 3.184)]  [G loss: -71.303] \n",
                        "2347 [D loss: (3.182)(R 3.179, F 3.185)]  [G loss: -71.329] \n",
                        "2348 [D loss: (3.183)(R 3.180, F 3.186)]  [G loss: -71.353] \n",
                        "2349 [D loss: (3.184)(R 3.181, F 3.187)]  [G loss: -71.379] \n",
                        "2350 [D loss: (3.186)(R 3.183, F 3.189)]  [G loss: -71.403] \n",
                        "2351 [D loss: (3.187)(R 3.184, F 3.190)]  [G loss: -71.427] \n",
                        "2352 [D loss: (3.188)(R 3.185, F 3.191)]  [G loss: -71.453] \n",
                        "2353 [D loss: (3.190)(R 3.187, F 3.193)]  [G loss: -71.477] \n",
                        "2354 [D loss: (3.191)(R 3.188, F 3.194)]  [G loss: -71.502] \n",
                        "2355 [D loss: (3.192)(R 3.189, F 3.195)]  [G loss: -71.527] \n",
                        "2356 [D loss: (3.194)(R 3.191, F 3.196)]  [G loss: -71.551] \n",
                        "2357 [D loss: (3.195)(R 3.192, F 3.198)]  [G loss: -71.575] \n",
                        "2358 [D loss: (3.196)(R 3.193, F 3.199)]  [G loss: -71.599] \n",
                        "2359 [D loss: (3.197)(R 3.194, F 3.200)]  [G loss: -71.624] \n",
                        "2360 [D loss: (3.198)(R 3.195, F 3.201)]  [G loss: -71.649] \n",
                        "2361 [D loss: (3.199)(R 3.196, F 3.202)]  [G loss: -71.674] \n",
                        "2362 [D loss: (3.200)(R 3.198, F 3.203)]  [G loss: -71.698] \n",
                        "2363 [D loss: (3.202)(R 3.199, F 3.204)]  [G loss: -71.724] \n",
                        "2364 [D loss: (3.203)(R 3.200, F 3.206)]  [G loss: -71.747] \n",
                        "2365 [D loss: (3.204)(R 3.201, F 3.207)]  [G loss: -71.772] \n",
                        "2366 [D loss: (3.206)(R 3.203, F 3.208)]  [G loss: -71.796] \n",
                        "2367 [D loss: (3.207)(R 3.204, F 3.209)]  [G loss: -71.820] \n",
                        "2368 [D loss: (3.208)(R 3.205, F 3.211)]  [G loss: -71.844] \n",
                        "2369 [D loss: (3.209)(R 3.206, F 3.212)]  [G loss: -71.869] \n",
                        "2370 [D loss: (3.210)(R 3.207, F 3.213)]  [G loss: -71.893] \n",
                        "2371 [D loss: (3.211)(R 3.208, F 3.214)]  [G loss: -71.917] \n",
                        "2372 [D loss: (3.212)(R 3.210, F 3.215)]  [G loss: -71.941] \n",
                        "2373 [D loss: (3.213)(R 3.211, F 3.216)]  [G loss: -71.966] \n",
                        "2374 [D loss: (3.215)(R 3.212, F 3.218)]  [G loss: -71.989] \n",
                        "2375 [D loss: (3.216)(R 3.213, F 3.219)]  [G loss: -72.015] \n",
                        "2376 [D loss: (3.217)(R 3.214, F 3.220)]  [G loss: -72.038] \n",
                        "2377 [D loss: (3.218)(R 3.215, F 3.221)]  [G loss: -72.063] \n",
                        "2378 [D loss: (3.220)(R 3.217, F 3.223)]  [G loss: -72.087] \n",
                        "2379 [D loss: (3.221)(R 3.218, F 3.224)]  [G loss: -72.112] \n",
                        "2380 [D loss: (3.222)(R 3.220, F 3.225)]  [G loss: -72.134] \n",
                        "2381 [D loss: (3.223)(R 3.221, F 3.226)]  [G loss: -72.159] \n",
                        "2382 [D loss: (3.225)(R 3.222, F 3.227)]  [G loss: -72.183] \n",
                        "2383 [D loss: (3.226)(R 3.223, F 3.229)]  [G loss: -72.207] \n",
                        "2384 [D loss: (3.227)(R 3.224, F 3.230)]  [G loss: -72.232] \n",
                        "2385 [D loss: (3.228)(R 3.225, F 3.231)]  [G loss: -72.256] \n",
                        "2386 [D loss: (3.229)(R 3.226, F 3.232)]  [G loss: -72.281] \n",
                        "2387 [D loss: (3.231)(R 3.228, F 3.233)]  [G loss: -72.305] \n",
                        "2388 [D loss: (3.232)(R 3.229, F 3.235)]  [G loss: -72.329] \n",
                        "2389 [D loss: (3.233)(R 3.230, F 3.236)]  [G loss: -72.354] \n",
                        "2390 [D loss: (3.234)(R 3.231, F 3.237)]  [G loss: -72.378] \n",
                        "2391 [D loss: (3.235)(R 3.233, F 3.238)]  [G loss: -72.401] \n",
                        "2392 [D loss: (3.236)(R 3.233, F 3.239)]  [G loss: -72.427] \n",
                        "2393 [D loss: (3.238)(R 3.235, F 3.241)]  [G loss: -72.449] \n",
                        "2394 [D loss: (3.239)(R 3.236, F 3.242)]  [G loss: -72.472] \n",
                        "2395 [D loss: (3.240)(R 3.237, F 3.243)]  [G loss: -72.497] \n",
                        "2396 [D loss: (3.241)(R 3.238, F 3.244)]  [G loss: -72.522] \n",
                        "2397 [D loss: (3.243)(R 3.240, F 3.246)]  [G loss: -72.545] \n",
                        "2398 [D loss: (3.244)(R 3.241, F 3.247)]  [G loss: -72.569] \n",
                        "2399 [D loss: (3.245)(R 3.242, F 3.248)]  [G loss: -72.592] \n",
                        "2400 [D loss: (3.246)(R 3.243, F 3.249)]  [G loss: -72.616] \n",
                        "2401 [D loss: (3.247)(R 3.244, F 3.250)]  [G loss: -72.640] \n",
                        "2402 [D loss: (3.248)(R 3.245, F 3.251)]  [G loss: -72.664] \n",
                        "2403 [D loss: (3.249)(R 3.246, F 3.252)]  [G loss: -72.689] \n",
                        "2404 [D loss: (3.251)(R 3.248, F 3.254)]  [G loss: -72.711] \n",
                        "2405 [D loss: (3.252)(R 3.249, F 3.255)]  [G loss: -72.736] \n",
                        "2406 [D loss: (3.253)(R 3.250, F 3.256)]  [G loss: -72.760] \n",
                        "2407 [D loss: (3.254)(R 3.251, F 3.257)]  [G loss: -72.783] \n",
                        "2408 [D loss: (3.255)(R 3.252, F 3.258)]  [G loss: -72.808] \n",
                        "2409 [D loss: (3.257)(R 3.254, F 3.259)]  [G loss: -72.831] \n",
                        "2410 [D loss: (3.258)(R 3.255, F 3.260)]  [G loss: -72.855] \n",
                        "2411 [D loss: (3.259)(R 3.256, F 3.262)]  [G loss: -72.879] \n",
                        "2412 [D loss: (3.260)(R 3.257, F 3.263)]  [G loss: -72.903] \n",
                        "2413 [D loss: (3.261)(R 3.258, F 3.264)]  [G loss: -72.926] \n",
                        "2414 [D loss: (3.262)(R 3.259, F 3.265)]  [G loss: -72.951] \n",
                        "2415 [D loss: (3.263)(R 3.260, F 3.266)]  [G loss: -72.975] \n",
                        "2416 [D loss: (3.264)(R 3.261, F 3.267)]  [G loss: -72.999] \n",
                        "2417 [D loss: (3.265)(R 3.263, F 3.268)]  [G loss: -73.023] \n",
                        "2418 [D loss: (3.267)(R 3.264, F 3.270)]  [G loss: -73.046] \n",
                        "2419 [D loss: (3.268)(R 3.265, F 3.270)]  [G loss: -73.071] \n",
                        "2420 [D loss: (3.269)(R 3.266, F 3.272)]  [G loss: -73.094] \n",
                        "2421 [D loss: (3.270)(R 3.267, F 3.273)]  [G loss: -73.118] \n",
                        "2422 [D loss: (3.271)(R 3.269, F 3.274)]  [G loss: -73.142] \n",
                        "2423 [D loss: (3.273)(R 3.270, F 3.275)]  [G loss: -73.166] \n",
                        "2424 [D loss: (3.274)(R 3.271, F 3.277)]  [G loss: -73.189] \n",
                        "2425 [D loss: (3.275)(R 3.272, F 3.277)]  [G loss: -73.214] \n",
                        "2426 [D loss: (3.276)(R 3.273, F 3.279)]  [G loss: -73.236] \n",
                        "2427 [D loss: (3.277)(R 3.274, F 3.280)]  [G loss: -73.261] \n",
                        "2428 [D loss: (3.278)(R 3.276, F 3.281)]  [G loss: -73.284] \n",
                        "2429 [D loss: (3.279)(R 3.277, F 3.282)]  [G loss: -73.308] \n",
                        "2430 [D loss: (3.280)(R 3.278, F 3.283)]  [G loss: -73.332] \n",
                        "2431 [D loss: (3.282)(R 3.279, F 3.285)]  [G loss: -73.355] \n",
                        "2432 [D loss: (3.283)(R 3.280, F 3.286)]  [G loss: -73.379] \n",
                        "2433 [D loss: (3.284)(R 3.281, F 3.287)]  [G loss: -73.403] \n",
                        "2434 [D loss: (3.285)(R 3.282, F 3.288)]  [G loss: -73.426] \n",
                        "2435 [D loss: (3.286)(R 3.284, F 3.289)]  [G loss: -73.450] \n",
                        "2436 [D loss: (3.288)(R 3.285, F 3.290)]  [G loss: -73.474] \n",
                        "2437 [D loss: (3.289)(R 3.286, F 3.291)]  [G loss: -73.498] \n",
                        "2438 [D loss: (3.290)(R 3.287, F 3.293)]  [G loss: -73.521] \n",
                        "2439 [D loss: (3.291)(R 3.288, F 3.294)]  [G loss: -73.545] \n",
                        "2440 [D loss: (3.292)(R 3.290, F 3.295)]  [G loss: -73.567] \n",
                        "2441 [D loss: (3.293)(R 3.291, F 3.296)]  [G loss: -73.591] \n",
                        "2442 [D loss: (3.294)(R 3.292, F 3.297)]  [G loss: -73.615] \n",
                        "2443 [D loss: (3.296)(R 3.293, F 3.298)]  [G loss: -73.639] \n",
                        "2444 [D loss: (3.297)(R 3.294, F 3.300)]  [G loss: -73.662] \n",
                        "2445 [D loss: (3.298)(R 3.295, F 3.301)]  [G loss: -73.685] \n",
                        "2446 [D loss: (3.299)(R 3.296, F 3.302)]  [G loss: -73.709] \n",
                        "2447 [D loss: (3.301)(R 3.298, F 3.303)]  [G loss: -73.732] \n",
                        "2448 [D loss: (3.302)(R 3.299, F 3.305)]  [G loss: -73.756] \n",
                        "2449 [D loss: (3.303)(R 3.300, F 3.306)]  [G loss: -73.780] \n",
                        "2450 [D loss: (3.304)(R 3.301, F 3.307)]  [G loss: -73.803] \n",
                        "2451 [D loss: (3.305)(R 3.302, F 3.308)]  [G loss: -73.827] \n",
                        "2452 [D loss: (3.306)(R 3.304, F 3.309)]  [G loss: -73.850] \n",
                        "2453 [D loss: (3.307)(R 3.304, F 3.310)]  [G loss: -73.875] \n",
                        "2454 [D loss: (3.309)(R 3.306, F 3.312)]  [G loss: -73.897] \n",
                        "2455 [D loss: (3.310)(R 3.307, F 3.313)]  [G loss: -73.921] \n",
                        "2456 [D loss: (3.311)(R 3.308, F 3.314)]  [G loss: -73.944] \n",
                        "2457 [D loss: (3.312)(R 3.309, F 3.315)]  [G loss: -73.968] \n",
                        "2458 [D loss: (3.313)(R 3.311, F 3.316)]  [G loss: -73.991] \n",
                        "2459 [D loss: (3.314)(R 3.312, F 3.317)]  [G loss: -74.016] \n",
                        "2460 [D loss: (3.316)(R 3.313, F 3.319)]  [G loss: -74.038] \n",
                        "2461 [D loss: (3.317)(R 3.314, F 3.320)]  [G loss: -74.061] \n",
                        "2462 [D loss: (3.318)(R 3.315, F 3.321)]  [G loss: -74.085] \n",
                        "2463 [D loss: (3.319)(R 3.316, F 3.322)]  [G loss: -74.108] \n",
                        "2464 [D loss: (3.320)(R 3.317, F 3.323)]  [G loss: -74.132] \n",
                        "2465 [D loss: (3.322)(R 3.319, F 3.324)]  [G loss: -74.155] \n",
                        "2466 [D loss: (3.323)(R 3.320, F 3.326)]  [G loss: -74.178] \n",
                        "2467 [D loss: (3.324)(R 3.321, F 3.327)]  [G loss: -74.202] \n",
                        "2468 [D loss: (3.325)(R 3.322, F 3.328)]  [G loss: -74.224] \n",
                        "2469 [D loss: (3.326)(R 3.323, F 3.329)]  [G loss: -74.248] \n",
                        "2470 [D loss: (3.327)(R 3.325, F 3.330)]  [G loss: -74.270] \n",
                        "2471 [D loss: (3.328)(R 3.326, F 3.331)]  [G loss: -74.293] \n",
                        "2472 [D loss: (3.329)(R 3.327, F 3.332)]  [G loss: -74.317] \n",
                        "2473 [D loss: (3.331)(R 3.328, F 3.333)]  [G loss: -74.339] \n",
                        "2474 [D loss: (3.332)(R 3.329, F 3.334)]  [G loss: -74.362] \n",
                        "2475 [D loss: (3.333)(R 3.330, F 3.336)]  [G loss: -74.385] \n",
                        "2476 [D loss: (3.334)(R 3.331, F 3.337)]  [G loss: -74.407] \n",
                        "2477 [D loss: (3.335)(R 3.332, F 3.338)]  [G loss: -74.431] \n",
                        "2478 [D loss: (3.336)(R 3.333, F 3.339)]  [G loss: -74.453] \n",
                        "2479 [D loss: (3.337)(R 3.334, F 3.340)]  [G loss: -74.477] \n",
                        "2480 [D loss: (3.338)(R 3.335, F 3.341)]  [G loss: -74.499] \n",
                        "2481 [D loss: (3.339)(R 3.336, F 3.342)]  [G loss: -74.522] \n",
                        "2482 [D loss: (3.340)(R 3.337, F 3.343)]  [G loss: -74.545] \n",
                        "2483 [D loss: (3.342)(R 3.339, F 3.344)]  [G loss: -74.568] \n",
                        "2484 [D loss: (3.343)(R 3.340, F 3.345)]  [G loss: -74.592] \n",
                        "2485 [D loss: (3.344)(R 3.341, F 3.346)]  [G loss: -74.616] \n",
                        "2486 [D loss: (3.345)(R 3.342, F 3.348)]  [G loss: -74.640] \n",
                        "2487 [D loss: (3.346)(R 3.343, F 3.349)]  [G loss: -74.662] \n",
                        "2488 [D loss: (3.347)(R 3.344, F 3.350)]  [G loss: -74.685] \n",
                        "2489 [D loss: (3.348)(R 3.346, F 3.351)]  [G loss: -74.708] \n",
                        "2490 [D loss: (3.349)(R 3.347, F 3.352)]  [G loss: -74.731] \n",
                        "2491 [D loss: (3.351)(R 3.348, F 3.353)]  [G loss: -74.753] \n",
                        "2492 [D loss: (3.351)(R 3.349, F 3.354)]  [G loss: -74.777] \n",
                        "2493 [D loss: (3.353)(R 3.350, F 3.356)]  [G loss: -74.801] \n",
                        "2494 [D loss: (3.354)(R 3.351, F 3.357)]  [G loss: -74.825] \n",
                        "2495 [D loss: (3.355)(R 3.353, F 3.358)]  [G loss: -74.847] \n",
                        "2496 [D loss: (3.357)(R 3.354, F 3.359)]  [G loss: -74.869] \n",
                        "2497 [D loss: (3.357)(R 3.354, F 3.360)]  [G loss: -74.892] \n",
                        "2498 [D loss: (3.359)(R 3.356, F 3.362)]  [G loss: -74.914] \n",
                        "2499 [D loss: (3.360)(R 3.357, F 3.363)]  [G loss: -74.936] \n",
                        "2500 [D loss: (3.361)(R 3.358, F 3.364)]  [G loss: -74.959] \n",
                        "2501 [D loss: (3.362)(R 3.359, F 3.365)]  [G loss: -74.982] \n",
                        "2502 [D loss: (3.363)(R 3.360, F 3.366)]  [G loss: -75.005] \n",
                        "2503 [D loss: (3.364)(R 3.361, F 3.367)]  [G loss: -75.029] \n",
                        "2504 [D loss: (3.365)(R 3.362, F 3.368)]  [G loss: -75.054] \n",
                        "2505 [D loss: (3.367)(R 3.364, F 3.369)]  [G loss: -75.078] \n",
                        "2506 [D loss: (3.368)(R 3.365, F 3.371)]  [G loss: -75.100] \n",
                        "2507 [D loss: (3.368)(R 3.366, F 3.371)]  [G loss: -75.121] \n",
                        "2508 [D loss: (3.368)(R 3.365, F 3.371)]  [G loss: -75.143] \n",
                        "2509 [D loss: (3.369)(R 3.366, F 3.371)]  [G loss: -75.166] \n",
                        "2510 [D loss: (3.370)(R 3.368, F 3.373)]  [G loss: -75.190] \n",
                        "2511 [D loss: (3.373)(R 3.370, F 3.375)]  [G loss: -75.214] \n",
                        "2512 [D loss: (3.375)(R 3.372, F 3.377)]  [G loss: -75.236] \n",
                        "2513 [D loss: (3.376)(R 3.373, F 3.379)]  [G loss: -75.257] \n",
                        "2514 [D loss: (3.377)(R 3.374, F 3.379)]  [G loss: -75.278] \n",
                        "2515 [D loss: (3.376)(R 3.374, F 3.379)]  [G loss: -75.299] \n",
                        "2516 [D loss: (3.376)(R 3.373, F 3.378)]  [G loss: -75.321] \n",
                        "2517 [D loss: (3.374)(R 3.372, F 3.377)]  [G loss: -75.345] \n",
                        "2518 [D loss: (3.373)(R 3.371, F 3.376)]  [G loss: -75.373] \n",
                        "2519 [D loss: (3.372)(R 3.369, F 3.375)]  [G loss: -75.405] \n",
                        "2520 [D loss: (3.372)(R 3.369, F 3.375)]  [G loss: -75.441] \n",
                        "2521 [D loss: (3.374)(R 3.371, F 3.377)]  [G loss: -75.475] \n",
                        "2522 [D loss: (3.378)(R 3.375, F 3.381)]  [G loss: -75.506] \n",
                        "2523 [D loss: (3.381)(R 3.378, F 3.384)]  [G loss: -75.531] \n",
                        "2524 [D loss: (3.382)(R 3.379, F 3.385)]  [G loss: -75.551] \n",
                        "2525 [D loss: (3.380)(R 3.378, F 3.383)]  [G loss: -75.571] \n",
                        "2526 [D loss: (3.378)(R 3.375, F 3.380)]  [G loss: -75.593] \n",
                        "2527 [D loss: (3.374)(R 3.372, F 3.377)]  [G loss: -75.618] \n",
                        "2528 [D loss: (3.372)(R 3.369, F 3.375)]  [G loss: -75.646] \n",
                        "2529 [D loss: (3.369)(R 3.367, F 3.372)]  [G loss: -75.676] \n",
                        "2530 [D loss: (3.369)(R 3.366, F 3.372)]  [G loss: -75.707] \n",
                        "2531 [D loss: (3.370)(R 3.367, F 3.373)]  [G loss: -75.740] \n",
                        "2532 [D loss: (3.372)(R 3.369, F 3.375)]  [G loss: -75.774] \n",
                        "2533 [D loss: (3.377)(R 3.374, F 3.380)]  [G loss: -75.805] \n",
                        "2534 [D loss: (3.381)(R 3.378, F 3.384)]  [G loss: -75.836] \n",
                        "2535 [D loss: (3.386)(R 3.383, F 3.389)]  [G loss: -75.865] \n",
                        "2536 [D loss: (3.391)(R 3.388, F 3.394)]  [G loss: -75.891] \n",
                        "2537 [D loss: (3.396)(R 3.393, F 3.399)]  [G loss: -75.915] \n",
                        "2538 [D loss: (3.401)(R 3.398, F 3.403)]  [G loss: -75.934] \n",
                        "2539 [D loss: (3.404)(R 3.401, F 3.406)]  [G loss: -75.951] \n",
                        "2540 [D loss: (3.405)(R 3.403, F 3.408)]  [G loss: -75.965] \n",
                        "2541 [D loss: (3.406)(R 3.403, F 3.408)]  [G loss: -75.977] \n",
                        "2542 [D loss: (3.404)(R 3.402, F 3.406)]  [G loss: -75.988] \n",
                        "2543 [D loss: (3.402)(R 3.400, F 3.404)]  [G loss: -75.997] \n",
                        "2544 [D loss: (3.398)(R 3.396, F 3.400)]  [G loss: -76.007] \n",
                        "2545 [D loss: (3.393)(R 3.391, F 3.395)]  [G loss: -76.016] \n",
                        "2546 [D loss: (3.388)(R 3.385, F 3.390)]  [G loss: -76.027] \n",
                        "2547 [D loss: (3.382)(R 3.380, F 3.384)]  [G loss: -76.041] \n",
                        "2548 [D loss: (3.378)(R 3.376, F 3.380)]  [G loss: -76.058] \n",
                        "2549 [D loss: (3.375)(R 3.373, F 3.378)]  [G loss: -76.078] \n",
                        "2550 [D loss: (3.374)(R 3.372, F 3.377)]  [G loss: -76.101] \n",
                        "2551 [D loss: (3.374)(R 3.371, F 3.376)]  [G loss: -76.126] \n",
                        "2552 [D loss: (3.374)(R 3.371, F 3.376)]  [G loss: -76.151] \n",
                        "2553 [D loss: (3.375)(R 3.372, F 3.378)]  [G loss: -76.177] \n",
                        "2554 [D loss: (3.377)(R 3.374, F 3.379)]  [G loss: -76.204] \n",
                        "2555 [D loss: (3.379)(R 3.376, F 3.381)]  [G loss: -76.230] \n",
                        "2556 [D loss: (3.381)(R 3.378, F 3.383)]  [G loss: -76.256] \n",
                        "2557 [D loss: (3.382)(R 3.379, F 3.385)]  [G loss: -76.281] \n",
                        "2558 [D loss: (3.383)(R 3.380, F 3.386)]  [G loss: -76.305] \n",
                        "2559 [D loss: (3.384)(R 3.381, F 3.386)]  [G loss: -76.329] \n",
                        "2560 [D loss: (3.384)(R 3.381, F 3.387)]  [G loss: -76.352] \n",
                        "2561 [D loss: (3.385)(R 3.382, F 3.388)]  [G loss: -76.376] \n",
                        "2562 [D loss: (3.385)(R 3.383, F 3.388)]  [G loss: -76.399] \n",
                        "2563 [D loss: (3.388)(R 3.385, F 3.391)]  [G loss: -76.422] \n",
                        "2564 [D loss: (3.391)(R 3.388, F 3.393)]  [G loss: -76.443] \n",
                        "2565 [D loss: (3.394)(R 3.391, F 3.397)]  [G loss: -76.462] \n",
                        "2566 [D loss: (3.397)(R 3.394, F 3.399)]  [G loss: -76.479] \n",
                        "2567 [D loss: (3.399)(R 3.397, F 3.401)]  [G loss: -76.493] \n",
                        "2568 [D loss: (3.401)(R 3.398, F 3.403)]  [G loss: -76.504] \n",
                        "2569 [D loss: (3.402)(R 3.399, F 3.404)]  [G loss: -76.512] \n",
                        "2570 [D loss: (3.402)(R 3.400, F 3.404)]  [G loss: -76.518] \n",
                        "2571 [D loss: (3.401)(R 3.399, F 3.403)]  [G loss: -76.523] \n",
                        "2572 [D loss: (3.401)(R 3.399, F 3.403)]  [G loss: -76.528] \n",
                        "2573 [D loss: (3.400)(R 3.398, F 3.402)]  [G loss: -76.532] \n",
                        "2574 [D loss: (3.400)(R 3.398, F 3.402)]  [G loss: -76.537] \n",
                        "2575 [D loss: (3.400)(R 3.398, F 3.402)]  [G loss: -76.542] \n",
                        "2576 [D loss: (3.399)(R 3.397, F 3.401)]  [G loss: -76.550] \n",
                        "2577 [D loss: (3.399)(R 3.397, F 3.401)]  [G loss: -76.557] \n",
                        "2578 [D loss: (3.399)(R 3.397, F 3.401)]  [G loss: -76.565] \n",
                        "2579 [D loss: (3.397)(R 3.395, F 3.400)]  [G loss: -76.575] \n",
                        "2580 [D loss: (3.396)(R 3.394, F 3.398)]  [G loss: -76.585] \n",
                        "2581 [D loss: (3.394)(R 3.392, F 3.396)]  [G loss: -76.597] \n",
                        "2582 [D loss: (3.392)(R 3.390, F 3.394)]  [G loss: -76.610] \n",
                        "2583 [D loss: (3.391)(R 3.388, F 3.393)]  [G loss: -76.624] \n",
                        "2584 [D loss: (3.390)(R 3.387, F 3.392)]  [G loss: -76.639] \n",
                        "2585 [D loss: (3.390)(R 3.387, F 3.392)]  [G loss: -76.656] \n",
                        "2586 [D loss: (3.390)(R 3.388, F 3.393)]  [G loss: -76.674] \n",
                        "2587 [D loss: (3.391)(R 3.389, F 3.394)]  [G loss: -76.693] \n",
                        "2588 [D loss: (3.393)(R 3.390, F 3.395)]  [G loss: -76.713] \n",
                        "2589 [D loss: (3.394)(R 3.391, F 3.397)]  [G loss: -76.735] \n",
                        "2590 [D loss: (3.395)(R 3.393, F 3.398)]  [G loss: -76.760] \n",
                        "2591 [D loss: (3.397)(R 3.394, F 3.400)]  [G loss: -76.787] \n",
                        "2592 [D loss: (3.398)(R 3.395, F 3.401)]  [G loss: -76.815] \n",
                        "2593 [D loss: (3.400)(R 3.397, F 3.403)]  [G loss: -76.843] \n",
                        "2594 [D loss: (3.402)(R 3.399, F 3.405)]  [G loss: -76.867] \n",
                        "2595 [D loss: (3.403)(R 3.401, F 3.406)]  [G loss: -76.889] \n",
                        "2596 [D loss: (3.404)(R 3.401, F 3.407)]  [G loss: -76.909] \n",
                        "2597 [D loss: (3.404)(R 3.402, F 3.407)]  [G loss: -76.927] \n",
                        "2598 [D loss: (3.404)(R 3.402, F 3.407)]  [G loss: -76.943] \n",
                        "2599 [D loss: (3.403)(R 3.401, F 3.406)]  [G loss: -76.956] \n",
                        "2600 [D loss: (3.402)(R 3.400, F 3.404)]  [G loss: -76.969] \n",
                        "2601 [D loss: (3.400)(R 3.398, F 3.403)]  [G loss: -76.982] \n",
                        "2602 [D loss: (3.399)(R 3.397, F 3.401)]  [G loss: -76.995] \n",
                        "2603 [D loss: (3.398)(R 3.396, F 3.401)]  [G loss: -77.010] \n",
                        "2604 [D loss: (3.397)(R 3.395, F 3.400)]  [G loss: -77.028] \n",
                        "2605 [D loss: (3.397)(R 3.395, F 3.400)]  [G loss: -77.049] \n",
                        "2606 [D loss: (3.398)(R 3.395, F 3.401)]  [G loss: -77.073] \n",
                        "2607 [D loss: (3.399)(R 3.397, F 3.402)]  [G loss: -77.098] \n",
                        "2608 [D loss: (3.401)(R 3.398, F 3.404)]  [G loss: -77.122] \n",
                        "2609 [D loss: (3.402)(R 3.400, F 3.405)]  [G loss: -77.147] \n",
                        "2610 [D loss: (3.404)(R 3.401, F 3.407)]  [G loss: -77.173] \n",
                        "2611 [D loss: (3.405)(R 3.402, F 3.408)]  [G loss: -77.200] \n",
                        "2612 [D loss: (3.407)(R 3.404, F 3.410)]  [G loss: -77.229] \n",
                        "2613 [D loss: (3.410)(R 3.407, F 3.413)]  [G loss: -77.258] \n",
                        "2614 [D loss: (3.413)(R 3.410, F 3.416)]  [G loss: -77.286] \n",
                        "2615 [D loss: (3.415)(R 3.412, F 3.418)]  [G loss: -77.312] \n",
                        "2616 [D loss: (3.418)(R 3.415, F 3.421)]  [G loss: -77.337] \n",
                        "2617 [D loss: (3.420)(R 3.417, F 3.423)]  [G loss: -77.360] \n",
                        "2618 [D loss: (3.422)(R 3.419, F 3.425)]  [G loss: -77.380] \n",
                        "2619 [D loss: (3.424)(R 3.421, F 3.426)]  [G loss: -77.397] \n",
                        "2620 [D loss: (3.425)(R 3.422, F 3.427)]  [G loss: -77.411] \n",
                        "2621 [D loss: (3.425)(R 3.423, F 3.428)]  [G loss: -77.422] \n",
                        "2622 [D loss: (3.426)(R 3.423, F 3.428)]  [G loss: -77.433] \n",
                        "2623 [D loss: (3.426)(R 3.424, F 3.428)]  [G loss: -77.442] \n",
                        "2624 [D loss: (3.426)(R 3.424, F 3.428)]  [G loss: -77.453] \n",
                        "2625 [D loss: (3.426)(R 3.424, F 3.428)]  [G loss: -77.464] \n",
                        "2626 [D loss: (3.427)(R 3.424, F 3.429)]  [G loss: -77.476] \n",
                        "2627 [D loss: (3.427)(R 3.425, F 3.430)]  [G loss: -77.489] \n",
                        "2628 [D loss: (3.428)(R 3.426, F 3.430)]  [G loss: -77.504] \n",
                        "2629 [D loss: (3.428)(R 3.426, F 3.431)]  [G loss: -77.520] \n",
                        "2630 [D loss: (3.429)(R 3.427, F 3.432)]  [G loss: -77.537] \n",
                        "2631 [D loss: (3.430)(R 3.427, F 3.432)]  [G loss: -77.556] \n",
                        "2632 [D loss: (3.431)(R 3.428, F 3.433)]  [G loss: -77.577] \n",
                        "2633 [D loss: (3.432)(R 3.429, F 3.434)]  [G loss: -77.597] \n",
                        "2634 [D loss: (3.433)(R 3.430, F 3.435)]  [G loss: -77.618] \n",
                        "2635 [D loss: (3.434)(R 3.431, F 3.436)]  [G loss: -77.639] \n",
                        "2636 [D loss: (3.435)(R 3.432, F 3.438)]  [G loss: -77.661] \n",
                        "2637 [D loss: (3.436)(R 3.433, F 3.439)]  [G loss: -77.681] \n",
                        "2638 [D loss: (3.437)(R 3.434, F 3.440)]  [G loss: -77.702] \n",
                        "2639 [D loss: (3.438)(R 3.435, F 3.441)]  [G loss: -77.724] \n",
                        "2640 [D loss: (3.439)(R 3.437, F 3.442)]  [G loss: -77.745] \n",
                        "2641 [D loss: (3.441)(R 3.438, F 3.443)]  [G loss: -77.766] \n",
                        "2642 [D loss: (3.442)(R 3.439, F 3.444)]  [G loss: -77.789] \n",
                        "2643 [D loss: (3.443)(R 3.440, F 3.446)]  [G loss: -77.811] \n",
                        "2644 [D loss: (3.444)(R 3.441, F 3.447)]  [G loss: -77.835] \n",
                        "2645 [D loss: (3.445)(R 3.443, F 3.448)]  [G loss: -77.859] \n",
                        "2646 [D loss: (3.447)(R 3.444, F 3.450)]  [G loss: -77.883] \n",
                        "2647 [D loss: (3.448)(R 3.446, F 3.451)]  [G loss: -77.905] \n",
                        "2648 [D loss: (3.450)(R 3.447, F 3.453)]  [G loss: -77.929] \n",
                        "2649 [D loss: (3.451)(R 3.449, F 3.454)]  [G loss: -77.952] \n",
                        "2650 [D loss: (3.453)(R 3.450, F 3.456)]  [G loss: -77.973] \n",
                        "2651 [D loss: (3.454)(R 3.452, F 3.457)]  [G loss: -77.994] \n",
                        "2652 [D loss: (3.456)(R 3.453, F 3.458)]  [G loss: -78.015] \n",
                        "2653 [D loss: (3.457)(R 3.454, F 3.460)]  [G loss: -78.036] \n",
                        "2654 [D loss: (3.458)(R 3.455, F 3.461)]  [G loss: -78.058] \n",
                        "2655 [D loss: (3.460)(R 3.457, F 3.462)]  [G loss: -78.081] \n",
                        "2656 [D loss: (3.461)(R 3.458, F 3.464)]  [G loss: -78.103] \n",
                        "2657 [D loss: (3.462)(R 3.459, F 3.465)]  [G loss: -78.124] \n",
                        "2658 [D loss: (3.463)(R 3.460, F 3.466)]  [G loss: -78.146] \n",
                        "2659 [D loss: (3.465)(R 3.462, F 3.467)]  [G loss: -78.168] \n",
                        "2660 [D loss: (3.467)(R 3.464, F 3.469)]  [G loss: -78.188] \n",
                        "2661 [D loss: (3.468)(R 3.465, F 3.471)]  [G loss: -78.208] \n",
                        "2662 [D loss: (3.469)(R 3.467, F 3.472)]  [G loss: -78.227] \n",
                        "2663 [D loss: (3.470)(R 3.468, F 3.473)]  [G loss: -78.245] \n",
                        "2664 [D loss: (3.471)(R 3.469, F 3.474)]  [G loss: -78.264] \n",
                        "2665 [D loss: (3.472)(R 3.469, F 3.474)]  [G loss: -78.283] \n",
                        "2666 [D loss: (3.473)(R 3.470, F 3.475)]  [G loss: -78.305] \n",
                        "2667 [D loss: (3.473)(R 3.471, F 3.476)]  [G loss: -78.327] \n",
                        "2668 [D loss: (3.474)(R 3.472, F 3.477)]  [G loss: -78.351] \n",
                        "2669 [D loss: (3.476)(R 3.473, F 3.479)]  [G loss: -78.374] \n",
                        "2670 [D loss: (3.477)(R 3.474, F 3.480)]  [G loss: -78.396] \n",
                        "2671 [D loss: (3.477)(R 3.475, F 3.480)]  [G loss: -78.418] \n",
                        "2672 [D loss: (3.477)(R 3.474, F 3.480)]  [G loss: -78.444] \n",
                        "2673 [D loss: (3.477)(R 3.475, F 3.480)]  [G loss: -78.473] \n",
                        "2674 [D loss: (3.479)(R 3.476, F 3.482)]  [G loss: -78.502] \n",
                        "2675 [D loss: (3.483)(R 3.480, F 3.486)]  [G loss: -78.531] \n",
                        "2676 [D loss: (3.487)(R 3.484, F 3.490)]  [G loss: -78.556] \n",
                        "2677 [D loss: (3.491)(R 3.488, F 3.494)]  [G loss: -78.578] \n",
                        "2678 [D loss: (3.494)(R 3.491, F 3.496)]  [G loss: -78.595] \n",
                        "2679 [D loss: (3.495)(R 3.493, F 3.498)]  [G loss: -78.609] \n",
                        "2680 [D loss: (3.496)(R 3.494, F 3.498)]  [G loss: -78.621] \n",
                        "2681 [D loss: (3.496)(R 3.494, F 3.498)]  [G loss: -78.632] \n",
                        "2682 [D loss: (3.495)(R 3.493, F 3.497)]  [G loss: -78.643] \n",
                        "2683 [D loss: (3.494)(R 3.492, F 3.497)]  [G loss: -78.655] \n",
                        "2684 [D loss: (3.493)(R 3.491, F 3.495)]  [G loss: -78.668] \n",
                        "2685 [D loss: (3.491)(R 3.488, F 3.493)]  [G loss: -78.684] \n",
                        "2686 [D loss: (3.488)(R 3.486, F 3.490)]  [G loss: -78.703] \n",
                        "2687 [D loss: (3.484)(R 3.482, F 3.486)]  [G loss: -78.727] \n",
                        "2688 [D loss: (3.480)(R 3.477, F 3.482)]  [G loss: -78.756] \n",
                        "2689 [D loss: (3.476)(R 3.474, F 3.479)]  [G loss: -78.792] \n",
                        "2690 [D loss: (3.475)(R 3.472, F 3.478)]  [G loss: -78.831] \n",
                        "2691 [D loss: (3.476)(R 3.473, F 3.479)]  [G loss: -78.873] \n",
                        "2692 [D loss: (3.478)(R 3.475, F 3.482)]  [G loss: -78.915] \n",
                        "2693 [D loss: (3.481)(R 3.478, F 3.485)]  [G loss: -78.955] \n",
                        "2694 [D loss: (3.484)(R 3.480, F 3.487)]  [G loss: -78.994] \n",
                        "2695 [D loss: (3.485)(R 3.482, F 3.489)]  [G loss: -79.033] \n",
                        "2696 [D loss: (3.486)(R 3.482, F 3.489)]  [G loss: -79.073] \n",
                        "2697 [D loss: (3.485)(R 3.482, F 3.488)]  [G loss: -79.115] \n",
                        "2698 [D loss: (3.483)(R 3.480, F 3.487)]  [G loss: -79.162] \n",
                        "2699 [D loss: (3.481)(R 3.478, F 3.485)]  [G loss: -79.217] \n",
                        "2700 [D loss: (3.482)(R 3.478, F 3.486)]  [G loss: -79.274] \n",
                        "2701 [D loss: (3.484)(R 3.481, F 3.488)]  [G loss: -79.336] \n",
                        "2702 [D loss: (3.489)(R 3.485, F 3.492)]  [G loss: -79.396] \n",
                        "2703 [D loss: (3.495)(R 3.491, F 3.498)]  [G loss: -79.454] \n",
                        "2704 [D loss: (3.502)(R 3.498, F 3.506)]  [G loss: -79.507] \n",
                        "2705 [D loss: (3.511)(R 3.507, F 3.514)]  [G loss: -79.553] \n",
                        "2706 [D loss: (3.519)(R 3.516, F 3.522)]  [G loss: -79.591] \n",
                        "2707 [D loss: (3.527)(R 3.524, F 3.529)]  [G loss: -79.622] \n",
                        "2708 [D loss: (3.533)(R 3.531, F 3.536)]  [G loss: -79.644] \n",
                        "2709 [D loss: (3.538)(R 3.536, F 3.540)]  [G loss: -79.658] \n",
                        "2710 [D loss: (3.541)(R 3.540, F 3.543)]  [G loss: -79.664] \n",
                        "2711 [D loss: (3.543)(R 3.542, F 3.545)]  [G loss: -79.665] \n",
                        "2712 [D loss: (3.544)(R 3.543, F 3.545)]  [G loss: -79.661] \n",
                        "2713 [D loss: (3.544)(R 3.543, F 3.545)]  [G loss: -79.653] \n",
                        "2714 [D loss: (3.543)(R 3.542, F 3.544)]  [G loss: -79.644] \n",
                        "2715 [D loss: (3.543)(R 3.542, F 3.544)]  [G loss: -79.634] \n",
                        "2716 [D loss: (3.544)(R 3.542, F 3.545)]  [G loss: -79.624] \n",
                        "2717 [D loss: (3.544)(R 3.542, F 3.545)]  [G loss: -79.613] \n",
                        "2718 [D loss: (3.542)(R 3.541, F 3.544)]  [G loss: -79.602] \n",
                        "2719 [D loss: (3.540)(R 3.538, F 3.541)]  [G loss: -79.593] \n",
                        "2720 [D loss: (3.536)(R 3.535, F 3.538)]  [G loss: -79.585] \n",
                        "2721 [D loss: (3.532)(R 3.530, F 3.534)]  [G loss: -79.580] \n",
                        "2722 [D loss: (3.528)(R 3.526, F 3.530)]  [G loss: -79.578] \n",
                        "2723 [D loss: (3.523)(R 3.521, F 3.525)]  [G loss: -79.580] \n",
                        "2724 [D loss: (3.519)(R 3.517, F 3.521)]  [G loss: -79.586] \n",
                        "2725 [D loss: (3.516)(R 3.513, F 3.518)]  [G loss: -79.596] \n",
                        "2726 [D loss: (3.514)(R 3.511, F 3.517)]  [G loss: -79.611] \n",
                        "2727 [D loss: (3.512)(R 3.510, F 3.515)]  [G loss: -79.631] \n",
                        "2728 [D loss: (3.510)(R 3.508, F 3.513)]  [G loss: -79.654] \n",
                        "2729 [D loss: (3.509)(R 3.506, F 3.511)]  [G loss: -79.682] \n",
                        "2730 [D loss: (3.507)(R 3.504, F 3.510)]  [G loss: -79.714] \n",
                        "2731 [D loss: (3.506)(R 3.502, F 3.509)]  [G loss: -79.751] \n",
                        "2732 [D loss: (3.505)(R 3.502, F 3.509)]  [G loss: -79.794] \n",
                        "2733 [D loss: (3.506)(R 3.503, F 3.510)]  [G loss: -79.840] \n",
                        "2734 [D loss: (3.509)(R 3.505, F 3.513)]  [G loss: -79.890] \n",
                        "2735 [D loss: (3.513)(R 3.509, F 3.517)]  [G loss: -79.942] \n",
                        "2736 [D loss: (3.518)(R 3.514, F 3.522)]  [G loss: -79.995] \n",
                        "2737 [D loss: (3.525)(R 3.520, F 3.529)]  [G loss: -80.049] \n",
                        "2738 [D loss: (3.532)(R 3.527, F 3.536)]  [G loss: -80.102] \n",
                        "2739 [D loss: (3.539)(R 3.535, F 3.543)]  [G loss: -80.154] \n",
                        "2740 [D loss: (3.546)(R 3.542, F 3.550)]  [G loss: -80.205] \n",
                        "2741 [D loss: (3.554)(R 3.550, F 3.558)]  [G loss: -80.252] \n",
                        "2742 [D loss: (3.561)(R 3.557, F 3.565)]  [G loss: -80.296] \n",
                        "2743 [D loss: (3.567)(R 3.564, F 3.571)]  [G loss: -80.335] \n",
                        "2744 [D loss: (3.574)(R 3.570, F 3.577)]  [G loss: -80.369] \n",
                        "2745 [D loss: (3.579)(R 3.575, F 3.582)]  [G loss: -80.402] \n",
                        "2746 [D loss: (3.583)(R 3.580, F 3.587)]  [G loss: -80.432] \n",
                        "2747 [D loss: (3.587)(R 3.584, F 3.590)]  [G loss: -80.457] \n",
                        "2748 [D loss: (3.591)(R 3.588, F 3.594)]  [G loss: -80.479] \n",
                        "2749 [D loss: (3.594)(R 3.591, F 3.596)]  [G loss: -80.495] \n",
                        "2750 [D loss: (3.595)(R 3.593, F 3.598)]  [G loss: -80.508] \n",
                        "2751 [D loss: (3.597)(R 3.594, F 3.599)]  [G loss: -80.518] \n",
                        "2752 [D loss: (3.597)(R 3.595, F 3.599)]  [G loss: -80.524] \n",
                        "2753 [D loss: (3.597)(R 3.595, F 3.599)]  [G loss: -80.529] \n",
                        "2754 [D loss: (3.597)(R 3.595, F 3.599)]  [G loss: -80.531] \n",
                        "2755 [D loss: (3.596)(R 3.594, F 3.598)]  [G loss: -80.532] \n",
                        "2756 [D loss: (3.595)(R 3.593, F 3.597)]  [G loss: -80.531] \n",
                        "2757 [D loss: (3.594)(R 3.592, F 3.596)]  [G loss: -80.531] \n",
                        "2758 [D loss: (3.593)(R 3.591, F 3.594)]  [G loss: -80.529] \n",
                        "2759 [D loss: (3.592)(R 3.590, F 3.593)]  [G loss: -80.529] \n",
                        "2760 [D loss: (3.591)(R 3.589, F 3.592)]  [G loss: -80.529] \n",
                        "2761 [D loss: (3.590)(R 3.588, F 3.592)]  [G loss: -80.531] \n",
                        "2762 [D loss: (3.590)(R 3.588, F 3.592)]  [G loss: -80.534] \n",
                        "2763 [D loss: (3.589)(R 3.588, F 3.591)]  [G loss: -80.539] \n",
                        "2764 [D loss: (3.589)(R 3.587, F 3.591)]  [G loss: -80.546] \n",
                        "2765 [D loss: (3.589)(R 3.587, F 3.591)]  [G loss: -80.555] \n",
                        "2766 [D loss: (3.589)(R 3.586, F 3.591)]  [G loss: -80.565] \n",
                        "2767 [D loss: (3.588)(R 3.586, F 3.591)]  [G loss: -80.578] \n",
                        "2768 [D loss: (3.589)(R 3.586, F 3.591)]  [G loss: -80.593] \n",
                        "2769 [D loss: (3.589)(R 3.587, F 3.592)]  [G loss: -80.609] \n",
                        "2770 [D loss: (3.590)(R 3.588, F 3.593)]  [G loss: -80.628] \n",
                        "2771 [D loss: (3.592)(R 3.590, F 3.595)]  [G loss: -80.648] \n",
                        "2772 [D loss: (3.595)(R 3.592, F 3.598)]  [G loss: -80.669] \n",
                        "2773 [D loss: (3.598)(R 3.595, F 3.601)]  [G loss: -80.690] \n",
                        "2774 [D loss: (3.602)(R 3.599, F 3.604)]  [G loss: -80.710] \n",
                        "2775 [D loss: (3.605)(R 3.602, F 3.607)]  [G loss: -80.728] \n",
                        "2776 [D loss: (3.607)(R 3.605, F 3.610)]  [G loss: -80.744] \n",
                        "2777 [D loss: (3.609)(R 3.607, F 3.612)]  [G loss: -80.759] \n",
                        "2778 [D loss: (3.612)(R 3.609, F 3.614)]  [G loss: -80.775] \n",
                        "2779 [D loss: (3.613)(R 3.611, F 3.616)]  [G loss: -80.792] \n",
                        "2780 [D loss: (3.615)(R 3.612, F 3.617)]  [G loss: -80.808] \n",
                        "2781 [D loss: (3.616)(R 3.614, F 3.618)]  [G loss: -80.823] \n",
                        "2782 [D loss: (3.617)(R 3.614, F 3.619)]  [G loss: -80.836] \n",
                        "2783 [D loss: (3.617)(R 3.614, F 3.619)]  [G loss: -80.848] \n",
                        "2784 [D loss: (3.617)(R 3.614, F 3.619)]  [G loss: -80.861] \n",
                        "2785 [D loss: (3.616)(R 3.614, F 3.619)]  [G loss: -80.874] \n",
                        "2786 [D loss: (3.616)(R 3.614, F 3.619)]  [G loss: -80.890] \n",
                        "2787 [D loss: (3.617)(R 3.614, F 3.619)]  [G loss: -80.906] \n",
                        "2788 [D loss: (3.618)(R 3.615, F 3.620)]  [G loss: -80.923] \n",
                        "2789 [D loss: (3.619)(R 3.616, F 3.621)]  [G loss: -80.941] \n",
                        "2790 [D loss: (3.620)(R 3.617, F 3.623)]  [G loss: -80.961] \n",
                        "2791 [D loss: (3.622)(R 3.619, F 3.624)]  [G loss: -80.982] \n",
                        "2792 [D loss: (3.624)(R 3.621, F 3.627)]  [G loss: -81.004] \n",
                        "2793 [D loss: (3.626)(R 3.624, F 3.629)]  [G loss: -81.026] \n",
                        "2794 [D loss: (3.629)(R 3.626, F 3.632)]  [G loss: -81.047] \n",
                        "2795 [D loss: (3.631)(R 3.629, F 3.634)]  [G loss: -81.068] \n",
                        "2796 [D loss: (3.633)(R 3.631, F 3.636)]  [G loss: -81.088] \n",
                        "2797 [D loss: (3.635)(R 3.633, F 3.638)]  [G loss: -81.107] \n",
                        "2798 [D loss: (3.637)(R 3.635, F 3.640)]  [G loss: -81.126] \n",
                        "2799 [D loss: (3.639)(R 3.637, F 3.642)]  [G loss: -81.143] \n",
                        "2800 [D loss: (3.641)(R 3.638, F 3.643)]  [G loss: -81.157] \n",
                        "2801 [D loss: (3.641)(R 3.639, F 3.644)]  [G loss: -81.172] \n",
                        "2802 [D loss: (3.642)(R 3.640, F 3.645)]  [G loss: -81.187] \n",
                        "2803 [D loss: (3.643)(R 3.641, F 3.646)]  [G loss: -81.201] \n",
                        "2804 [D loss: (3.644)(R 3.642, F 3.646)]  [G loss: -81.216] \n",
                        "2805 [D loss: (3.645)(R 3.642, F 3.647)]  [G loss: -81.231] \n",
                        "2806 [D loss: (3.646)(R 3.643, F 3.648)]  [G loss: -81.246] \n",
                        "2807 [D loss: (3.646)(R 3.644, F 3.649)]  [G loss: -81.262] \n",
                        "2808 [D loss: (3.647)(R 3.645, F 3.649)]  [G loss: -81.278] \n",
                        "2809 [D loss: (3.648)(R 3.645, F 3.650)]  [G loss: -81.295] \n",
                        "2810 [D loss: (3.649)(R 3.646, F 3.651)]  [G loss: -81.313] \n",
                        "2811 [D loss: (3.650)(R 3.647, F 3.652)]  [G loss: -81.331] \n",
                        "2812 [D loss: (3.651)(R 3.648, F 3.653)]  [G loss: -81.350] \n",
                        "2813 [D loss: (3.652)(R 3.649, F 3.654)]  [G loss: -81.368] \n",
                        "2814 [D loss: (3.653)(R 3.651, F 3.656)]  [G loss: -81.387] \n",
                        "2815 [D loss: (3.654)(R 3.652, F 3.657)]  [G loss: -81.405] \n",
                        "2816 [D loss: (3.655)(R 3.653, F 3.658)]  [G loss: -81.423] \n",
                        "2817 [D loss: (3.656)(R 3.654, F 3.659)]  [G loss: -81.442] \n",
                        "2818 [D loss: (3.657)(R 3.655, F 3.660)]  [G loss: -81.461] \n",
                        "2819 [D loss: (3.658)(R 3.656, F 3.661)]  [G loss: -81.480] \n",
                        "2820 [D loss: (3.659)(R 3.657, F 3.662)]  [G loss: -81.499] \n",
                        "2821 [D loss: (3.661)(R 3.658, F 3.663)]  [G loss: -81.519] \n",
                        "2822 [D loss: (3.662)(R 3.660, F 3.665)]  [G loss: -81.539] \n",
                        "2823 [D loss: (3.663)(R 3.661, F 3.666)]  [G loss: -81.560] \n",
                        "2824 [D loss: (3.665)(R 3.662, F 3.667)]  [G loss: -81.580] \n",
                        "2825 [D loss: (3.666)(R 3.664, F 3.669)]  [G loss: -81.600] \n",
                        "2826 [D loss: (3.667)(R 3.665, F 3.670)]  [G loss: -81.620] \n",
                        "2827 [D loss: (3.669)(R 3.666, F 3.671)]  [G loss: -81.641] \n",
                        "2828 [D loss: (3.670)(R 3.667, F 3.673)]  [G loss: -81.660] \n",
                        "2829 [D loss: (3.671)(R 3.669, F 3.674)]  [G loss: -81.680] \n",
                        "2830 [D loss: (3.672)(R 3.670, F 3.675)]  [G loss: -81.698] \n",
                        "2831 [D loss: (3.673)(R 3.671, F 3.676)]  [G loss: -81.717] \n",
                        "2832 [D loss: (3.675)(R 3.672, F 3.677)]  [G loss: -81.735] \n",
                        "2833 [D loss: (3.676)(R 3.673, F 3.678)]  [G loss: -81.752] \n",
                        "2834 [D loss: (3.677)(R 3.675, F 3.679)]  [G loss: -81.768] \n",
                        "2835 [D loss: (3.678)(R 3.675, F 3.680)]  [G loss: -81.784] \n",
                        "2836 [D loss: (3.679)(R 3.676, F 3.681)]  [G loss: -81.799] \n",
                        "2837 [D loss: (3.680)(R 3.677, F 3.682)]  [G loss: -81.815] \n",
                        "2838 [D loss: (3.680)(R 3.678, F 3.683)]  [G loss: -81.831] \n",
                        "2839 [D loss: (3.681)(R 3.679, F 3.684)]  [G loss: -81.847] \n",
                        "2840 [D loss: (3.682)(R 3.679, F 3.684)]  [G loss: -81.863] \n",
                        "2841 [D loss: (3.683)(R 3.680, F 3.685)]  [G loss: -81.879] \n",
                        "2842 [D loss: (3.684)(R 3.681, F 3.686)]  [G loss: -81.896] \n",
                        "2843 [D loss: (3.684)(R 3.682, F 3.687)]  [G loss: -81.912] \n",
                        "2844 [D loss: (3.685)(R 3.683, F 3.688)]  [G loss: -81.929] \n",
                        "2845 [D loss: (3.686)(R 3.684, F 3.689)]  [G loss: -81.946] \n",
                        "2846 [D loss: (3.687)(R 3.684, F 3.689)]  [G loss: -81.964] \n",
                        "2847 [D loss: (3.688)(R 3.686, F 3.691)]  [G loss: -81.979] \n",
                        "2848 [D loss: (3.689)(R 3.687, F 3.691)]  [G loss: -81.994] \n",
                        "2849 [D loss: (3.690)(R 3.687, F 3.692)]  [G loss: -82.012] \n",
                        "2850 [D loss: (3.691)(R 3.688, F 3.693)]  [G loss: -82.028] \n",
                        "2851 [D loss: (3.692)(R 3.689, F 3.694)]  [G loss: -82.045] \n",
                        "2852 [D loss: (3.693)(R 3.690, F 3.695)]  [G loss: -82.062] \n",
                        "2853 [D loss: (3.694)(R 3.691, F 3.696)]  [G loss: -82.079] \n",
                        "2854 [D loss: (3.695)(R 3.692, F 3.697)]  [G loss: -82.096] \n",
                        "2855 [D loss: (3.695)(R 3.693, F 3.698)]  [G loss: -82.114] \n",
                        "2856 [D loss: (3.696)(R 3.694, F 3.699)]  [G loss: -82.132] \n",
                        "2857 [D loss: (3.697)(R 3.695, F 3.700)]  [G loss: -82.149] \n",
                        "2858 [D loss: (3.698)(R 3.696, F 3.701)]  [G loss: -82.168] \n",
                        "2859 [D loss: (3.699)(R 3.697, F 3.702)]  [G loss: -82.186] \n",
                        "2860 [D loss: (3.700)(R 3.698, F 3.703)]  [G loss: -82.204] \n",
                        "2861 [D loss: (3.701)(R 3.699, F 3.704)]  [G loss: -82.223] \n",
                        "2862 [D loss: (3.702)(R 3.700, F 3.705)]  [G loss: -82.243] \n",
                        "2863 [D loss: (3.704)(R 3.701, F 3.706)]  [G loss: -82.262] \n",
                        "2864 [D loss: (3.705)(R 3.702, F 3.707)]  [G loss: -82.281] \n",
                        "2865 [D loss: (3.706)(R 3.703, F 3.709)]  [G loss: -82.301] \n",
                        "2866 [D loss: (3.707)(R 3.705, F 3.710)]  [G loss: -82.320] \n",
                        "2867 [D loss: (3.708)(R 3.706, F 3.711)]  [G loss: -82.341] \n",
                        "2868 [D loss: (3.710)(R 3.707, F 3.712)]  [G loss: -82.361] \n",
                        "2869 [D loss: (3.711)(R 3.708, F 3.714)]  [G loss: -82.381] \n",
                        "2870 [D loss: (3.712)(R 3.710, F 3.715)]  [G loss: -82.402] \n",
                        "2871 [D loss: (3.714)(R 3.711, F 3.716)]  [G loss: -82.423] \n",
                        "2872 [D loss: (3.715)(R 3.712, F 3.717)]  [G loss: -82.445] \n",
                        "2873 [D loss: (3.716)(R 3.714, F 3.719)]  [G loss: -82.465] \n",
                        "2874 [D loss: (3.718)(R 3.715, F 3.721)]  [G loss: -82.486] \n",
                        "2875 [D loss: (3.719)(R 3.717, F 3.722)]  [G loss: -82.506] \n",
                        "2876 [D loss: (3.721)(R 3.718, F 3.723)]  [G loss: -82.527] \n",
                        "2877 [D loss: (3.722)(R 3.720, F 3.725)]  [G loss: -82.547] \n",
                        "2878 [D loss: (3.724)(R 3.721, F 3.726)]  [G loss: -82.567] \n",
                        "2879 [D loss: (3.725)(R 3.722, F 3.727)]  [G loss: -82.587] \n",
                        "2880 [D loss: (3.726)(R 3.724, F 3.729)]  [G loss: -82.607] \n",
                        "2881 [D loss: (3.728)(R 3.725, F 3.730)]  [G loss: -82.627] \n",
                        "2882 [D loss: (3.729)(R 3.726, F 3.732)]  [G loss: -82.647] \n",
                        "2883 [D loss: (3.730)(R 3.728, F 3.733)]  [G loss: -82.666] \n",
                        "2884 [D loss: (3.732)(R 3.729, F 3.734)]  [G loss: -82.686] \n",
                        "2885 [D loss: (3.733)(R 3.730, F 3.735)]  [G loss: -82.705] \n",
                        "2886 [D loss: (3.734)(R 3.732, F 3.737)]  [G loss: -82.725] \n",
                        "2887 [D loss: (3.735)(R 3.733, F 3.738)]  [G loss: -82.744] \n",
                        "2888 [D loss: (3.737)(R 3.734, F 3.739)]  [G loss: -82.765] \n",
                        "2889 [D loss: (3.738)(R 3.736, F 3.741)]  [G loss: -82.785] \n",
                        "2890 [D loss: (3.740)(R 3.737, F 3.742)]  [G loss: -82.804] \n",
                        "2891 [D loss: (3.741)(R 3.738, F 3.743)]  [G loss: -82.823] \n",
                        "2892 [D loss: (3.742)(R 3.739, F 3.744)]  [G loss: -82.842] \n",
                        "2893 [D loss: (3.743)(R 3.741, F 3.746)]  [G loss: -82.860] \n",
                        "2894 [D loss: (3.745)(R 3.742, F 3.747)]  [G loss: -82.878] \n",
                        "2895 [D loss: (3.746)(R 3.743, F 3.748)]  [G loss: -82.895] \n",
                        "2896 [D loss: (3.747)(R 3.744, F 3.749)]  [G loss: -82.913] \n",
                        "2897 [D loss: (3.748)(R 3.745, F 3.750)]  [G loss: -82.930] \n",
                        "2898 [D loss: (3.749)(R 3.746, F 3.751)]  [G loss: -82.947] \n",
                        "2899 [D loss: (3.750)(R 3.747, F 3.752)]  [G loss: -82.964] \n",
                        "2900 [D loss: (3.750)(R 3.748, F 3.753)]  [G loss: -82.982] \n",
                        "2901 [D loss: (3.751)(R 3.749, F 3.754)]  [G loss: -83.001] \n",
                        "2902 [D loss: (3.752)(R 3.749, F 3.754)]  [G loss: -83.022] \n",
                        "2903 [D loss: (3.753)(R 3.750, F 3.755)]  [G loss: -83.044] \n",
                        "2904 [D loss: (3.754)(R 3.751, F 3.757)]  [G loss: -83.065] \n",
                        "2905 [D loss: (3.755)(R 3.753, F 3.758)]  [G loss: -83.085] \n",
                        "2906 [D loss: (3.756)(R 3.754, F 3.759)]  [G loss: -83.103] \n",
                        "2907 [D loss: (3.757)(R 3.754, F 3.759)]  [G loss: -83.121] \n",
                        "2908 [D loss: (3.757)(R 3.755, F 3.760)]  [G loss: -83.139] \n",
                        "2909 [D loss: (3.758)(R 3.756, F 3.761)]  [G loss: -83.157] \n",
                        "2910 [D loss: (3.760)(R 3.757, F 3.762)]  [G loss: -83.174] \n",
                        "2911 [D loss: (3.761)(R 3.758, F 3.763)]  [G loss: -83.191] \n",
                        "2912 [D loss: (3.762)(R 3.760, F 3.764)]  [G loss: -83.206] \n",
                        "2913 [D loss: (3.763)(R 3.761, F 3.765)]  [G loss: -83.220] \n",
                        "2914 [D loss: (3.764)(R 3.762, F 3.766)]  [G loss: -83.233] \n",
                        "2915 [D loss: (3.764)(R 3.762, F 3.766)]  [G loss: -83.246] \n",
                        "2916 [D loss: (3.764)(R 3.762, F 3.767)]  [G loss: -83.260] \n",
                        "2917 [D loss: (3.764)(R 3.762, F 3.766)]  [G loss: -83.274] \n",
                        "2918 [D loss: (3.764)(R 3.761, F 3.766)]  [G loss: -83.291] \n",
                        "2919 [D loss: (3.763)(R 3.760, F 3.765)]  [G loss: -83.312] \n",
                        "2920 [D loss: (3.762)(R 3.760, F 3.764)]  [G loss: -83.336] \n",
                        "2921 [D loss: (3.762)(R 3.760, F 3.764)]  [G loss: -83.363] \n",
                        "2922 [D loss: (3.763)(R 3.761, F 3.766)]  [G loss: -83.392] \n",
                        "2923 [D loss: (3.766)(R 3.763, F 3.769)]  [G loss: -83.418] \n",
                        "2924 [D loss: (3.768)(R 3.765, F 3.771)]  [G loss: -83.441] \n",
                        "2925 [D loss: (3.769)(R 3.766, F 3.772)]  [G loss: -83.462] \n",
                        "2926 [D loss: (3.769)(R 3.766, F 3.772)]  [G loss: -83.481] \n",
                        "2927 [D loss: (3.768)(R 3.766, F 3.771)]  [G loss: -83.501] \n",
                        "2928 [D loss: (3.768)(R 3.765, F 3.770)]  [G loss: -83.525] \n",
                        "2929 [D loss: (3.767)(R 3.765, F 3.770)]  [G loss: -83.551] \n",
                        "2930 [D loss: (3.767)(R 3.765, F 3.770)]  [G loss: -83.580] \n",
                        "2931 [D loss: (3.767)(R 3.765, F 3.770)]  [G loss: -83.611] \n",
                        "2932 [D loss: (3.767)(R 3.764, F 3.770)]  [G loss: -83.645] \n",
                        "2933 [D loss: (3.769)(R 3.766, F 3.772)]  [G loss: -83.676] \n",
                        "2934 [D loss: (3.772)(R 3.769, F 3.774)]  [G loss: -83.705] \n",
                        "2935 [D loss: (3.775)(R 3.772, F 3.777)]  [G loss: -83.731] \n",
                        "2936 [D loss: (3.778)(R 3.775, F 3.780)]  [G loss: -83.754] \n",
                        "2937 [D loss: (3.781)(R 3.778, F 3.783)]  [G loss: -83.772] \n",
                        "2938 [D loss: (3.784)(R 3.782, F 3.786)]  [G loss: -83.784] \n",
                        "2939 [D loss: (3.786)(R 3.784, F 3.788)]  [G loss: -83.792] \n",
                        "2940 [D loss: (3.786)(R 3.784, F 3.788)]  [G loss: -83.795] \n",
                        "2941 [D loss: (3.785)(R 3.783, F 3.786)]  [G loss: -83.794] \n",
                        "2942 [D loss: (3.782)(R 3.781, F 3.783)]  [G loss: -83.791] \n",
                        "2943 [D loss: (3.778)(R 3.777, F 3.779)]  [G loss: -83.788] \n",
                        "2944 [D loss: (3.773)(R 3.772, F 3.775)]  [G loss: -83.786] \n",
                        "2945 [D loss: (3.767)(R 3.766, F 3.769)]  [G loss: -83.784] \n",
                        "2946 [D loss: (3.761)(R 3.760, F 3.762)]  [G loss: -83.783] \n",
                        "2947 [D loss: (3.754)(R 3.752, F 3.755)]  [G loss: -83.786] \n",
                        "2948 [D loss: (3.746)(R 3.744, F 3.747)]  [G loss: -83.793] \n",
                        "2949 [D loss: (3.737)(R 3.735, F 3.738)]  [G loss: -83.807] \n",
                        "2950 [D loss: (3.728)(R 3.726, F 3.729)]  [G loss: -83.828] \n",
                        "2951 [D loss: (3.720)(R 3.718, F 3.722)]  [G loss: -83.858] \n",
                        "2952 [D loss: (3.713)(R 3.710, F 3.715)]  [G loss: -83.898] \n",
                        "2953 [D loss: (3.709)(R 3.706, F 3.711)]  [G loss: -83.946] \n",
                        "2954 [D loss: (3.710)(R 3.707, F 3.713)]  [G loss: -83.997] \n",
                        "2955 [D loss: (3.714)(R 3.711, F 3.718)]  [G loss: -84.049] \n",
                        "2956 [D loss: (3.722)(R 3.718, F 3.725)]  [G loss: -84.096] \n",
                        "2957 [D loss: (3.728)(R 3.725, F 3.732)]  [G loss: -84.140] \n",
                        "2958 [D loss: (3.733)(R 3.730, F 3.737)]  [G loss: -84.179] \n",
                        "2959 [D loss: (3.735)(R 3.731, F 3.738)]  [G loss: -84.214] \n",
                        "2960 [D loss: (3.733)(R 3.730, F 3.737)]  [G loss: -84.246] \n",
                        "2961 [D loss: (3.730)(R 3.727, F 3.733)]  [G loss: -84.279] \n",
                        "2962 [D loss: (3.726)(R 3.722, F 3.729)]  [G loss: -84.316] \n",
                        "2963 [D loss: (3.722)(R 3.719, F 3.725)]  [G loss: -84.360] \n",
                        "2964 [D loss: (3.719)(R 3.715, F 3.722)]  [G loss: -84.410] \n",
                        "2965 [D loss: (3.717)(R 3.714, F 3.721)]  [G loss: -84.466] \n",
                        "2966 [D loss: (3.717)(R 3.714, F 3.720)]  [G loss: -84.529] \n",
                        "2967 [D loss: (3.719)(R 3.715, F 3.722)]  [G loss: -84.590] \n",
                        "2968 [D loss: (3.722)(R 3.719, F 3.726)]  [G loss: -84.652] \n",
                        "2969 [D loss: (3.727)(R 3.723, F 3.730)]  [G loss: -84.709] \n",
                        "2970 [D loss: (3.733)(R 3.730, F 3.736)]  [G loss: -84.763] \n",
                        "2971 [D loss: (3.739)(R 3.736, F 3.743)]  [G loss: -84.812] \n",
                        "2972 [D loss: (3.746)(R 3.744, F 3.749)]  [G loss: -84.854] \n",
                        "2973 [D loss: (3.753)(R 3.750, F 3.756)]  [G loss: -84.888] \n",
                        "2974 [D loss: (3.759)(R 3.757, F 3.762)]  [G loss: -84.915] \n",
                        "2975 [D loss: (3.765)(R 3.763, F 3.768)]  [G loss: -84.935] \n",
                        "2976 [D loss: (3.770)(R 3.768, F 3.772)]  [G loss: -84.946] \n",
                        "2977 [D loss: (3.774)(R 3.772, F 3.775)]  [G loss: -84.952] \n",
                        "2978 [D loss: (3.777)(R 3.775, F 3.778)]  [G loss: -84.951] \n",
                        "2979 [D loss: (3.779)(R 3.778, F 3.780)]  [G loss: -84.943] \n",
                        "2980 [D loss: (3.780)(R 3.779, F 3.781)]  [G loss: -84.932] \n",
                        "2981 [D loss: (3.780)(R 3.780, F 3.781)]  [G loss: -84.919] \n",
                        "2982 [D loss: (3.780)(R 3.779, F 3.781)]  [G loss: -84.904] \n",
                        "2983 [D loss: (3.779)(R 3.779, F 3.780)]  [G loss: -84.886] \n",
                        "2984 [D loss: (3.777)(R 3.776, F 3.778)]  [G loss: -84.868] \n",
                        "2985 [D loss: (3.773)(R 3.773, F 3.774)]  [G loss: -84.850] \n",
                        "2986 [D loss: (3.768)(R 3.768, F 3.769)]  [G loss: -84.831] \n",
                        "2987 [D loss: (3.762)(R 3.762, F 3.763)]  [G loss: -84.815] \n",
                        "2988 [D loss: (3.756)(R 3.755, F 3.757)]  [G loss: -84.801] \n",
                        "2989 [D loss: (3.750)(R 3.749, F 3.751)]  [G loss: -84.791] \n",
                        "2990 [D loss: (3.744)(R 3.743, F 3.745)]  [G loss: -84.783] \n",
                        "2991 [D loss: (3.738)(R 3.737, F 3.740)]  [G loss: -84.779] \n",
                        "2992 [D loss: (3.733)(R 3.732, F 3.735)]  [G loss: -84.778] \n",
                        "2993 [D loss: (3.728)(R 3.727, F 3.730)]  [G loss: -84.779] \n",
                        "2994 [D loss: (3.723)(R 3.721, F 3.725)]  [G loss: -84.784] \n",
                        "2995 [D loss: (3.718)(R 3.716, F 3.720)]  [G loss: -84.791] \n",
                        "2996 [D loss: (3.713)(R 3.711, F 3.715)]  [G loss: -84.800] \n",
                        "2997 [D loss: (3.710)(R 3.708, F 3.712)]  [G loss: -84.812] \n",
                        "2998 [D loss: (3.707)(R 3.704, F 3.709)]  [G loss: -84.827] \n",
                        "2999 [D loss: (3.704)(R 3.702, F 3.706)]  [G loss: -84.845] \n",
                        "3000 [D loss: (3.702)(R 3.700, F 3.704)]  [G loss: -84.865] \n",
                        "3001 [D loss: (3.701)(R 3.698, F 3.703)]  [G loss: -84.888] \n",
                        "3002 [D loss: (3.701)(R 3.698, F 3.703)]  [G loss: -84.913] \n",
                        "3003 [D loss: (3.701)(R 3.698, F 3.703)]  [G loss: -84.939] \n",
                        "3004 [D loss: (3.701)(R 3.699, F 3.704)]  [G loss: -84.966] \n",
                        "3005 [D loss: (3.702)(R 3.700, F 3.705)]  [G loss: -84.994] \n",
                        "3006 [D loss: (3.704)(R 3.701, F 3.707)]  [G loss: -85.021] \n",
                        "3007 [D loss: (3.706)(R 3.703, F 3.708)]  [G loss: -85.049] \n",
                        "3008 [D loss: (3.707)(R 3.705, F 3.710)]  [G loss: -85.076] \n",
                        "3009 [D loss: (3.709)(R 3.706, F 3.712)]  [G loss: -85.103] \n",
                        "3010 [D loss: (3.711)(R 3.708, F 3.714)]  [G loss: -85.130] \n",
                        "3011 [D loss: (3.714)(R 3.711, F 3.717)]  [G loss: -85.157] \n",
                        "3012 [D loss: (3.717)(R 3.714, F 3.720)]  [G loss: -85.185] \n",
                        "3013 [D loss: (3.720)(R 3.717, F 3.723)]  [G loss: -85.213] \n",
                        "3014 [D loss: (3.724)(R 3.721, F 3.727)]  [G loss: -85.240] \n",
                        "3015 [D loss: (3.728)(R 3.725, F 3.731)]  [G loss: -85.267] \n",
                        "3016 [D loss: (3.732)(R 3.729, F 3.735)]  [G loss: -85.293] \n",
                        "3017 [D loss: (3.736)(R 3.733, F 3.739)]  [G loss: -85.318] \n",
                        "3018 [D loss: (3.740)(R 3.737, F 3.743)]  [G loss: -85.342] \n",
                        "3019 [D loss: (3.744)(R 3.741, F 3.747)]  [G loss: -85.363] \n",
                        "3020 [D loss: (3.748)(R 3.745, F 3.750)]  [G loss: -85.383] \n",
                        "3021 [D loss: (3.751)(R 3.749, F 3.754)]  [G loss: -85.400] \n",
                        "3022 [D loss: (3.754)(R 3.751, F 3.756)]  [G loss: -85.415] \n",
                        "3023 [D loss: (3.756)(R 3.754, F 3.759)]  [G loss: -85.429] \n",
                        "3024 [D loss: (3.758)(R 3.756, F 3.761)]  [G loss: -85.442] \n",
                        "3025 [D loss: (3.760)(R 3.758, F 3.762)]  [G loss: -85.455] \n",
                        "3026 [D loss: (3.762)(R 3.759, F 3.764)]  [G loss: -85.467] \n",
                        "3027 [D loss: (3.763)(R 3.761, F 3.765)]  [G loss: -85.479] \n",
                        "3028 [D loss: (3.764)(R 3.762, F 3.766)]  [G loss: -85.491] \n",
                        "3029 [D loss: (3.765)(R 3.763, F 3.767)]  [G loss: -85.502] \n",
                        "3030 [D loss: (3.766)(R 3.764, F 3.768)]  [G loss: -85.512] \n",
                        "3031 [D loss: (3.766)(R 3.764, F 3.768)]  [G loss: -85.520] \n",
                        "3032 [D loss: (3.766)(R 3.764, F 3.768)]  [G loss: -85.526] \n",
                        "3033 [D loss: (3.766)(R 3.764, F 3.768)]  [G loss: -85.533] \n",
                        "3034 [D loss: (3.766)(R 3.764, F 3.768)]  [G loss: -85.540] \n",
                        "3035 [D loss: (3.766)(R 3.764, F 3.768)]  [G loss: -85.548] \n",
                        "3036 [D loss: (3.766)(R 3.764, F 3.768)]  [G loss: -85.556] \n",
                        "3037 [D loss: (3.767)(R 3.765, F 3.769)]  [G loss: -85.565] \n",
                        "3038 [D loss: (3.767)(R 3.765, F 3.769)]  [G loss: -85.576] \n",
                        "3039 [D loss: (3.767)(R 3.765, F 3.770)]  [G loss: -85.588] \n",
                        "3040 [D loss: (3.768)(R 3.766, F 3.771)]  [G loss: -85.599] \n",
                        "3041 [D loss: (3.770)(R 3.767, F 3.772)]  [G loss: -85.610] \n",
                        "3042 [D loss: (3.771)(R 3.768, F 3.773)]  [G loss: -85.619] \n",
                        "3043 [D loss: (3.771)(R 3.769, F 3.773)]  [G loss: -85.630] \n",
                        "3044 [D loss: (3.772)(R 3.770, F 3.774)]  [G loss: -85.643] \n",
                        "3045 [D loss: (3.773)(R 3.771, F 3.775)]  [G loss: -85.656] \n",
                        "3046 [D loss: (3.775)(R 3.773, F 3.777)]  [G loss: -85.669] \n",
                        "3047 [D loss: (3.776)(R 3.774, F 3.778)]  [G loss: -85.680] \n",
                        "3048 [D loss: (3.777)(R 3.775, F 3.779)]  [G loss: -85.691] \n",
                        "3049 [D loss: (3.778)(R 3.776, F 3.780)]  [G loss: -85.703] \n",
                        "3050 [D loss: (3.779)(R 3.777, F 3.782)]  [G loss: -85.714] \n",
                        "3051 [D loss: (3.780)(R 3.778, F 3.782)]  [G loss: -85.726] \n",
                        "3052 [D loss: (3.781)(R 3.779, F 3.783)]  [G loss: -85.738] \n",
                        "3053 [D loss: (3.782)(R 3.780, F 3.784)]  [G loss: -85.751] \n",
                        "3054 [D loss: (3.783)(R 3.781, F 3.785)]  [G loss: -85.763] \n",
                        "3055 [D loss: (3.784)(R 3.782, F 3.786)]  [G loss: -85.776] \n",
                        "3056 [D loss: (3.784)(R 3.782, F 3.787)]  [G loss: -85.790] \n",
                        "3057 [D loss: (3.785)(R 3.783, F 3.787)]  [G loss: -85.804] \n",
                        "3058 [D loss: (3.786)(R 3.784, F 3.788)]  [G loss: -85.818] \n",
                        "3059 [D loss: (3.787)(R 3.784, F 3.789)]  [G loss: -85.834] \n",
                        "3060 [D loss: (3.787)(R 3.785, F 3.790)]  [G loss: -85.849] \n",
                        "3061 [D loss: (3.788)(R 3.786, F 3.791)]  [G loss: -85.865] \n",
                        "3062 [D loss: (3.789)(R 3.787, F 3.792)]  [G loss: -85.882] \n",
                        "3063 [D loss: (3.791)(R 3.788, F 3.793)]  [G loss: -85.899] \n",
                        "3064 [D loss: (3.792)(R 3.790, F 3.794)]  [G loss: -85.915] \n",
                        "3065 [D loss: (3.794)(R 3.791, F 3.796)]  [G loss: -85.932] \n",
                        "3066 [D loss: (3.795)(R 3.793, F 3.797)]  [G loss: -85.947] \n",
                        "3067 [D loss: (3.796)(R 3.794, F 3.799)]  [G loss: -85.962] \n",
                        "3068 [D loss: (3.798)(R 3.795, F 3.800)]  [G loss: -85.977] \n",
                        "3069 [D loss: (3.799)(R 3.797, F 3.801)]  [G loss: -85.992] \n",
                        "3070 [D loss: (3.800)(R 3.798, F 3.803)]  [G loss: -86.006] \n",
                        "3071 [D loss: (3.801)(R 3.799, F 3.804)]  [G loss: -86.022] \n",
                        "3072 [D loss: (3.803)(R 3.800, F 3.805)]  [G loss: -86.038] \n",
                        "3073 [D loss: (3.804)(R 3.801, F 3.806)]  [G loss: -86.054] \n",
                        "3074 [D loss: (3.805)(R 3.803, F 3.807)]  [G loss: -86.070] \n",
                        "3075 [D loss: (3.806)(R 3.803, F 3.808)]  [G loss: -86.088] \n",
                        "3076 [D loss: (3.807)(R 3.805, F 3.809)]  [G loss: -86.106] \n",
                        "3077 [D loss: (3.808)(R 3.806, F 3.811)]  [G loss: -86.124] \n",
                        "3078 [D loss: (3.810)(R 3.807, F 3.812)]  [G loss: -86.140] \n",
                        "3079 [D loss: (3.811)(R 3.809, F 3.813)]  [G loss: -86.157] \n",
                        "3080 [D loss: (3.812)(R 3.810, F 3.814)]  [G loss: -86.173] \n",
                        "3081 [D loss: (3.813)(R 3.811, F 3.816)]  [G loss: -86.189] \n",
                        "3082 [D loss: (3.814)(R 3.812, F 3.817)]  [G loss: -86.206] \n",
                        "3083 [D loss: (3.815)(R 3.813, F 3.818)]  [G loss: -86.221] \n",
                        "3084 [D loss: (3.816)(R 3.814, F 3.819)]  [G loss: -86.236] \n",
                        "3085 [D loss: (3.817)(R 3.815, F 3.819)]  [G loss: -86.251] \n",
                        "3086 [D loss: (3.818)(R 3.816, F 3.820)]  [G loss: -86.266] \n",
                        "3087 [D loss: (3.819)(R 3.817, F 3.821)]  [G loss: -86.280] \n",
                        "3088 [D loss: (3.820)(R 3.817, F 3.822)]  [G loss: -86.294] \n",
                        "3089 [D loss: (3.821)(R 3.818, F 3.823)]  [G loss: -86.308] \n",
                        "3090 [D loss: (3.821)(R 3.819, F 3.824)]  [G loss: -86.322] \n",
                        "3091 [D loss: (3.822)(R 3.820, F 3.824)]  [G loss: -86.336] \n",
                        "3092 [D loss: (3.823)(R 3.821, F 3.826)]  [G loss: -86.351] \n",
                        "3093 [D loss: (3.824)(R 3.822, F 3.827)]  [G loss: -86.365] \n",
                        "3094 [D loss: (3.825)(R 3.823, F 3.828)]  [G loss: -86.380] \n",
                        "3095 [D loss: (3.826)(R 3.824, F 3.829)]  [G loss: -86.395] \n",
                        "3096 [D loss: (3.827)(R 3.825, F 3.830)]  [G loss: -86.410] \n",
                        "3097 [D loss: (3.828)(R 3.826, F 3.831)]  [G loss: -86.425] \n",
                        "3098 [D loss: (3.829)(R 3.827, F 3.832)]  [G loss: -86.440] \n",
                        "3099 [D loss: (3.830)(R 3.828, F 3.833)]  [G loss: -86.456] \n",
                        "3100 [D loss: (3.831)(R 3.829, F 3.834)]  [G loss: -86.472] \n",
                        "3101 [D loss: (3.832)(R 3.830, F 3.835)]  [G loss: -86.490] \n",
                        "3102 [D loss: (3.833)(R 3.831, F 3.835)]  [G loss: -86.508] \n",
                        "3103 [D loss: (3.835)(R 3.832, F 3.837)]  [G loss: -86.526] \n",
                        "3104 [D loss: (3.836)(R 3.834, F 3.839)]  [G loss: -86.543] \n",
                        "3105 [D loss: (3.838)(R 3.835, F 3.840)]  [G loss: -86.560] \n",
                        "3106 [D loss: (3.839)(R 3.837, F 3.841)]  [G loss: -86.577] \n",
                        "3107 [D loss: (3.840)(R 3.838, F 3.843)]  [G loss: -86.594] \n",
                        "3108 [D loss: (3.842)(R 3.839, F 3.844)]  [G loss: -86.609] \n",
                        "3109 [D loss: (3.843)(R 3.840, F 3.845)]  [G loss: -86.625] \n",
                        "3110 [D loss: (3.844)(R 3.842, F 3.846)]  [G loss: -86.641] \n",
                        "3111 [D loss: (3.845)(R 3.843, F 3.847)]  [G loss: -86.657] \n",
                        "3112 [D loss: (3.846)(R 3.844, F 3.848)]  [G loss: -86.675] \n",
                        "3113 [D loss: (3.847)(R 3.845, F 3.850)]  [G loss: -86.692] \n",
                        "3114 [D loss: (3.848)(R 3.846, F 3.851)]  [G loss: -86.710] \n",
                        "3115 [D loss: (3.850)(R 3.847, F 3.852)]  [G loss: -86.728] \n",
                        "3116 [D loss: (3.851)(R 3.849, F 3.853)]  [G loss: -86.745] \n",
                        "3117 [D loss: (3.852)(R 3.850, F 3.855)]  [G loss: -86.761] \n",
                        "3118 [D loss: (3.853)(R 3.851, F 3.856)]  [G loss: -86.777] \n",
                        "3119 [D loss: (3.854)(R 3.852, F 3.856)]  [G loss: -86.794] \n",
                        "3120 [D loss: (3.855)(R 3.853, F 3.857)]  [G loss: -86.810] \n",
                        "3121 [D loss: (3.856)(R 3.854, F 3.858)]  [G loss: -86.826] \n",
                        "3122 [D loss: (3.857)(R 3.855, F 3.859)]  [G loss: -86.843] \n",
                        "3123 [D loss: (3.858)(R 3.856, F 3.860)]  [G loss: -86.860] \n",
                        "3124 [D loss: (3.859)(R 3.857, F 3.862)]  [G loss: -86.877] \n",
                        "3125 [D loss: (3.861)(R 3.858, F 3.863)]  [G loss: -86.895] \n",
                        "3126 [D loss: (3.862)(R 3.859, F 3.864)]  [G loss: -86.912] \n",
                        "3127 [D loss: (3.863)(R 3.861, F 3.865)]  [G loss: -86.929] \n",
                        "3128 [D loss: (3.864)(R 3.862, F 3.867)]  [G loss: -86.945] \n",
                        "3129 [D loss: (3.865)(R 3.863, F 3.868)]  [G loss: -86.962] \n",
                        "3130 [D loss: (3.867)(R 3.864, F 3.869)]  [G loss: -86.978] \n",
                        "3131 [D loss: (3.868)(R 3.866, F 3.870)]  [G loss: -86.993] \n",
                        "3132 [D loss: (3.869)(R 3.866, F 3.871)]  [G loss: -87.009] \n",
                        "3133 [D loss: (3.870)(R 3.867, F 3.872)]  [G loss: -87.025] \n",
                        "3134 [D loss: (3.870)(R 3.868, F 3.873)]  [G loss: -87.041] \n",
                        "3135 [D loss: (3.871)(R 3.869, F 3.874)]  [G loss: -87.058] \n",
                        "3136 [D loss: (3.872)(R 3.870, F 3.875)]  [G loss: -87.075] \n",
                        "3137 [D loss: (3.873)(R 3.871, F 3.876)]  [G loss: -87.092] \n",
                        "3138 [D loss: (3.874)(R 3.872, F 3.877)]  [G loss: -87.111] \n",
                        "3139 [D loss: (3.876)(R 3.873, F 3.878)]  [G loss: -87.128] \n",
                        "3140 [D loss: (3.877)(R 3.874, F 3.879)]  [G loss: -87.145] \n",
                        "3141 [D loss: (3.878)(R 3.875, F 3.880)]  [G loss: -87.161] \n",
                        "3142 [D loss: (3.879)(R 3.877, F 3.881)]  [G loss: -87.178] \n",
                        "3143 [D loss: (3.880)(R 3.878, F 3.882)]  [G loss: -87.194] \n",
                        "3144 [D loss: (3.881)(R 3.879, F 3.884)]  [G loss: -87.210] \n",
                        "3145 [D loss: (3.882)(R 3.880, F 3.884)]  [G loss: -87.227] \n",
                        "3146 [D loss: (3.883)(R 3.881, F 3.885)]  [G loss: -87.242] \n",
                        "3147 [D loss: (3.884)(R 3.882, F 3.886)]  [G loss: -87.258] \n",
                        "3148 [D loss: (3.885)(R 3.883, F 3.887)]  [G loss: -87.274] \n",
                        "3149 [D loss: (3.886)(R 3.884, F 3.888)]  [G loss: -87.290] \n",
                        "3150 [D loss: (3.887)(R 3.885, F 3.889)]  [G loss: -87.306] \n",
                        "3151 [D loss: (3.888)(R 3.886, F 3.890)]  [G loss: -87.323] \n",
                        "3152 [D loss: (3.889)(R 3.887, F 3.891)]  [G loss: -87.339] \n",
                        "3153 [D loss: (3.890)(R 3.888, F 3.892)]  [G loss: -87.355] \n",
                        "3154 [D loss: (3.891)(R 3.889, F 3.893)]  [G loss: -87.371] \n",
                        "3155 [D loss: (3.892)(R 3.890, F 3.894)]  [G loss: -87.386] \n",
                        "3156 [D loss: (3.893)(R 3.891, F 3.895)]  [G loss: -87.402] \n",
                        "3157 [D loss: (3.894)(R 3.892, F 3.896)]  [G loss: -87.417] \n",
                        "3158 [D loss: (3.895)(R 3.892, F 3.897)]  [G loss: -87.433] \n",
                        "3159 [D loss: (3.896)(R 3.893, F 3.898)]  [G loss: -87.450] \n",
                        "3160 [D loss: (3.897)(R 3.894, F 3.899)]  [G loss: -87.467] \n",
                        "3161 [D loss: (3.898)(R 3.895, F 3.900)]  [G loss: -87.485] \n",
                        "3162 [D loss: (3.899)(R 3.896, F 3.901)]  [G loss: -87.504] \n",
                        "3163 [D loss: (3.900)(R 3.897, F 3.902)]  [G loss: -87.522] \n",
                        "3164 [D loss: (3.901)(R 3.899, F 3.904)]  [G loss: -87.541] \n",
                        "3165 [D loss: (3.902)(R 3.900, F 3.905)]  [G loss: -87.559] \n",
                        "3166 [D loss: (3.904)(R 3.901, F 3.906)]  [G loss: -87.577] \n",
                        "3167 [D loss: (3.905)(R 3.902, F 3.907)]  [G loss: -87.594] \n",
                        "3168 [D loss: (3.906)(R 3.904, F 3.908)]  [G loss: -87.612] \n",
                        "3169 [D loss: (3.907)(R 3.905, F 3.910)]  [G loss: -87.629] \n",
                        "3170 [D loss: (3.909)(R 3.906, F 3.911)]  [G loss: -87.646] \n",
                        "3171 [D loss: (3.910)(R 3.908, F 3.912)]  [G loss: -87.662] \n",
                        "3172 [D loss: (3.911)(R 3.908, F 3.913)]  [G loss: -87.678] \n",
                        "3173 [D loss: (3.912)(R 3.909, F 3.914)]  [G loss: -87.694] \n",
                        "3174 [D loss: (3.912)(R 3.910, F 3.915)]  [G loss: -87.710] \n",
                        "3175 [D loss: (3.913)(R 3.911, F 3.916)]  [G loss: -87.726] \n",
                        "3176 [D loss: (3.914)(R 3.912, F 3.917)]  [G loss: -87.741] \n",
                        "3177 [D loss: (3.915)(R 3.913, F 3.918)]  [G loss: -87.756] \n",
                        "3178 [D loss: (3.916)(R 3.914, F 3.919)]  [G loss: -87.772] \n",
                        "3179 [D loss: (3.918)(R 3.915, F 3.920)]  [G loss: -87.789] \n",
                        "3180 [D loss: (3.919)(R 3.916, F 3.921)]  [G loss: -87.806] \n",
                        "3181 [D loss: (3.920)(R 3.917, F 3.922)]  [G loss: -87.824] \n",
                        "3182 [D loss: (3.920)(R 3.918, F 3.922)]  [G loss: -87.842] \n",
                        "3183 [D loss: (3.920)(R 3.918, F 3.923)]  [G loss: -87.861] \n",
                        "3184 [D loss: (3.920)(R 3.918, F 3.923)]  [G loss: -87.880] \n",
                        "3185 [D loss: (3.921)(R 3.919, F 3.924)]  [G loss: -87.902] \n",
                        "3186 [D loss: (3.923)(R 3.920, F 3.925)]  [G loss: -87.925] \n",
                        "3187 [D loss: (3.925)(R 3.922, F 3.927)]  [G loss: -87.949] \n",
                        "3188 [D loss: (3.927)(R 3.924, F 3.930)]  [G loss: -87.974] \n",
                        "3189 [D loss: (3.930)(R 3.928, F 3.933)]  [G loss: -87.997] \n",
                        "3190 [D loss: (3.933)(R 3.931, F 3.936)]  [G loss: -88.019] \n",
                        "3191 [D loss: (3.936)(R 3.933, F 3.938)]  [G loss: -88.039] \n",
                        "3192 [D loss: (3.937)(R 3.935, F 3.939)]  [G loss: -88.056] \n",
                        "3193 [D loss: (3.938)(R 3.935, F 3.940)]  [G loss: -88.072] \n",
                        "3194 [D loss: (3.938)(R 3.936, F 3.940)]  [G loss: -88.088] \n",
                        "3195 [D loss: (3.938)(R 3.936, F 3.940)]  [G loss: -88.102] \n",
                        "3196 [D loss: (3.938)(R 3.935, F 3.940)]  [G loss: -88.117] \n",
                        "3197 [D loss: (3.938)(R 3.936, F 3.940)]  [G loss: -88.131] \n",
                        "3198 [D loss: (3.938)(R 3.936, F 3.940)]  [G loss: -88.144] \n",
                        "3199 [D loss: (3.939)(R 3.937, F 3.941)]  [G loss: -88.155] \n",
                        "3200 [D loss: (3.939)(R 3.938, F 3.941)]  [G loss: -88.166] \n",
                        "3201 [D loss: (3.939)(R 3.937, F 3.941)]  [G loss: -88.175] \n",
                        "3202 [D loss: (3.938)(R 3.937, F 3.940)]  [G loss: -88.186] \n",
                        "3203 [D loss: (3.938)(R 3.936, F 3.939)]  [G loss: -88.195] \n",
                        "3204 [D loss: (3.937)(R 3.936, F 3.939)]  [G loss: -88.202] \n",
                        "3205 [D loss: (3.937)(R 3.935, F 3.938)]  [G loss: -88.207] \n",
                        "3206 [D loss: (3.936)(R 3.935, F 3.938)]  [G loss: -88.210] \n",
                        "3207 [D loss: (3.934)(R 3.933, F 3.936)]  [G loss: -88.212] \n",
                        "3208 [D loss: (3.933)(R 3.931, F 3.934)]  [G loss: -88.214] \n",
                        "3209 [D loss: (3.931)(R 3.929, F 3.932)]  [G loss: -88.216] \n",
                        "3210 [D loss: (3.927)(R 3.926, F 3.929)]  [G loss: -88.219] \n",
                        "3211 [D loss: (3.923)(R 3.922, F 3.925)]  [G loss: -88.224] \n",
                        "3212 [D loss: (3.918)(R 3.917, F 3.920)]  [G loss: -88.232] \n",
                        "3213 [D loss: (3.913)(R 3.911, F 3.914)]  [G loss: -88.243] \n",
                        "3214 [D loss: (3.907)(R 3.905, F 3.909)]  [G loss: -88.260] \n",
                        "3215 [D loss: (3.901)(R 3.899, F 3.903)]  [G loss: -88.283] \n",
                        "3216 [D loss: (3.896)(R 3.894, F 3.898)]  [G loss: -88.314] \n",
                        "3217 [D loss: (3.891)(R 3.889, F 3.893)]  [G loss: -88.354] \n",
                        "3218 [D loss: (3.886)(R 3.884, F 3.889)]  [G loss: -88.402] \n",
                        "3219 [D loss: (3.884)(R 3.881, F 3.887)]  [G loss: -88.460] \n",
                        "3220 [D loss: (3.885)(R 3.882, F 3.888)]  [G loss: -88.523] \n",
                        "3221 [D loss: (3.888)(R 3.885, F 3.892)]  [G loss: -88.590] \n",
                        "3222 [D loss: (3.896)(R 3.892, F 3.899)]  [G loss: -88.660] \n",
                        "3223 [D loss: (3.905)(R 3.901, F 3.909)]  [G loss: -88.728] \n",
                        "3224 [D loss: (3.917)(R 3.913, F 3.921)]  [G loss: -88.792] \n",
                        "3225 [D loss: (3.928)(R 3.924, F 3.932)]  [G loss: -88.851] \n",
                        "3226 [D loss: (3.940)(R 3.936, F 3.944)]  [G loss: -88.902] \n",
                        "3227 [D loss: (3.949)(R 3.945, F 3.952)]  [G loss: -88.947] \n",
                        "3228 [D loss: (3.956)(R 3.953, F 3.959)]  [G loss: -88.986] \n",
                        "3229 [D loss: (3.961)(R 3.958, F 3.964)]  [G loss: -89.019] \n",
                        "3230 [D loss: (3.964)(R 3.962, F 3.967)]  [G loss: -89.046] \n",
                        "3231 [D loss: (3.965)(R 3.963, F 3.968)]  [G loss: -89.070] \n",
                        "3232 [D loss: (3.965)(R 3.963, F 3.967)]  [G loss: -89.091] \n",
                        "3233 [D loss: (3.964)(R 3.962, F 3.966)]  [G loss: -89.111] \n",
                        "3234 [D loss: (3.963)(R 3.961, F 3.965)]  [G loss: -89.130] \n",
                        "3235 [D loss: (3.962)(R 3.960, F 3.964)]  [G loss: -89.149] \n",
                        "3236 [D loss: (3.961)(R 3.959, F 3.963)]  [G loss: -89.165] \n",
                        "3237 [D loss: (3.961)(R 3.959, F 3.963)]  [G loss: -89.178] \n",
                        "3238 [D loss: (3.960)(R 3.959, F 3.962)]  [G loss: -89.186] \n",
                        "3239 [D loss: (3.959)(R 3.957, F 3.961)]  [G loss: -89.190] \n",
                        "3240 [D loss: (3.957)(R 3.955, F 3.958)]  [G loss: -89.192] \n",
                        "3241 [D loss: (3.954)(R 3.953, F 3.956)]  [G loss: -89.195] \n",
                        "3242 [D loss: (3.952)(R 3.950, F 3.953)]  [G loss: -89.197] \n",
                        "3243 [D loss: (3.948)(R 3.947, F 3.950)]  [G loss: -89.202] \n",
                        "3244 [D loss: (3.945)(R 3.943, F 3.946)]  [G loss: -89.209] \n",
                        "3245 [D loss: (3.942)(R 3.940, F 3.944)]  [G loss: -89.216] \n",
                        "3246 [D loss: (3.939)(R 3.937, F 3.941)]  [G loss: -89.225] \n",
                        "3247 [D loss: (3.936)(R 3.934, F 3.938)]  [G loss: -89.234] \n",
                        "3248 [D loss: (3.934)(R 3.932, F 3.936)]  [G loss: -89.245] \n",
                        "3249 [D loss: (3.931)(R 3.929, F 3.933)]  [G loss: -89.257] \n",
                        "3250 [D loss: (3.927)(R 3.925, F 3.930)]  [G loss: -89.270] \n",
                        "3251 [D loss: (3.925)(R 3.923, F 3.927)]  [G loss: -89.284] \n",
                        "3252 [D loss: (3.922)(R 3.920, F 3.924)]  [G loss: -89.300] \n",
                        "3253 [D loss: (3.920)(R 3.918, F 3.922)]  [G loss: -89.318] \n",
                        "3254 [D loss: (3.917)(R 3.915, F 3.919)]  [G loss: -89.336] \n",
                        "3255 [D loss: (3.914)(R 3.912, F 3.916)]  [G loss: -89.357] \n",
                        "3256 [D loss: (3.910)(R 3.908, F 3.913)]  [G loss: -89.381] \n",
                        "3257 [D loss: (3.907)(R 3.905, F 3.909)]  [G loss: -89.408] \n",
                        "3258 [D loss: (3.905)(R 3.903, F 3.907)]  [G loss: -89.436] \n",
                        "3259 [D loss: (3.902)(R 3.900, F 3.904)]  [G loss: -89.467] \n",
                        "3260 [D loss: (3.901)(R 3.898, F 3.903)]  [G loss: -89.498] \n",
                        "3261 [D loss: (3.899)(R 3.897, F 3.902)]  [G loss: -89.529] \n",
                        "3262 [D loss: (3.899)(R 3.897, F 3.901)]  [G loss: -89.557] \n",
                        "3263 [D loss: (3.899)(R 3.897, F 3.901)]  [G loss: -89.583] \n",
                        "3264 [D loss: (3.900)(R 3.898, F 3.902)]  [G loss: -89.603] \n",
                        "3265 [D loss: (3.900)(R 3.898, F 3.902)]  [G loss: -89.616] \n",
                        "3266 [D loss: (3.899)(R 3.897, F 3.901)]  [G loss: -89.623] \n",
                        "3267 [D loss: (3.897)(R 3.896, F 3.899)]  [G loss: -89.624] \n",
                        "3268 [D loss: (3.895)(R 3.894, F 3.897)]  [G loss: -89.622] \n",
                        "3269 [D loss: (3.893)(R 3.891, F 3.894)]  [G loss: -89.617] \n",
                        "3270 [D loss: (3.890)(R 3.889, F 3.892)]  [G loss: -89.612] \n",
                        "3271 [D loss: (3.889)(R 3.887, F 3.890)]  [G loss: -89.607] \n",
                        "3272 [D loss: (3.888)(R 3.886, F 3.889)]  [G loss: -89.603] \n",
                        "3273 [D loss: (3.886)(R 3.885, F 3.888)]  [G loss: -89.598] \n",
                        "3274 [D loss: (3.885)(R 3.884, F 3.886)]  [G loss: -89.594] \n",
                        "3275 [D loss: (3.884)(R 3.883, F 3.885)]  [G loss: -89.588] \n",
                        "3276 [D loss: (3.883)(R 3.882, F 3.884)]  [G loss: -89.583] \n",
                        "3277 [D loss: (3.883)(R 3.881, F 3.884)]  [G loss: -89.577] \n",
                        "3278 [D loss: (3.882)(R 3.881, F 3.884)]  [G loss: -89.571] \n",
                        "3279 [D loss: (3.882)(R 3.881, F 3.884)]  [G loss: -89.565] \n",
                        "3280 [D loss: (3.883)(R 3.881, F 3.884)]  [G loss: -89.560] \n",
                        "3281 [D loss: (3.883)(R 3.882, F 3.885)]  [G loss: -89.555] \n",
                        "3282 [D loss: (3.884)(R 3.883, F 3.886)]  [G loss: -89.552] \n",
                        "3283 [D loss: (3.886)(R 3.884, F 3.887)]  [G loss: -89.551] \n",
                        "3284 [D loss: (3.888)(R 3.886, F 3.889)]  [G loss: -89.551] \n",
                        "3285 [D loss: (3.890)(R 3.888, F 3.891)]  [G loss: -89.554] \n",
                        "3286 [D loss: (3.892)(R 3.890, F 3.894)]  [G loss: -89.559] \n",
                        "3287 [D loss: (3.894)(R 3.892, F 3.896)]  [G loss: -89.566] \n",
                        "3288 [D loss: (3.897)(R 3.895, F 3.899)]  [G loss: -89.575] \n",
                        "3289 [D loss: (3.900)(R 3.898, F 3.902)]  [G loss: -89.586] \n",
                        "3290 [D loss: (3.902)(R 3.900, F 3.905)]  [G loss: -89.599] \n",
                        "3291 [D loss: (3.905)(R 3.903, F 3.907)]  [G loss: -89.612] \n",
                        "3292 [D loss: (3.908)(R 3.906, F 3.910)]  [G loss: -89.626] \n",
                        "3293 [D loss: (3.910)(R 3.908, F 3.912)]  [G loss: -89.640] \n",
                        "3294 [D loss: (3.912)(R 3.910, F 3.914)]  [G loss: -89.653] \n",
                        "3295 [D loss: (3.914)(R 3.911, F 3.916)]  [G loss: -89.665] \n",
                        "3296 [D loss: (3.915)(R 3.913, F 3.917)]  [G loss: -89.678] \n",
                        "3297 [D loss: (3.916)(R 3.914, F 3.918)]  [G loss: -89.690] \n",
                        "3298 [D loss: (3.917)(R 3.915, F 3.919)]  [G loss: -89.704] \n",
                        "3299 [D loss: (3.918)(R 3.916, F 3.920)]  [G loss: -89.718] \n",
                        "3300 [D loss: (3.919)(R 3.916, F 3.921)]  [G loss: -89.733] \n",
                        "3301 [D loss: (3.919)(R 3.917, F 3.922)]  [G loss: -89.747] \n",
                        "3302 [D loss: (3.920)(R 3.918, F 3.923)]  [G loss: -89.761] \n",
                        "3303 [D loss: (3.922)(R 3.919, F 3.924)]  [G loss: -89.775] \n",
                        "3304 [D loss: (3.923)(R 3.921, F 3.925)]  [G loss: -89.788] \n",
                        "3305 [D loss: (3.924)(R 3.922, F 3.926)]  [G loss: -89.800] \n",
                        "3306 [D loss: (3.925)(R 3.923, F 3.927)]  [G loss: -89.811] \n",
                        "3307 [D loss: (3.926)(R 3.924, F 3.928)]  [G loss: -89.823] \n",
                        "3308 [D loss: (3.927)(R 3.925, F 3.929)]  [G loss: -89.835] \n",
                        "3309 [D loss: (3.928)(R 3.926, F 3.930)]  [G loss: -89.847] \n",
                        "3310 [D loss: (3.930)(R 3.928, F 3.932)]  [G loss: -89.859] \n",
                        "3311 [D loss: (3.931)(R 3.929, F 3.933)]  [G loss: -89.872] \n",
                        "3312 [D loss: (3.933)(R 3.931, F 3.935)]  [G loss: -89.886] \n",
                        "3313 [D loss: (3.935)(R 3.933, F 3.937)]  [G loss: -89.899] \n",
                        "3314 [D loss: (3.937)(R 3.935, F 3.940)]  [G loss: -89.912] \n",
                        "3315 [D loss: (3.939)(R 3.937, F 3.942)]  [G loss: -89.924] \n",
                        "3316 [D loss: (3.941)(R 3.939, F 3.943)]  [G loss: -89.937] \n",
                        "3317 [D loss: (3.943)(R 3.941, F 3.945)]  [G loss: -89.948] \n",
                        "3318 [D loss: (3.945)(R 3.943, F 3.947)]  [G loss: -89.959] \n",
                        "3319 [D loss: (3.946)(R 3.944, F 3.948)]  [G loss: -89.967] \n",
                        "3320 [D loss: (3.948)(R 3.946, F 3.949)]  [G loss: -89.976] \n",
                        "3321 [D loss: (3.949)(R 3.947, F 3.950)]  [G loss: -89.982] \n",
                        "3322 [D loss: (3.949)(R 3.948, F 3.951)]  [G loss: -89.986] \n",
                        "3323 [D loss: (3.950)(R 3.948, F 3.951)]  [G loss: -89.988] \n",
                        "3324 [D loss: (3.949)(R 3.948, F 3.951)]  [G loss: -89.990] \n",
                        "3325 [D loss: (3.949)(R 3.947, F 3.951)]  [G loss: -89.991] \n",
                        "3326 [D loss: (3.949)(R 3.947, F 3.950)]  [G loss: -89.992] \n",
                        "3327 [D loss: (3.948)(R 3.946, F 3.949)]  [G loss: -89.992] \n",
                        "3328 [D loss: (3.947)(R 3.946, F 3.949)]  [G loss: -89.993] \n",
                        "3329 [D loss: (3.947)(R 3.945, F 3.948)]  [G loss: -89.993] \n",
                        "3330 [D loss: (3.946)(R 3.944, F 3.947)]  [G loss: -89.994] \n",
                        "3331 [D loss: (3.945)(R 3.944, F 3.947)]  [G loss: -89.996] \n",
                        "3332 [D loss: (3.945)(R 3.944, F 3.947)]  [G loss: -89.998] \n",
                        "3333 [D loss: (3.945)(R 3.944, F 3.947)]  [G loss: -90.000] \n",
                        "3334 [D loss: (3.945)(R 3.944, F 3.947)]  [G loss: -90.003] \n",
                        "3335 [D loss: (3.946)(R 3.944, F 3.947)]  [G loss: -90.007] \n",
                        "3336 [D loss: (3.946)(R 3.944, F 3.948)]  [G loss: -90.011] \n",
                        "3337 [D loss: (3.946)(R 3.945, F 3.948)]  [G loss: -90.016] \n",
                        "3338 [D loss: (3.947)(R 3.945, F 3.948)]  [G loss: -90.022] \n",
                        "3339 [D loss: (3.947)(R 3.945, F 3.949)]  [G loss: -90.029] \n",
                        "3340 [D loss: (3.948)(R 3.946, F 3.950)]  [G loss: -90.036] \n",
                        "3341 [D loss: (3.949)(R 3.947, F 3.950)]  [G loss: -90.044] \n",
                        "3342 [D loss: (3.949)(R 3.947, F 3.951)]  [G loss: -90.053] \n",
                        "3343 [D loss: (3.950)(R 3.948, F 3.952)]  [G loss: -90.062] \n",
                        "3344 [D loss: (3.951)(R 3.949, F 3.953)]  [G loss: -90.072] \n",
                        "3345 [D loss: (3.952)(R 3.950, F 3.954)]  [G loss: -90.082] \n",
                        "3346 [D loss: (3.953)(R 3.951, F 3.955)]  [G loss: -90.092] \n",
                        "3347 [D loss: (3.954)(R 3.952, F 3.956)]  [G loss: -90.103] \n",
                        "3348 [D loss: (3.955)(R 3.953, F 3.957)]  [G loss: -90.113] \n",
                        "3349 [D loss: (3.956)(R 3.954, F 3.958)]  [G loss: -90.124] \n",
                        "3350 [D loss: (3.957)(R 3.955, F 3.959)]  [G loss: -90.135] \n",
                        "3351 [D loss: (3.958)(R 3.956, F 3.960)]  [G loss: -90.145] \n",
                        "3352 [D loss: (3.959)(R 3.957, F 3.961)]  [G loss: -90.157] \n",
                        "3353 [D loss: (3.961)(R 3.958, F 3.963)]  [G loss: -90.168] \n",
                        "3354 [D loss: (3.962)(R 3.960, F 3.964)]  [G loss: -90.180] \n",
                        "3355 [D loss: (3.963)(R 3.961, F 3.965)]  [G loss: -90.193] \n",
                        "3356 [D loss: (3.964)(R 3.962, F 3.966)]  [G loss: -90.205] \n",
                        "3357 [D loss: (3.965)(R 3.963, F 3.967)]  [G loss: -90.216] \n",
                        "3358 [D loss: (3.966)(R 3.964, F 3.969)]  [G loss: -90.227] \n",
                        "3359 [D loss: (3.968)(R 3.966, F 3.970)]  [G loss: -90.239] \n",
                        "3360 [D loss: (3.969)(R 3.967, F 3.971)]  [G loss: -90.250] \n",
                        "3361 [D loss: (3.970)(R 3.968, F 3.972)]  [G loss: -90.262] \n",
                        "3362 [D loss: (3.971)(R 3.969, F 3.973)]  [G loss: -90.273] \n",
                        "3363 [D loss: (3.972)(R 3.970, F 3.974)]  [G loss: -90.285] \n",
                        "3364 [D loss: (3.973)(R 3.971, F 3.975)]  [G loss: -90.296] \n",
                        "3365 [D loss: (3.974)(R 3.972, F 3.976)]  [G loss: -90.307] \n",
                        "3366 [D loss: (3.975)(R 3.973, F 3.977)]  [G loss: -90.318] \n",
                        "3367 [D loss: (3.976)(R 3.974, F 3.978)]  [G loss: -90.329] \n",
                        "3368 [D loss: (3.977)(R 3.975, F 3.979)]  [G loss: -90.340] \n",
                        "3369 [D loss: (3.978)(R 3.976, F 3.980)]  [G loss: -90.351] \n",
                        "3370 [D loss: (3.979)(R 3.977, F 3.981)]  [G loss: -90.362] \n",
                        "3371 [D loss: (3.980)(R 3.978, F 3.982)]  [G loss: -90.373] \n",
                        "3372 [D loss: (3.981)(R 3.979, F 3.983)]  [G loss: -90.385] \n",
                        "3373 [D loss: (3.982)(R 3.980, F 3.984)]  [G loss: -90.396] \n",
                        "3374 [D loss: (3.983)(R 3.981, F 3.985)]  [G loss: -90.408] \n",
                        "3375 [D loss: (3.984)(R 3.982, F 3.986)]  [G loss: -90.419] \n",
                        "3376 [D loss: (3.985)(R 3.983, F 3.987)]  [G loss: -90.431] \n",
                        "3377 [D loss: (3.986)(R 3.984, F 3.988)]  [G loss: -90.442] \n",
                        "3378 [D loss: (3.987)(R 3.985, F 3.989)]  [G loss: -90.454] \n",
                        "3379 [D loss: (3.988)(R 3.986, F 3.990)]  [G loss: -90.465] \n",
                        "3380 [D loss: (3.989)(R 3.987, F 3.991)]  [G loss: -90.476] \n",
                        "3381 [D loss: (3.990)(R 3.988, F 3.992)]  [G loss: -90.488] \n",
                        "3382 [D loss: (3.991)(R 3.989, F 3.993)]  [G loss: -90.499] \n",
                        "3383 [D loss: (3.992)(R 3.990, F 3.994)]  [G loss: -90.511] \n",
                        "3384 [D loss: (3.993)(R 3.991, F 3.995)]  [G loss: -90.523] \n",
                        "3385 [D loss: (3.994)(R 3.992, F 3.996)]  [G loss: -90.534] \n",
                        "3386 [D loss: (3.995)(R 3.993, F 3.998)]  [G loss: -90.546] \n",
                        "3387 [D loss: (3.997)(R 3.995, F 3.999)]  [G loss: -90.557] \n",
                        "3388 [D loss: (3.998)(R 3.996, F 4.000)]  [G loss: -90.568] \n",
                        "3389 [D loss: (3.999)(R 3.997, F 4.001)]  [G loss: -90.579] \n",
                        "3390 [D loss: (4.000)(R 3.998, F 4.002)]  [G loss: -90.590] \n",
                        "3391 [D loss: (4.000)(R 3.998, F 4.002)]  [G loss: -90.601] \n",
                        "3392 [D loss: (4.001)(R 3.999, F 4.003)]  [G loss: -90.612] \n",
                        "3393 [D loss: (4.002)(R 4.000, F 4.004)]  [G loss: -90.623] \n",
                        "3394 [D loss: (4.003)(R 4.001, F 4.005)]  [G loss: -90.634] \n",
                        "3395 [D loss: (4.004)(R 4.002, F 4.006)]  [G loss: -90.644] \n",
                        "3396 [D loss: (4.005)(R 4.003, F 4.007)]  [G loss: -90.655] \n",
                        "3397 [D loss: (4.005)(R 4.003, F 4.007)]  [G loss: -90.666] \n",
                        "3398 [D loss: (4.006)(R 4.004, F 4.008)]  [G loss: -90.676] \n",
                        "3399 [D loss: (4.007)(R 4.005, F 4.009)]  [G loss: -90.687] \n",
                        "3400 [D loss: (4.008)(R 4.006, F 4.010)]  [G loss: -90.697] \n",
                        "3401 [D loss: (4.008)(R 4.006, F 4.010)]  [G loss: -90.707] \n",
                        "3402 [D loss: (4.009)(R 4.007, F 4.011)]  [G loss: -90.718] \n",
                        "3403 [D loss: (4.010)(R 4.008, F 4.012)]  [G loss: -90.728] \n",
                        "3404 [D loss: (4.010)(R 4.008, F 4.012)]  [G loss: -90.739] \n",
                        "3405 [D loss: (4.011)(R 4.009, F 4.013)]  [G loss: -90.749] \n",
                        "3406 [D loss: (4.012)(R 4.010, F 4.014)]  [G loss: -90.759] \n",
                        "3407 [D loss: (4.013)(R 4.011, F 4.015)]  [G loss: -90.769] \n",
                        "3408 [D loss: (4.013)(R 4.011, F 4.015)]  [G loss: -90.779] \n",
                        "3409 [D loss: (4.014)(R 4.012, F 4.016)]  [G loss: -90.789] \n",
                        "3410 [D loss: (4.014)(R 4.012, F 4.016)]  [G loss: -90.799] \n",
                        "3411 [D loss: (4.015)(R 4.013, F 4.017)]  [G loss: -90.809] \n",
                        "3412 [D loss: (4.016)(R 4.014, F 4.018)]  [G loss: -90.820] \n",
                        "3413 [D loss: (4.017)(R 4.015, F 4.019)]  [G loss: -90.831] \n",
                        "3414 [D loss: (4.017)(R 4.015, F 4.019)]  [G loss: -90.842] \n",
                        "3415 [D loss: (4.018)(R 4.016, F 4.020)]  [G loss: -90.853] \n",
                        "3416 [D loss: (4.019)(R 4.017, F 4.021)]  [G loss: -90.864] \n",
                        "3417 [D loss: (4.020)(R 4.018, F 4.022)]  [G loss: -90.875] \n",
                        "3418 [D loss: (4.021)(R 4.019, F 4.023)]  [G loss: -90.887] \n",
                        "3419 [D loss: (4.022)(R 4.020, F 4.024)]  [G loss: -90.898] \n",
                        "3420 [D loss: (4.023)(R 4.021, F 4.025)]  [G loss: -90.910] \n",
                        "3421 [D loss: (4.023)(R 4.021, F 4.025)]  [G loss: -90.922] \n",
                        "3422 [D loss: (4.024)(R 4.022, F 4.026)]  [G loss: -90.934] \n",
                        "3423 [D loss: (4.026)(R 4.024, F 4.028)]  [G loss: -90.945] \n",
                        "3424 [D loss: (4.026)(R 4.024, F 4.028)]  [G loss: -90.957] \n",
                        "3425 [D loss: (4.027)(R 4.025, F 4.029)]  [G loss: -90.968] \n",
                        "3426 [D loss: (4.028)(R 4.026, F 4.030)]  [G loss: -90.980] \n",
                        "3427 [D loss: (4.029)(R 4.027, F 4.031)]  [G loss: -90.992] \n",
                        "3428 [D loss: (4.030)(R 4.028, F 4.032)]  [G loss: -91.004] \n",
                        "3429 [D loss: (4.031)(R 4.029, F 4.033)]  [G loss: -91.016] \n",
                        "3430 [D loss: (4.032)(R 4.030, F 4.034)]  [G loss: -91.027] \n",
                        "3431 [D loss: (4.032)(R 4.030, F 4.034)]  [G loss: -91.039] \n",
                        "3432 [D loss: (4.033)(R 4.031, F 4.035)]  [G loss: -91.051] \n",
                        "3433 [D loss: (4.034)(R 4.032, F 4.036)]  [G loss: -91.062] \n",
                        "3434 [D loss: (4.035)(R 4.033, F 4.037)]  [G loss: -91.073] \n",
                        "3435 [D loss: (4.035)(R 4.033, F 4.037)]  [G loss: -91.085] \n",
                        "3436 [D loss: (4.036)(R 4.034, F 4.038)]  [G loss: -91.095] \n",
                        "3437 [D loss: (4.037)(R 4.035, F 4.039)]  [G loss: -91.106] \n",
                        "3438 [D loss: (4.037)(R 4.035, F 4.039)]  [G loss: -91.117] \n",
                        "3439 [D loss: (4.038)(R 4.036, F 4.040)]  [G loss: -91.128] \n",
                        "3440 [D loss: (4.039)(R 4.037, F 4.041)]  [G loss: -91.139] \n",
                        "3441 [D loss: (4.039)(R 4.037, F 4.041)]  [G loss: -91.149] \n",
                        "3442 [D loss: (4.040)(R 4.038, F 4.042)]  [G loss: -91.160] \n",
                        "3443 [D loss: (4.041)(R 4.039, F 4.043)]  [G loss: -91.170] \n",
                        "3444 [D loss: (4.042)(R 4.040, F 4.044)]  [G loss: -91.181] \n",
                        "3445 [D loss: (4.042)(R 4.040, F 4.044)]  [G loss: -91.192] \n",
                        "3446 [D loss: (4.043)(R 4.041, F 4.045)]  [G loss: -91.203] \n",
                        "3447 [D loss: (4.044)(R 4.042, F 4.046)]  [G loss: -91.213] \n",
                        "3448 [D loss: (4.044)(R 4.042, F 4.046)]  [G loss: -91.224] \n",
                        "3449 [D loss: (4.045)(R 4.043, F 4.047)]  [G loss: -91.235] \n",
                        "3450 [D loss: (4.046)(R 4.044, F 4.048)]  [G loss: -91.246] \n",
                        "3451 [D loss: (4.046)(R 4.044, F 4.048)]  [G loss: -91.257] \n",
                        "3452 [D loss: (4.047)(R 4.045, F 4.049)]  [G loss: -91.269] \n",
                        "3453 [D loss: (4.048)(R 4.046, F 4.050)]  [G loss: -91.281] \n",
                        "3454 [D loss: (4.049)(R 4.047, F 4.051)]  [G loss: -91.293] \n",
                        "3455 [D loss: (4.050)(R 4.048, F 4.052)]  [G loss: -91.306] \n",
                        "3456 [D loss: (4.051)(R 4.049, F 4.053)]  [G loss: -91.318] \n",
                        "3457 [D loss: (4.051)(R 4.049, F 4.054)]  [G loss: -91.329] \n",
                        "3458 [D loss: (4.052)(R 4.050, F 4.054)]  [G loss: -91.343] \n",
                        "3459 [D loss: (4.053)(R 4.051, F 4.055)]  [G loss: -91.355] \n",
                        "3460 [D loss: (4.054)(R 4.052, F 4.056)]  [G loss: -91.366] \n",
                        "3461 [D loss: (4.055)(R 4.053, F 4.057)]  [G loss: -91.379] \n",
                        "3462 [D loss: (4.056)(R 4.054, F 4.058)]  [G loss: -91.392] \n",
                        "3463 [D loss: (4.057)(R 4.055, F 4.059)]  [G loss: -91.403] \n",
                        "3464 [D loss: (4.058)(R 4.056, F 4.060)]  [G loss: -91.415] \n",
                        "3465 [D loss: (4.059)(R 4.056, F 4.061)]  [G loss: -91.427] \n",
                        "3466 [D loss: (4.059)(R 4.057, F 4.061)]  [G loss: -91.440] \n",
                        "3467 [D loss: (4.060)(R 4.058, F 4.062)]  [G loss: -91.451] \n",
                        "3468 [D loss: (4.061)(R 4.059, F 4.063)]  [G loss: -91.463] \n",
                        "3469 [D loss: (4.061)(R 4.059, F 4.063)]  [G loss: -91.475] \n",
                        "3470 [D loss: (4.062)(R 4.060, F 4.064)]  [G loss: -91.487] \n",
                        "3471 [D loss: (4.063)(R 4.061, F 4.065)]  [G loss: -91.498] \n",
                        "3472 [D loss: (4.064)(R 4.062, F 4.066)]  [G loss: -91.511] \n",
                        "3473 [D loss: (4.064)(R 4.062, F 4.066)]  [G loss: -91.524] \n",
                        "3474 [D loss: (4.065)(R 4.063, F 4.068)]  [G loss: -91.537] \n",
                        "3475 [D loss: (4.067)(R 4.065, F 4.069)]  [G loss: -91.547] \n",
                        "3476 [D loss: (4.067)(R 4.065, F 4.069)]  [G loss: -91.559] \n",
                        "3477 [D loss: (4.068)(R 4.066, F 4.070)]  [G loss: -91.571] \n",
                        "3478 [D loss: (4.069)(R 4.067, F 4.071)]  [G loss: -91.584] \n",
                        "3479 [D loss: (4.069)(R 4.067, F 4.071)]  [G loss: -91.598] \n",
                        "3480 [D loss: (4.070)(R 4.067, F 4.072)]  [G loss: -91.613] \n",
                        "3481 [D loss: (4.071)(R 4.069, F 4.073)]  [G loss: -91.628] \n",
                        "3482 [D loss: (4.073)(R 4.071, F 4.075)]  [G loss: -91.641] \n",
                        "3483 [D loss: (4.075)(R 4.073, F 4.077)]  [G loss: -91.652] \n",
                        "3484 [D loss: (4.075)(R 4.073, F 4.077)]  [G loss: -91.662] \n",
                        "3485 [D loss: (4.076)(R 4.074, F 4.078)]  [G loss: -91.673] \n",
                        "3486 [D loss: (4.077)(R 4.075, F 4.079)]  [G loss: -91.683] \n",
                        "3487 [D loss: (4.077)(R 4.075, F 4.079)]  [G loss: -91.695] \n",
                        "3488 [D loss: (4.078)(R 4.076, F 4.080)]  [G loss: -91.707] \n",
                        "3489 [D loss: (4.079)(R 4.077, F 4.081)]  [G loss: -91.719] \n",
                        "3490 [D loss: (4.079)(R 4.077, F 4.081)]  [G loss: -91.732] \n",
                        "3491 [D loss: (4.080)(R 4.078, F 4.082)]  [G loss: -91.745] \n",
                        "3492 [D loss: (4.081)(R 4.079, F 4.083)]  [G loss: -91.758] \n",
                        "3493 [D loss: (4.082)(R 4.080, F 4.084)]  [G loss: -91.771] \n",
                        "3494 [D loss: (4.083)(R 4.081, F 4.085)]  [G loss: -91.783] \n",
                        "3495 [D loss: (4.084)(R 4.082, F 4.086)]  [G loss: -91.796] \n",
                        "3496 [D loss: (4.085)(R 4.083, F 4.087)]  [G loss: -91.809] \n",
                        "3497 [D loss: (4.086)(R 4.084, F 4.088)]  [G loss: -91.822] \n",
                        "3498 [D loss: (4.087)(R 4.084, F 4.089)]  [G loss: -91.835] \n",
                        "3499 [D loss: (4.088)(R 4.085, F 4.090)]  [G loss: -91.848] \n",
                        "3500 [D loss: (4.088)(R 4.086, F 4.090)]  [G loss: -91.860] \n",
                        "3501 [D loss: (4.089)(R 4.087, F 4.091)]  [G loss: -91.872] \n",
                        "3502 [D loss: (4.090)(R 4.088, F 4.092)]  [G loss: -91.884] \n",
                        "3503 [D loss: (4.091)(R 4.089, F 4.093)]  [G loss: -91.895] \n",
                        "3504 [D loss: (4.092)(R 4.090, F 4.094)]  [G loss: -91.905] \n",
                        "3505 [D loss: (4.092)(R 4.090, F 4.094)]  [G loss: -91.915] \n",
                        "3506 [D loss: (4.093)(R 4.091, F 4.095)]  [G loss: -91.925] \n",
                        "3507 [D loss: (4.093)(R 4.091, F 4.095)]  [G loss: -91.936] \n",
                        "3508 [D loss: (4.094)(R 4.092, F 4.096)]  [G loss: -91.947] \n",
                        "3509 [D loss: (4.094)(R 4.092, F 4.096)]  [G loss: -91.959] \n",
                        "3510 [D loss: (4.095)(R 4.093, F 4.097)]  [G loss: -91.970] \n",
                        "3511 [D loss: (4.096)(R 4.094, F 4.098)]  [G loss: -91.982] \n",
                        "3512 [D loss: (4.096)(R 4.094, F 4.098)]  [G loss: -91.994] \n",
                        "3513 [D loss: (4.097)(R 4.095, F 4.099)]  [G loss: -92.005] \n",
                        "3514 [D loss: (4.098)(R 4.096, F 4.100)]  [G loss: -92.017] \n",
                        "3515 [D loss: (4.098)(R 4.096, F 4.100)]  [G loss: -92.029] \n",
                        "3516 [D loss: (4.099)(R 4.097, F 4.101)]  [G loss: -92.040] \n",
                        "3517 [D loss: (4.100)(R 4.098, F 4.102)]  [G loss: -92.052] \n",
                        "3518 [D loss: (4.101)(R 4.099, F 4.103)]  [G loss: -92.064] \n",
                        "3519 [D loss: (4.101)(R 4.099, F 4.103)]  [G loss: -92.076] \n",
                        "3520 [D loss: (4.102)(R 4.100, F 4.104)]  [G loss: -92.088] \n",
                        "3521 [D loss: (4.103)(R 4.101, F 4.105)]  [G loss: -92.100] \n",
                        "3522 [D loss: (4.104)(R 4.102, F 4.106)]  [G loss: -92.113] \n",
                        "3523 [D loss: (4.104)(R 4.102, F 4.106)]  [G loss: -92.125] \n",
                        "3524 [D loss: (4.105)(R 4.103, F 4.107)]  [G loss: -92.138] \n",
                        "3525 [D loss: (4.106)(R 4.104, F 4.108)]  [G loss: -92.150] \n",
                        "3526 [D loss: (4.107)(R 4.105, F 4.109)]  [G loss: -92.163] \n",
                        "3527 [D loss: (4.107)(R 4.105, F 4.110)]  [G loss: -92.175] \n",
                        "3528 [D loss: (4.108)(R 4.106, F 4.110)]  [G loss: -92.187] \n",
                        "3529 [D loss: (4.109)(R 4.107, F 4.111)]  [G loss: -92.199] \n",
                        "3530 [D loss: (4.110)(R 4.108, F 4.112)]  [G loss: -92.211] \n",
                        "3531 [D loss: (4.111)(R 4.109, F 4.113)]  [G loss: -92.222] \n",
                        "3532 [D loss: (4.112)(R 4.110, F 4.113)]  [G loss: -92.234] \n",
                        "3533 [D loss: (4.112)(R 4.110, F 4.114)]  [G loss: -92.244] \n",
                        "3534 [D loss: (4.113)(R 4.111, F 4.115)]  [G loss: -92.254] \n",
                        "3535 [D loss: (4.113)(R 4.111, F 4.115)]  [G loss: -92.265] \n",
                        "3536 [D loss: (4.114)(R 4.112, F 4.116)]  [G loss: -92.275] \n",
                        "3537 [D loss: (4.114)(R 4.112, F 4.116)]  [G loss: -92.287] \n",
                        "3538 [D loss: (4.115)(R 4.113, F 4.117)]  [G loss: -92.299] \n",
                        "3539 [D loss: (4.115)(R 4.113, F 4.117)]  [G loss: -92.312] \n",
                        "3540 [D loss: (4.116)(R 4.114, F 4.118)]  [G loss: -92.326] \n",
                        "3541 [D loss: (4.117)(R 4.114, F 4.119)]  [G loss: -92.339] \n",
                        "3542 [D loss: (4.117)(R 4.115, F 4.120)]  [G loss: -92.353] \n",
                        "3543 [D loss: (4.118)(R 4.116, F 4.120)]  [G loss: -92.367] \n",
                        "3544 [D loss: (4.120)(R 4.118, F 4.122)]  [G loss: -92.381] \n",
                        "3545 [D loss: (4.121)(R 4.119, F 4.123)]  [G loss: -92.393] \n",
                        "3546 [D loss: (4.122)(R 4.120, F 4.124)]  [G loss: -92.405] \n",
                        "3547 [D loss: (4.122)(R 4.120, F 4.124)]  [G loss: -92.416] \n",
                        "3548 [D loss: (4.123)(R 4.121, F 4.125)]  [G loss: -92.428] \n",
                        "3549 [D loss: (4.124)(R 4.122, F 4.126)]  [G loss: -92.440] \n",
                        "3550 [D loss: (4.124)(R 4.122, F 4.126)]  [G loss: -92.452] \n",
                        "3551 [D loss: (4.125)(R 4.123, F 4.127)]  [G loss: -92.464] \n",
                        "3552 [D loss: (4.125)(R 4.123, F 4.127)]  [G loss: -92.478] \n",
                        "3553 [D loss: (4.126)(R 4.124, F 4.128)]  [G loss: -92.492] \n",
                        "3554 [D loss: (4.127)(R 4.125, F 4.129)]  [G loss: -92.505] \n",
                        "3555 [D loss: (4.128)(R 4.126, F 4.130)]  [G loss: -92.519] \n",
                        "3556 [D loss: (4.129)(R 4.127, F 4.131)]  [G loss: -92.531] \n",
                        "3557 [D loss: (4.130)(R 4.128, F 4.132)]  [G loss: -92.544] \n",
                        "3558 [D loss: (4.131)(R 4.129, F 4.133)]  [G loss: -92.557] \n",
                        "3559 [D loss: (4.132)(R 4.130, F 4.134)]  [G loss: -92.568] \n",
                        "3560 [D loss: (4.132)(R 4.130, F 4.134)]  [G loss: -92.580] \n",
                        "3561 [D loss: (4.133)(R 4.131, F 4.135)]  [G loss: -92.592] \n",
                        "3562 [D loss: (4.133)(R 4.131, F 4.135)]  [G loss: -92.606] \n",
                        "3563 [D loss: (4.134)(R 4.132, F 4.136)]  [G loss: -92.619] \n",
                        "3564 [D loss: (4.135)(R 4.133, F 4.137)]  [G loss: -92.634] \n",
                        "3565 [D loss: (4.136)(R 4.133, F 4.138)]  [G loss: -92.648] \n",
                        "3566 [D loss: (4.137)(R 4.135, F 4.139)]  [G loss: -92.662] \n",
                        "3567 [D loss: (4.138)(R 4.136, F 4.140)]  [G loss: -92.674] \n",
                        "3568 [D loss: (4.139)(R 4.137, F 4.140)]  [G loss: -92.684] \n",
                        "3569 [D loss: (4.139)(R 4.137, F 4.141)]  [G loss: -92.695] \n",
                        "3570 [D loss: (4.140)(R 4.138, F 4.142)]  [G loss: -92.705] \n",
                        "3571 [D loss: (4.140)(R 4.138, F 4.142)]  [G loss: -92.715] \n",
                        "3572 [D loss: (4.141)(R 4.139, F 4.143)]  [G loss: -92.726] \n",
                        "3573 [D loss: (4.141)(R 4.139, F 4.143)]  [G loss: -92.739] \n",
                        "3574 [D loss: (4.141)(R 4.139, F 4.144)]  [G loss: -92.752] \n",
                        "3575 [D loss: (4.142)(R 4.140, F 4.144)]  [G loss: -92.766] \n",
                        "3576 [D loss: (4.143)(R 4.141, F 4.145)]  [G loss: -92.781] \n",
                        "3577 [D loss: (4.144)(R 4.142, F 4.147)]  [G loss: -92.796] \n",
                        "3578 [D loss: (4.145)(R 4.143, F 4.148)]  [G loss: -92.809] \n",
                        "3579 [D loss: (4.146)(R 4.144, F 4.148)]  [G loss: -92.822] \n",
                        "3580 [D loss: (4.147)(R 4.145, F 4.149)]  [G loss: -92.834] \n",
                        "3581 [D loss: (4.148)(R 4.146, F 4.150)]  [G loss: -92.845] \n",
                        "3582 [D loss: (4.149)(R 4.147, F 4.151)]  [G loss: -92.855] \n",
                        "3583 [D loss: (4.149)(R 4.147, F 4.151)]  [G loss: -92.865] \n",
                        "3584 [D loss: (4.150)(R 4.148, F 4.151)]  [G loss: -92.876] \n",
                        "3585 [D loss: (4.150)(R 4.148, F 4.152)]  [G loss: -92.888] \n",
                        "3586 [D loss: (4.150)(R 4.148, F 4.152)]  [G loss: -92.900] \n",
                        "3587 [D loss: (4.151)(R 4.149, F 4.153)]  [G loss: -92.913] \n",
                        "3588 [D loss: (4.151)(R 4.149, F 4.153)]  [G loss: -92.928] \n",
                        "3589 [D loss: (4.152)(R 4.150, F 4.154)]  [G loss: -92.943] \n",
                        "3590 [D loss: (4.153)(R 4.151, F 4.156)]  [G loss: -92.958] \n",
                        "3591 [D loss: (4.154)(R 4.152, F 4.157)]  [G loss: -92.971] \n",
                        "3592 [D loss: (4.155)(R 4.153, F 4.157)]  [G loss: -92.983] \n",
                        "3593 [D loss: (4.156)(R 4.154, F 4.158)]  [G loss: -92.996] \n",
                        "3594 [D loss: (4.157)(R 4.155, F 4.159)]  [G loss: -93.006] \n",
                        "3595 [D loss: (4.157)(R 4.155, F 4.159)]  [G loss: -93.015] \n",
                        "3596 [D loss: (4.158)(R 4.156, F 4.159)]  [G loss: -93.024] \n",
                        "3597 [D loss: (4.158)(R 4.156, F 4.160)]  [G loss: -93.033] \n",
                        "3598 [D loss: (4.158)(R 4.156, F 4.160)]  [G loss: -93.042] \n",
                        "3599 [D loss: (4.159)(R 4.157, F 4.161)]  [G loss: -93.052] \n",
                        "3600 [D loss: (4.159)(R 4.157, F 4.161)]  [G loss: -93.063] \n",
                        "3601 [D loss: (4.158)(R 4.157, F 4.160)]  [G loss: -93.076] \n",
                        "3602 [D loss: (4.158)(R 4.156, F 4.160)]  [G loss: -93.090] \n",
                        "3603 [D loss: (4.159)(R 4.156, F 4.161)]  [G loss: -93.108] \n",
                        "3604 [D loss: (4.160)(R 4.157, F 4.162)]  [G loss: -93.128] \n",
                        "3605 [D loss: (4.161)(R 4.159, F 4.164)]  [G loss: -93.147] \n",
                        "3606 [D loss: (4.163)(R 4.161, F 4.166)]  [G loss: -93.165] \n",
                        "3607 [D loss: (4.165)(R 4.163, F 4.167)]  [G loss: -93.181] \n",
                        "3608 [D loss: (4.167)(R 4.164, F 4.169)]  [G loss: -93.197] \n",
                        "3609 [D loss: (4.169)(R 4.167, F 4.171)]  [G loss: -93.212] \n",
                        "3610 [D loss: (4.170)(R 4.168, F 4.172)]  [G loss: -93.225] \n",
                        "3611 [D loss: (4.171)(R 4.170, F 4.173)]  [G loss: -93.235] \n",
                        "3612 [D loss: (4.172)(R 4.170, F 4.174)]  [G loss: -93.242] \n",
                        "3613 [D loss: (4.172)(R 4.171, F 4.174)]  [G loss: -93.247] \n",
                        "3614 [D loss: (4.172)(R 4.170, F 4.173)]  [G loss: -93.251] \n",
                        "3615 [D loss: (4.171)(R 4.170, F 4.173)]  [G loss: -93.253] \n",
                        "3616 [D loss: (4.171)(R 4.169, F 4.172)]  [G loss: -93.255] \n",
                        "3617 [D loss: (4.170)(R 4.168, F 4.171)]  [G loss: -93.257] \n",
                        "3618 [D loss: (4.170)(R 4.168, F 4.171)]  [G loss: -93.260] \n",
                        "3619 [D loss: (4.169)(R 4.168, F 4.171)]  [G loss: -93.265] \n",
                        "3620 [D loss: (4.169)(R 4.168, F 4.171)]  [G loss: -93.272] \n",
                        "3621 [D loss: (4.169)(R 4.167, F 4.171)]  [G loss: -93.282] \n",
                        "3622 [D loss: (4.169)(R 4.167, F 4.171)]  [G loss: -93.295] \n",
                        "3623 [D loss: (4.169)(R 4.167, F 4.171)]  [G loss: -93.310] \n",
                        "3624 [D loss: (4.169)(R 4.167, F 4.171)]  [G loss: -93.328] \n",
                        "3625 [D loss: (4.169)(R 4.167, F 4.172)]  [G loss: -93.349] \n",
                        "3626 [D loss: (4.171)(R 4.168, F 4.173)]  [G loss: -93.372] \n",
                        "3627 [D loss: (4.174)(R 4.171, F 4.176)]  [G loss: -93.397] \n",
                        "3628 [D loss: (4.177)(R 4.174, F 4.179)]  [G loss: -93.422] \n",
                        "3629 [D loss: (4.180)(R 4.177, F 4.183)]  [G loss: -93.447] \n",
                        "3630 [D loss: (4.184)(R 4.181, F 4.186)]  [G loss: -93.469] \n",
                        "3631 [D loss: (4.187)(R 4.185, F 4.190)]  [G loss: -93.488] \n",
                        "3632 [D loss: (4.190)(R 4.188, F 4.192)]  [G loss: -93.503] \n",
                        "3633 [D loss: (4.192)(R 4.190, F 4.194)]  [G loss: -93.515] \n",
                        "3634 [D loss: (4.193)(R 4.191, F 4.195)]  [G loss: -93.523] \n",
                        "3635 [D loss: (4.193)(R 4.191, F 4.195)]  [G loss: -93.527] \n",
                        "3636 [D loss: (4.192)(R 4.191, F 4.194)]  [G loss: -93.529] \n",
                        "3637 [D loss: (4.191)(R 4.190, F 4.193)]  [G loss: -93.530] \n",
                        "3638 [D loss: (4.190)(R 4.189, F 4.192)]  [G loss: -93.529] \n",
                        "3639 [D loss: (4.189)(R 4.188, F 4.190)]  [G loss: -93.529] \n",
                        "3640 [D loss: (4.188)(R 4.186, F 4.189)]  [G loss: -93.529] \n",
                        "3641 [D loss: (4.187)(R 4.186, F 4.188)]  [G loss: -93.530] \n",
                        "3642 [D loss: (4.186)(R 4.185, F 4.188)]  [G loss: -93.533] \n",
                        "3643 [D loss: (4.187)(R 4.185, F 4.188)]  [G loss: -93.537] \n",
                        "3644 [D loss: (4.187)(R 4.185, F 4.189)]  [G loss: -93.543] \n",
                        "3645 [D loss: (4.187)(R 4.185, F 4.189)]  [G loss: -93.551] \n",
                        "3646 [D loss: (4.187)(R 4.185, F 4.189)]  [G loss: -93.562] \n",
                        "3647 [D loss: (4.187)(R 4.185, F 4.189)]  [G loss: -93.575] \n",
                        "3648 [D loss: (4.186)(R 4.184, F 4.188)]  [G loss: -93.590] \n",
                        "3649 [D loss: (4.186)(R 4.184, F 4.189)]  [G loss: -93.609] \n",
                        "3650 [D loss: (4.187)(R 4.185, F 4.189)]  [G loss: -93.630] \n",
                        "3651 [D loss: (4.188)(R 4.185, F 4.190)]  [G loss: -93.654] \n",
                        "3652 [D loss: (4.189)(R 4.187, F 4.192)]  [G loss: -93.679] \n",
                        "3653 [D loss: (4.192)(R 4.190, F 4.195)]  [G loss: -93.706] \n",
                        "3654 [D loss: (4.196)(R 4.194, F 4.199)]  [G loss: -93.733] \n",
                        "3655 [D loss: (4.201)(R 4.198, F 4.203)]  [G loss: -93.757] \n",
                        "3656 [D loss: (4.205)(R 4.202, F 4.207)]  [G loss: -93.778] \n",
                        "3657 [D loss: (4.208)(R 4.205, F 4.210)]  [G loss: -93.795] \n",
                        "3658 [D loss: (4.210)(R 4.208, F 4.212)]  [G loss: -93.809] \n",
                        "3659 [D loss: (4.211)(R 4.209, F 4.213)]  [G loss: -93.819] \n",
                        "3660 [D loss: (4.212)(R 4.210, F 4.214)]  [G loss: -93.827] \n",
                        "3661 [D loss: (4.212)(R 4.210, F 4.214)]  [G loss: -93.832] \n",
                        "3662 [D loss: (4.211)(R 4.210, F 4.213)]  [G loss: -93.834] \n",
                        "3663 [D loss: (4.210)(R 4.209, F 4.212)]  [G loss: -93.834] \n",
                        "3664 [D loss: (4.208)(R 4.207, F 4.210)]  [G loss: -93.833] \n",
                        "3665 [D loss: (4.207)(R 4.206, F 4.208)]  [G loss: -93.832] \n",
                        "3666 [D loss: (4.206)(R 4.204, F 4.207)]  [G loss: -93.831] \n",
                        "3667 [D loss: (4.204)(R 4.203, F 4.205)]  [G loss: -93.830] \n",
                        "3668 [D loss: (4.203)(R 4.202, F 4.205)]  [G loss: -93.832] \n",
                        "3669 [D loss: (4.203)(R 4.202, F 4.205)]  [G loss: -93.835] \n",
                        "3670 [D loss: (4.204)(R 4.202, F 4.205)]  [G loss: -93.840] \n",
                        "3671 [D loss: (4.205)(R 4.203, F 4.206)]  [G loss: -93.847] \n",
                        "3672 [D loss: (4.206)(R 4.204, F 4.207)]  [G loss: -93.854] \n",
                        "3673 [D loss: (4.206)(R 4.204, F 4.208)]  [G loss: -93.864] \n",
                        "3674 [D loss: (4.206)(R 4.204, F 4.208)]  [G loss: -93.876] \n",
                        "3675 [D loss: (4.206)(R 4.204, F 4.208)]  [G loss: -93.890] \n",
                        "3676 [D loss: (4.205)(R 4.203, F 4.207)]  [G loss: -93.905] \n",
                        "3677 [D loss: (4.204)(R 4.202, F 4.206)]  [G loss: -93.922] \n",
                        "3678 [D loss: (4.204)(R 4.201, F 4.206)]  [G loss: -93.943] \n",
                        "3679 [D loss: (4.205)(R 4.202, F 4.208)]  [G loss: -93.967] \n",
                        "3680 [D loss: (4.207)(R 4.205, F 4.210)]  [G loss: -93.992] \n",
                        "3681 [D loss: (4.210)(R 4.207, F 4.213)]  [G loss: -94.018] \n",
                        "3682 [D loss: (4.213)(R 4.211, F 4.216)]  [G loss: -94.045] \n",
                        "3683 [D loss: (4.217)(R 4.215, F 4.220)]  [G loss: -94.072] \n",
                        "3684 [D loss: (4.221)(R 4.218, F 4.224)]  [G loss: -94.097] \n",
                        "3685 [D loss: (4.225)(R 4.222, F 4.227)]  [G loss: -94.121] \n",
                        "3686 [D loss: (4.228)(R 4.226, F 4.231)]  [G loss: -94.142] \n",
                        "3687 [D loss: (4.231)(R 4.229, F 4.233)]  [G loss: -94.159] \n",
                        "3688 [D loss: (4.233)(R 4.231, F 4.236)]  [G loss: -94.173] \n",
                        "3689 [D loss: (4.235)(R 4.233, F 4.237)]  [G loss: -94.183] \n",
                        "3690 [D loss: (4.235)(R 4.234, F 4.237)]  [G loss: -94.190] \n",
                        "3691 [D loss: (4.236)(R 4.234, F 4.237)]  [G loss: -94.195] \n",
                        "3692 [D loss: (4.235)(R 4.233, F 4.237)]  [G loss: -94.197] \n",
                        "3693 [D loss: (4.234)(R 4.233, F 4.235)]  [G loss: -94.197] \n",
                        "3694 [D loss: (4.233)(R 4.231, F 4.234)]  [G loss: -94.195] \n",
                        "3695 [D loss: (4.231)(R 4.230, F 4.232)]  [G loss: -94.193] \n",
                        "3696 [D loss: (4.229)(R 4.228, F 4.230)]  [G loss: -94.190] \n",
                        "3697 [D loss: (4.228)(R 4.226, F 4.229)]  [G loss: -94.188] \n",
                        "3698 [D loss: (4.227)(R 4.225, F 4.228)]  [G loss: -94.186] \n",
                        "3699 [D loss: (4.226)(R 4.224, F 4.227)]  [G loss: -94.186] \n",
                        "3700 [D loss: (4.225)(R 4.224, F 4.226)]  [G loss: -94.187] \n",
                        "3701 [D loss: (4.225)(R 4.224, F 4.227)]  [G loss: -94.190] \n",
                        "3702 [D loss: (4.225)(R 4.224, F 4.227)]  [G loss: -94.195] \n",
                        "3703 [D loss: (4.226)(R 4.224, F 4.227)]  [G loss: -94.202] \n",
                        "3704 [D loss: (4.226)(R 4.224, F 4.228)]  [G loss: -94.212] \n",
                        "3705 [D loss: (4.226)(R 4.224, F 4.227)]  [G loss: -94.225] \n",
                        "3706 [D loss: (4.224)(R 4.222, F 4.227)]  [G loss: -94.240] \n",
                        "3707 [D loss: (4.223)(R 4.221, F 4.226)]  [G loss: -94.259] \n",
                        "3708 [D loss: (4.222)(R 4.220, F 4.225)]  [G loss: -94.281] \n",
                        "3709 [D loss: (4.222)(R 4.219, F 4.224)]  [G loss: -94.307] \n",
                        "3710 [D loss: (4.223)(R 4.220, F 4.226)]  [G loss: -94.336] \n",
                        "3711 [D loss: (4.224)(R 4.222, F 4.227)]  [G loss: -94.368] \n",
                        "3712 [D loss: (4.228)(R 4.225, F 4.231)]  [G loss: -94.402] \n",
                        "3713 [D loss: (4.232)(R 4.229, F 4.235)]  [G loss: -94.435] \n",
                        "3714 [D loss: (4.237)(R 4.234, F 4.240)]  [G loss: -94.468] \n",
                        "3715 [D loss: (4.242)(R 4.239, F 4.245)]  [G loss: -94.499] \n",
                        "3716 [D loss: (4.246)(R 4.244, F 4.249)]  [G loss: -94.529] \n",
                        "3717 [D loss: (4.251)(R 4.249, F 4.254)]  [G loss: -94.555] \n",
                        "3718 [D loss: (4.256)(R 4.253, F 4.258)]  [G loss: -94.577] \n",
                        "3719 [D loss: (4.259)(R 4.256, F 4.261)]  [G loss: -94.596] \n",
                        "3720 [D loss: (4.261)(R 4.259, F 4.264)]  [G loss: -94.610] \n",
                        "3721 [D loss: (4.263)(R 4.261, F 4.265)]  [G loss: -94.619] \n",
                        "3722 [D loss: (4.264)(R 4.262, F 4.265)]  [G loss: -94.626] \n",
                        "3723 [D loss: (4.263)(R 4.262, F 4.265)]  [G loss: -94.629] \n",
                        "3724 [D loss: (4.263)(R 4.261, F 4.264)]  [G loss: -94.630] \n",
                        "3725 [D loss: (4.261)(R 4.260, F 4.262)]  [G loss: -94.627] \n",
                        "3726 [D loss: (4.259)(R 4.258, F 4.260)]  [G loss: -94.622] \n",
                        "3727 [D loss: (4.257)(R 4.256, F 4.258)]  [G loss: -94.617] \n",
                        "3728 [D loss: (4.255)(R 4.254, F 4.256)]  [G loss: -94.611] \n",
                        "3729 [D loss: (4.252)(R 4.251, F 4.253)]  [G loss: -94.606] \n",
                        "3730 [D loss: (4.250)(R 4.249, F 4.251)]  [G loss: -94.601] \n",
                        "3731 [D loss: (4.249)(R 4.248, F 4.250)]  [G loss: -94.599] \n",
                        "3732 [D loss: (4.248)(R 4.247, F 4.249)]  [G loss: -94.597] \n",
                        "3733 [D loss: (4.247)(R 4.246, F 4.249)]  [G loss: -94.596] \n",
                        "3734 [D loss: (4.247)(R 4.246, F 4.249)]  [G loss: -94.597] \n",
                        "3735 [D loss: (4.248)(R 4.246, F 4.249)]  [G loss: -94.599] \n",
                        "3736 [D loss: (4.248)(R 4.246, F 4.249)]  [G loss: -94.602] \n",
                        "3737 [D loss: (4.248)(R 4.247, F 4.250)]  [G loss: -94.608] \n",
                        "3738 [D loss: (4.248)(R 4.247, F 4.250)]  [G loss: -94.615] \n",
                        "3739 [D loss: (4.248)(R 4.246, F 4.250)]  [G loss: -94.624] \n",
                        "3740 [D loss: (4.248)(R 4.246, F 4.250)]  [G loss: -94.634] \n",
                        "3741 [D loss: (4.247)(R 4.245, F 4.249)]  [G loss: -94.647] \n",
                        "3742 [D loss: (4.247)(R 4.245, F 4.249)]  [G loss: -94.663] \n",
                        "3743 [D loss: (4.247)(R 4.244, F 4.249)]  [G loss: -94.681] \n",
                        "3744 [D loss: (4.247)(R 4.245, F 4.249)]  [G loss: -94.703] \n",
                        "3745 [D loss: (4.248)(R 4.245, F 4.250)]  [G loss: -94.727] \n",
                        "3746 [D loss: (4.250)(R 4.247, F 4.253)]  [G loss: -94.753] \n",
                        "3747 [D loss: (4.253)(R 4.250, F 4.256)]  [G loss: -94.780] \n",
                        "3748 [D loss: (4.256)(R 4.254, F 4.259)]  [G loss: -94.808] \n",
                        "3749 [D loss: (4.260)(R 4.257, F 4.263)]  [G loss: -94.835] \n",
                        "3750 [D loss: (4.264)(R 4.262, F 4.267)]  [G loss: -94.860] \n",
                        "3751 [D loss: (4.268)(R 4.265, F 4.270)]  [G loss: -94.882] \n",
                        "3752 [D loss: (4.271)(R 4.269, F 4.274)]  [G loss: -94.902] \n",
                        "3753 [D loss: (4.274)(R 4.272, F 4.276)]  [G loss: -94.917] \n",
                        "3754 [D loss: (4.276)(R 4.274, F 4.278)]  [G loss: -94.929] \n",
                        "3755 [D loss: (4.277)(R 4.275, F 4.279)]  [G loss: -94.938] \n",
                        "3756 [D loss: (4.277)(R 4.276, F 4.279)]  [G loss: -94.943] \n",
                        "3757 [D loss: (4.277)(R 4.276, F 4.279)]  [G loss: -94.947] \n",
                        "3758 [D loss: (4.276)(R 4.275, F 4.278)]  [G loss: -94.948] \n",
                        "3759 [D loss: (4.275)(R 4.274, F 4.277)]  [G loss: -94.948] \n",
                        "3760 [D loss: (4.274)(R 4.273, F 4.276)]  [G loss: -94.948] \n",
                        "3761 [D loss: (4.273)(R 4.272, F 4.274)]  [G loss: -94.947] \n",
                        "3762 [D loss: (4.272)(R 4.271, F 4.273)]  [G loss: -94.947] \n",
                        "3763 [D loss: (4.271)(R 4.270, F 4.273)]  [G loss: -94.948] \n",
                        "3764 [D loss: (4.271)(R 4.270, F 4.272)]  [G loss: -94.949] \n",
                        "3765 [D loss: (4.271)(R 4.269, F 4.272)]  [G loss: -94.952] \n",
                        "3766 [D loss: (4.271)(R 4.269, F 4.272)]  [G loss: -94.955] \n",
                        "3767 [D loss: (4.271)(R 4.270, F 4.273)]  [G loss: -94.960] \n",
                        "3768 [D loss: (4.271)(R 4.270, F 4.273)]  [G loss: -94.967] \n",
                        "3769 [D loss: (4.272)(R 4.270, F 4.273)]  [G loss: -94.976] \n",
                        "3770 [D loss: (4.272)(R 4.270, F 4.274)]  [G loss: -94.985] \n",
                        "3771 [D loss: (4.272)(R 4.270, F 4.274)]  [G loss: -94.997] \n",
                        "3772 [D loss: (4.271)(R 4.269, F 4.273)]  [G loss: -95.011] \n",
                        "3773 [D loss: (4.271)(R 4.269, F 4.273)]  [G loss: -95.027] \n",
                        "3774 [D loss: (4.271)(R 4.268, F 4.273)]  [G loss: -95.046] \n",
                        "3775 [D loss: (4.271)(R 4.269, F 4.274)]  [G loss: -95.067] \n",
                        "3776 [D loss: (4.272)(R 4.270, F 4.275)]  [G loss: -95.091] \n",
                        "3777 [D loss: (4.274)(R 4.272, F 4.277)]  [G loss: -95.115] \n",
                        "3778 [D loss: (4.276)(R 4.273, F 4.278)]  [G loss: -95.140] \n",
                        "3779 [D loss: (4.278)(R 4.276, F 4.281)]  [G loss: -95.166] \n",
                        "3780 [D loss: (4.281)(R 4.279, F 4.284)]  [G loss: -95.192] \n",
                        "3781 [D loss: (4.284)(R 4.282, F 4.287)]  [G loss: -95.216] \n",
                        "3782 [D loss: (4.287)(R 4.285, F 4.290)]  [G loss: -95.239] \n",
                        "3783 [D loss: (4.290)(R 4.288, F 4.293)]  [G loss: -95.259] \n",
                        "3784 [D loss: (4.293)(R 4.291, F 4.295)]  [G loss: -95.277] \n",
                        "3785 [D loss: (4.295)(R 4.293, F 4.297)]  [G loss: -95.290] \n",
                        "3786 [D loss: (4.296)(R 4.294, F 4.298)]  [G loss: -95.302] \n",
                        "3787 [D loss: (4.297)(R 4.295, F 4.299)]  [G loss: -95.310] \n",
                        "3788 [D loss: (4.297)(R 4.296, F 4.299)]  [G loss: -95.317] \n",
                        "3789 [D loss: (4.297)(R 4.296, F 4.299)]  [G loss: -95.321] \n",
                        "3790 [D loss: (4.297)(R 4.295, F 4.298)]  [G loss: -95.323] \n",
                        "3791 [D loss: (4.296)(R 4.294, F 4.297)]  [G loss: -95.325] \n",
                        "3792 [D loss: (4.295)(R 4.294, F 4.297)]  [G loss: -95.326] \n",
                        "3793 [D loss: (4.294)(R 4.293, F 4.296)]  [G loss: -95.327] \n",
                        "3794 [D loss: (4.293)(R 4.292, F 4.295)]  [G loss: -95.329] \n",
                        "3795 [D loss: (4.292)(R 4.291, F 4.294)]  [G loss: -95.331] \n",
                        "3796 [D loss: (4.291)(R 4.290, F 4.293)]  [G loss: -95.334] \n",
                        "3797 [D loss: (4.290)(R 4.289, F 4.292)]  [G loss: -95.338] \n",
                        "3798 [D loss: (4.290)(R 4.288, F 4.291)]  [G loss: -95.345] \n",
                        "3799 [D loss: (4.290)(R 4.288, F 4.291)]  [G loss: -95.353] \n",
                        "3800 [D loss: (4.290)(R 4.288, F 4.291)]  [G loss: -95.364] \n",
                        "3801 [D loss: (4.290)(R 4.288, F 4.292)]  [G loss: -95.376] \n",
                        "3802 [D loss: (4.291)(R 4.289, F 4.293)]  [G loss: -95.390] \n",
                        "3803 [D loss: (4.292)(R 4.290, F 4.294)]  [G loss: -95.406] \n",
                        "3804 [D loss: (4.293)(R 4.291, F 4.295)]  [G loss: -95.423] \n",
                        "3805 [D loss: (4.294)(R 4.292, F 4.296)]  [G loss: -95.440] \n",
                        "3806 [D loss: (4.295)(R 4.293, F 4.298)]  [G loss: -95.457] \n",
                        "3807 [D loss: (4.297)(R 4.295, F 4.299)]  [G loss: -95.475] \n",
                        "3808 [D loss: (4.299)(R 4.297, F 4.301)]  [G loss: -95.493] \n",
                        "3809 [D loss: (4.301)(R 4.298, F 4.303)]  [G loss: -95.511] \n",
                        "3810 [D loss: (4.302)(R 4.300, F 4.305)]  [G loss: -95.528] \n",
                        "3811 [D loss: (4.304)(R 4.302, F 4.306)]  [G loss: -95.543] \n",
                        "3812 [D loss: (4.305)(R 4.303, F 4.307)]  [G loss: -95.557] \n",
                        "3813 [D loss: (4.306)(R 4.304, F 4.308)]  [G loss: -95.570] \n",
                        "3814 [D loss: (4.307)(R 4.305, F 4.309)]  [G loss: -95.581] \n",
                        "3815 [D loss: (4.307)(R 4.305, F 4.309)]  [G loss: -95.592] \n",
                        "3816 [D loss: (4.307)(R 4.305, F 4.309)]  [G loss: -95.601] \n",
                        "3817 [D loss: (4.307)(R 4.305, F 4.309)]  [G loss: -95.609] \n",
                        "3818 [D loss: (4.306)(R 4.305, F 4.308)]  [G loss: -95.616] \n",
                        "3819 [D loss: (4.306)(R 4.304, F 4.308)]  [G loss: -95.623] \n",
                        "3820 [D loss: (4.305)(R 4.304, F 4.307)]  [G loss: -95.631] \n",
                        "3821 [D loss: (4.305)(R 4.303, F 4.307)]  [G loss: -95.638] \n",
                        "3822 [D loss: (4.305)(R 4.303, F 4.307)]  [G loss: -95.647] \n",
                        "3823 [D loss: (4.305)(R 4.303, F 4.307)]  [G loss: -95.657] \n",
                        "3824 [D loss: (4.305)(R 4.304, F 4.307)]  [G loss: -95.668] \n",
                        "3825 [D loss: (4.306)(R 4.304, F 4.308)]  [G loss: -95.680] \n",
                        "3826 [D loss: (4.307)(R 4.305, F 4.308)]  [G loss: -95.692] \n",
                        "3827 [D loss: (4.307)(R 4.305, F 4.309)]  [G loss: -95.706] \n",
                        "3828 [D loss: (4.308)(R 4.306, F 4.310)]  [G loss: -95.720] \n",
                        "3829 [D loss: (4.308)(R 4.306, F 4.310)]  [G loss: -95.735] \n",
                        "3830 [D loss: (4.309)(R 4.307, F 4.311)]  [G loss: -95.751] \n",
                        "3831 [D loss: (4.310)(R 4.308, F 4.312)]  [G loss: -95.768] \n",
                        "3832 [D loss: (4.311)(R 4.309, F 4.314)]  [G loss: -95.787] \n",
                        "3833 [D loss: (4.313)(R 4.311, F 4.315)]  [G loss: -95.806] \n",
                        "3834 [D loss: (4.315)(R 4.313, F 4.317)]  [G loss: -95.825] \n",
                        "3835 [D loss: (4.317)(R 4.315, F 4.319)]  [G loss: -95.843] \n",
                        "3836 [D loss: (4.319)(R 4.317, F 4.321)]  [G loss: -95.858] \n",
                        "3837 [D loss: (4.321)(R 4.319, F 4.323)]  [G loss: -95.872] \n",
                        "3838 [D loss: (4.322)(R 4.320, F 4.324)]  [G loss: -95.884] \n",
                        "3839 [D loss: (4.323)(R 4.321, F 4.325)]  [G loss: -95.894] \n",
                        "3840 [D loss: (4.323)(R 4.321, F 4.325)]  [G loss: -95.904] \n",
                        "3841 [D loss: (4.324)(R 4.322, F 4.325)]  [G loss: -95.911] \n",
                        "3842 [D loss: (4.324)(R 4.322, F 4.325)]  [G loss: -95.915] \n",
                        "3843 [D loss: (4.324)(R 4.322, F 4.325)]  [G loss: -95.922] \n",
                        "3844 [D loss: (4.324)(R 4.322, F 4.325)]  [G loss: -95.929] \n",
                        "3845 [D loss: (4.324)(R 4.322, F 4.326)]  [G loss: -95.934] \n",
                        "3846 [D loss: (4.324)(R 4.323, F 4.326)]  [G loss: -95.940] \n",
                        "3847 [D loss: (4.325)(R 4.323, F 4.326)]  [G loss: -95.948] \n",
                        "3848 [D loss: (4.325)(R 4.324, F 4.327)]  [G loss: -95.956] \n",
                        "3849 [D loss: (4.325)(R 4.324, F 4.327)]  [G loss: -95.964] \n",
                        "3850 [D loss: (4.325)(R 4.324, F 4.327)]  [G loss: -95.974] \n",
                        "3851 [D loss: (4.325)(R 4.324, F 4.327)]  [G loss: -95.984] \n",
                        "3852 [D loss: (4.325)(R 4.323, F 4.327)]  [G loss: -95.996] \n",
                        "3853 [D loss: (4.325)(R 4.323, F 4.327)]  [G loss: -96.008] \n",
                        "3854 [D loss: (4.324)(R 4.322, F 4.326)]  [G loss: -96.023] \n",
                        "3855 [D loss: (4.324)(R 4.322, F 4.326)]  [G loss: -96.040] \n",
                        "3856 [D loss: (4.324)(R 4.322, F 4.326)]  [G loss: -96.059] \n",
                        "3857 [D loss: (4.325)(R 4.323, F 4.327)]  [G loss: -96.079] \n",
                        "3858 [D loss: (4.326)(R 4.324, F 4.329)]  [G loss: -96.100] \n",
                        "3859 [D loss: (4.328)(R 4.326, F 4.331)]  [G loss: -96.121] \n",
                        "3860 [D loss: (4.330)(R 4.328, F 4.332)]  [G loss: -96.141] \n",
                        "3861 [D loss: (4.332)(R 4.330, F 4.334)]  [G loss: -96.160] \n",
                        "3862 [D loss: (4.334)(R 4.332, F 4.336)]  [G loss: -96.178] \n",
                        "3863 [D loss: (4.336)(R 4.334, F 4.338)]  [G loss: -96.193] \n",
                        "3864 [D loss: (4.337)(R 4.335, F 4.339)]  [G loss: -96.207] \n",
                        "3865 [D loss: (4.338)(R 4.336, F 4.340)]  [G loss: -96.219] \n",
                        "3866 [D loss: (4.339)(R 4.337, F 4.341)]  [G loss: -96.231] \n",
                        "3867 [D loss: (4.340)(R 4.338, F 4.342)]  [G loss: -96.242] \n",
                        "3868 [D loss: (4.341)(R 4.339, F 4.342)]  [G loss: -96.252] \n",
                        "3869 [D loss: (4.341)(R 4.339, F 4.343)]  [G loss: -96.261] \n",
                        "3870 [D loss: (4.341)(R 4.339, F 4.343)]  [G loss: -96.269] \n",
                        "3871 [D loss: (4.341)(R 4.340, F 4.343)]  [G loss: -96.278] \n",
                        "3872 [D loss: (4.341)(R 4.340, F 4.343)]  [G loss: -96.287] \n",
                        "3873 [D loss: (4.341)(R 4.339, F 4.343)]  [G loss: -96.296] \n",
                        "3874 [D loss: (4.341)(R 4.339, F 4.343)]  [G loss: -96.305] \n",
                        "3875 [D loss: (4.341)(R 4.339, F 4.343)]  [G loss: -96.316] \n",
                        "3876 [D loss: (4.341)(R 4.339, F 4.343)]  [G loss: -96.327] \n",
                        "3877 [D loss: (4.341)(R 4.339, F 4.343)]  [G loss: -96.340] \n",
                        "3878 [D loss: (4.342)(R 4.340, F 4.344)]  [G loss: -96.353] \n",
                        "3879 [D loss: (4.343)(R 4.341, F 4.345)]  [G loss: -96.368] \n",
                        "3880 [D loss: (4.344)(R 4.342, F 4.346)]  [G loss: -96.382] \n",
                        "3881 [D loss: (4.345)(R 4.343, F 4.347)]  [G loss: -96.396] \n",
                        "3882 [D loss: (4.346)(R 4.344, F 4.348)]  [G loss: -96.410] \n",
                        "3883 [D loss: (4.347)(R 4.345, F 4.349)]  [G loss: -96.424] \n",
                        "3884 [D loss: (4.348)(R 4.346, F 4.350)]  [G loss: -96.437] \n",
                        "3885 [D loss: (4.349)(R 4.347, F 4.351)]  [G loss: -96.450] \n",
                        "3886 [D loss: (4.350)(R 4.348, F 4.352)]  [G loss: -96.463] \n",
                        "3887 [D loss: (4.351)(R 4.349, F 4.353)]  [G loss: -96.476] \n",
                        "3888 [D loss: (4.352)(R 4.350, F 4.354)]  [G loss: -96.489] \n",
                        "3889 [D loss: (4.352)(R 4.350, F 4.354)]  [G loss: -96.502] \n",
                        "3890 [D loss: (4.353)(R 4.351, F 4.355)]  [G loss: -96.516] \n",
                        "3891 [D loss: (4.353)(R 4.351, F 4.355)]  [G loss: -96.530] \n",
                        "3892 [D loss: (4.354)(R 4.352, F 4.356)]  [G loss: -96.545] \n",
                        "3893 [D loss: (4.354)(R 4.352, F 4.356)]  [G loss: -96.560] \n",
                        "3894 [D loss: (4.355)(R 4.353, F 4.357)]  [G loss: -96.575] \n",
                        "3895 [D loss: (4.356)(R 4.354, F 4.358)]  [G loss: -96.589] \n",
                        "3896 [D loss: (4.357)(R 4.354, F 4.359)]  [G loss: -96.603] \n",
                        "3897 [D loss: (4.357)(R 4.355, F 4.359)]  [G loss: -96.617] \n",
                        "3898 [D loss: (4.358)(R 4.356, F 4.360)]  [G loss: -96.630] \n",
                        "3899 [D loss: (4.358)(R 4.356, F 4.360)]  [G loss: -96.644] \n",
                        "3900 [D loss: (4.359)(R 4.357, F 4.361)]  [G loss: -96.657] \n",
                        "3901 [D loss: (4.360)(R 4.358, F 4.362)]  [G loss: -96.671] \n",
                        "3902 [D loss: (4.360)(R 4.358, F 4.362)]  [G loss: -96.685] \n",
                        "3903 [D loss: (4.361)(R 4.359, F 4.363)]  [G loss: -96.699] \n",
                        "3904 [D loss: (4.362)(R 4.360, F 4.364)]  [G loss: -96.712] \n",
                        "3905 [D loss: (4.363)(R 4.361, F 4.365)]  [G loss: -96.726] \n",
                        "3906 [D loss: (4.364)(R 4.362, F 4.366)]  [G loss: -96.737] \n",
                        "3907 [D loss: (4.364)(R 4.362, F 4.366)]  [G loss: -96.750] \n",
                        "3908 [D loss: (4.365)(R 4.363, F 4.367)]  [G loss: -96.761] \n",
                        "3909 [D loss: (4.366)(R 4.364, F 4.368)]  [G loss: -96.771] \n",
                        "3910 [D loss: (4.366)(R 4.365, F 4.368)]  [G loss: -96.780] \n",
                        "3911 [D loss: (4.367)(R 4.365, F 4.369)]  [G loss: -96.790] \n",
                        "3912 [D loss: (4.368)(R 4.366, F 4.370)]  [G loss: -96.800] \n",
                        "3913 [D loss: (4.368)(R 4.367, F 4.370)]  [G loss: -96.808] \n",
                        "3914 [D loss: (4.369)(R 4.367, F 4.371)]  [G loss: -96.817] \n",
                        "3915 [D loss: (4.370)(R 4.368, F 4.371)]  [G loss: -96.827] \n",
                        "3916 [D loss: (4.370)(R 4.368, F 4.372)]  [G loss: -96.836] \n",
                        "3917 [D loss: (4.371)(R 4.369, F 4.373)]  [G loss: -96.846] \n",
                        "3918 [D loss: (4.371)(R 4.370, F 4.373)]  [G loss: -96.856] \n",
                        "3919 [D loss: (4.372)(R 4.370, F 4.374)]  [G loss: -96.867] \n",
                        "3920 [D loss: (4.372)(R 4.370, F 4.374)]  [G loss: -96.879] \n",
                        "3921 [D loss: (4.373)(R 4.371, F 4.375)]  [G loss: -96.892] \n",
                        "3922 [D loss: (4.373)(R 4.371, F 4.375)]  [G loss: -96.905] \n",
                        "3923 [D loss: (4.374)(R 4.372, F 4.376)]  [G loss: -96.919] \n",
                        "3924 [D loss: (4.375)(R 4.373, F 4.377)]  [G loss: -96.935] \n",
                        "3925 [D loss: (4.376)(R 4.374, F 4.378)]  [G loss: -96.950] \n",
                        "3926 [D loss: (4.377)(R 4.375, F 4.379)]  [G loss: -96.965] \n",
                        "3927 [D loss: (4.378)(R 4.376, F 4.380)]  [G loss: -96.978] \n",
                        "3928 [D loss: (4.379)(R 4.377, F 4.381)]  [G loss: -96.991] \n",
                        "3929 [D loss: (4.380)(R 4.378, F 4.382)]  [G loss: -97.003] \n",
                        "3930 [D loss: (4.380)(R 4.379, F 4.382)]  [G loss: -97.015] \n",
                        "3931 [D loss: (4.381)(R 4.379, F 4.383)]  [G loss: -97.025] \n",
                        "3932 [D loss: (4.381)(R 4.379, F 4.383)]  [G loss: -97.035] \n",
                        "3933 [D loss: (4.381)(R 4.379, F 4.383)]  [G loss: -97.044] \n",
                        "3934 [D loss: (4.381)(R 4.379, F 4.383)]  [G loss: -97.053] \n",
                        "3935 [D loss: (4.381)(R 4.379, F 4.383)]  [G loss: -97.062] \n",
                        "3936 [D loss: (4.381)(R 4.379, F 4.383)]  [G loss: -97.071] \n",
                        "3937 [D loss: (4.381)(R 4.379, F 4.383)]  [G loss: -97.081] \n",
                        "3938 [D loss: (4.381)(R 4.379, F 4.383)]  [G loss: -97.091] \n",
                        "3939 [D loss: (4.382)(R 4.380, F 4.383)]  [G loss: -97.103] \n",
                        "3940 [D loss: (4.382)(R 4.380, F 4.384)]  [G loss: -97.115] \n",
                        "3941 [D loss: (4.383)(R 4.381, F 4.385)]  [G loss: -97.127] \n",
                        "3942 [D loss: (4.383)(R 4.381, F 4.385)]  [G loss: -97.140] \n",
                        "3943 [D loss: (4.384)(R 4.382, F 4.386)]  [G loss: -97.153] \n",
                        "3944 [D loss: (4.385)(R 4.383, F 4.387)]  [G loss: -97.166] \n",
                        "3945 [D loss: (4.385)(R 4.383, F 4.387)]  [G loss: -97.181] \n",
                        "3946 [D loss: (4.386)(R 4.384, F 4.388)]  [G loss: -97.195] \n",
                        "3947 [D loss: (4.386)(R 4.384, F 4.389)]  [G loss: -97.210] \n",
                        "3948 [D loss: (4.387)(R 4.385, F 4.389)]  [G loss: -97.225] \n",
                        "3949 [D loss: (4.388)(R 4.386, F 4.390)]  [G loss: -97.241] \n",
                        "3950 [D loss: (4.389)(R 4.387, F 4.391)]  [G loss: -97.256] \n",
                        "3951 [D loss: (4.390)(R 4.388, F 4.392)]  [G loss: -97.271] \n",
                        "3952 [D loss: (4.391)(R 4.389, F 4.393)]  [G loss: -97.287] \n",
                        "3953 [D loss: (4.392)(R 4.390, F 4.394)]  [G loss: -97.302] \n",
                        "3954 [D loss: (4.393)(R 4.391, F 4.395)]  [G loss: -97.318] \n",
                        "3955 [D loss: (4.394)(R 4.392, F 4.396)]  [G loss: -97.332] \n",
                        "3956 [D loss: (4.395)(R 4.393, F 4.397)]  [G loss: -97.347] \n",
                        "3957 [D loss: (4.396)(R 4.394, F 4.398)]  [G loss: -97.361] \n",
                        "3958 [D loss: (4.396)(R 4.394, F 4.398)]  [G loss: -97.376] \n",
                        "3959 [D loss: (4.397)(R 4.395, F 4.399)]  [G loss: -97.388] \n",
                        "3960 [D loss: (4.398)(R 4.396, F 4.400)]  [G loss: -97.401] \n",
                        "3961 [D loss: (4.399)(R 4.397, F 4.400)]  [G loss: -97.413] \n",
                        "3962 [D loss: (4.399)(R 4.397, F 4.401)]  [G loss: -97.425] \n",
                        "3963 [D loss: (4.400)(R 4.398, F 4.402)]  [G loss: -97.436] \n",
                        "3964 [D loss: (4.400)(R 4.398, F 4.402)]  [G loss: -97.448] \n",
                        "3965 [D loss: (4.401)(R 4.399, F 4.403)]  [G loss: -97.458] \n",
                        "3966 [D loss: (4.401)(R 4.400, F 4.403)]  [G loss: -97.469] \n",
                        "3967 [D loss: (4.402)(R 4.400, F 4.404)]  [G loss: -97.481] \n",
                        "3968 [D loss: (4.403)(R 4.401, F 4.405)]  [G loss: -97.492] \n",
                        "3969 [D loss: (4.404)(R 4.402, F 4.405)]  [G loss: -97.503] \n",
                        "3970 [D loss: (4.404)(R 4.402, F 4.406)]  [G loss: -97.514] \n",
                        "3971 [D loss: (4.404)(R 4.402, F 4.406)]  [G loss: -97.525] \n",
                        "3972 [D loss: (4.405)(R 4.403, F 4.407)]  [G loss: -97.538] \n",
                        "3973 [D loss: (4.405)(R 4.403, F 4.407)]  [G loss: -97.550] \n",
                        "3974 [D loss: (4.406)(R 4.404, F 4.407)]  [G loss: -97.562] \n",
                        "3975 [D loss: (4.406)(R 4.404, F 4.408)]  [G loss: -97.574] \n",
                        "3976 [D loss: (4.406)(R 4.405, F 4.408)]  [G loss: -97.587] \n",
                        "3977 [D loss: (4.407)(R 4.405, F 4.409)]  [G loss: -97.601] \n",
                        "3978 [D loss: (4.408)(R 4.406, F 4.410)]  [G loss: -97.614] \n",
                        "3979 [D loss: (4.409)(R 4.407, F 4.411)]  [G loss: -97.626] \n",
                        "3980 [D loss: (4.410)(R 4.408, F 4.412)]  [G loss: -97.638] \n",
                        "3981 [D loss: (4.411)(R 4.409, F 4.413)]  [G loss: -97.649] \n",
                        "3982 [D loss: (4.412)(R 4.410, F 4.414)]  [G loss: -97.658] \n",
                        "3983 [D loss: (4.412)(R 4.410, F 4.414)]  [G loss: -97.668] \n",
                        "3984 [D loss: (4.412)(R 4.411, F 4.414)]  [G loss: -97.677] \n",
                        "3985 [D loss: (4.412)(R 4.411, F 4.414)]  [G loss: -97.685] \n",
                        "3986 [D loss: (4.413)(R 4.411, F 4.414)]  [G loss: -97.693] \n",
                        "3987 [D loss: (4.413)(R 4.411, F 4.414)]  [G loss: -97.700] \n",
                        "3988 [D loss: (4.413)(R 4.411, F 4.415)]  [G loss: -97.708] \n",
                        "3989 [D loss: (4.413)(R 4.411, F 4.415)]  [G loss: -97.715] \n",
                        "3990 [D loss: (4.413)(R 4.411, F 4.415)]  [G loss: -97.723] \n",
                        "3991 [D loss: (4.413)(R 4.411, F 4.415)]  [G loss: -97.731] \n",
                        "3992 [D loss: (4.413)(R 4.412, F 4.415)]  [G loss: -97.739] \n",
                        "3993 [D loss: (4.414)(R 4.412, F 4.415)]  [G loss: -97.748] \n",
                        "3994 [D loss: (4.414)(R 4.412, F 4.416)]  [G loss: -97.757] \n",
                        "3995 [D loss: (4.414)(R 4.412, F 4.416)]  [G loss: -97.768] \n",
                        "3996 [D loss: (4.414)(R 4.413, F 4.416)]  [G loss: -97.779] \n",
                        "3997 [D loss: (4.415)(R 4.413, F 4.416)]  [G loss: -97.790] \n",
                        "3998 [D loss: (4.415)(R 4.413, F 4.417)]  [G loss: -97.804] \n",
                        "3999 [D loss: (4.415)(R 4.413, F 4.417)]  [G loss: -97.817] \n",
                        "4000 [D loss: (4.416)(R 4.414, F 4.418)]  [G loss: -97.832] \n",
                        "4001 [D loss: (4.417)(R 4.415, F 4.419)]  [G loss: -97.847] \n",
                        "4002 [D loss: (4.418)(R 4.415, F 4.420)]  [G loss: -97.863] \n",
                        "4003 [D loss: (4.419)(R 4.417, F 4.421)]  [G loss: -97.880] \n",
                        "4004 [D loss: (4.420)(R 4.418, F 4.422)]  [G loss: -97.896] \n",
                        "4005 [D loss: (4.421)(R 4.419, F 4.423)]  [G loss: -97.912] \n",
                        "4006 [D loss: (4.422)(R 4.420, F 4.425)]  [G loss: -97.927] \n",
                        "4007 [D loss: (4.424)(R 4.421, F 4.426)]  [G loss: -97.943] \n",
                        "4008 [D loss: (4.425)(R 4.422, F 4.427)]  [G loss: -97.958] \n",
                        "4009 [D loss: (4.426)(R 4.424, F 4.428)]  [G loss: -97.972] \n",
                        "4010 [D loss: (4.427)(R 4.424, F 4.429)]  [G loss: -97.985] \n",
                        "4011 [D loss: (4.427)(R 4.425, F 4.429)]  [G loss: -97.998] \n",
                        "4012 [D loss: (4.428)(R 4.426, F 4.430)]  [G loss: -98.011] \n",
                        "4013 [D loss: (4.429)(R 4.427, F 4.431)]  [G loss: -98.024] \n",
                        "4014 [D loss: (4.429)(R 4.427, F 4.431)]  [G loss: -98.038] \n",
                        "4015 [D loss: (4.430)(R 4.428, F 4.432)]  [G loss: -98.051] \n",
                        "4016 [D loss: (4.431)(R 4.429, F 4.433)]  [G loss: -98.065] \n",
                        "4017 [D loss: (4.432)(R 4.430, F 4.434)]  [G loss: -98.078] \n",
                        "4018 [D loss: (4.433)(R 4.431, F 4.435)]  [G loss: -98.092] \n",
                        "4019 [D loss: (4.434)(R 4.432, F 4.435)]  [G loss: -98.105] \n",
                        "4020 [D loss: (4.434)(R 4.432, F 4.436)]  [G loss: -98.118] \n",
                        "4021 [D loss: (4.435)(R 4.433, F 4.437)]  [G loss: -98.131] \n",
                        "4022 [D loss: (4.436)(R 4.434, F 4.438)]  [G loss: -98.144] \n",
                        "4023 [D loss: (4.436)(R 4.434, F 4.438)]  [G loss: -98.157] \n",
                        "4024 [D loss: (4.437)(R 4.435, F 4.439)]  [G loss: -98.171] \n",
                        "4025 [D loss: (4.437)(R 4.435, F 4.439)]  [G loss: -98.185] \n",
                        "4026 [D loss: (4.438)(R 4.436, F 4.440)]  [G loss: -98.199] \n",
                        "4027 [D loss: (4.439)(R 4.437, F 4.441)]  [G loss: -98.213] \n",
                        "4028 [D loss: (4.440)(R 4.438, F 4.442)]  [G loss: -98.227] \n",
                        "4029 [D loss: (4.441)(R 4.439, F 4.443)]  [G loss: -98.241] \n",
                        "4030 [D loss: (4.442)(R 4.440, F 4.444)]  [G loss: -98.254] \n",
                        "4031 [D loss: (4.443)(R 4.441, F 4.445)]  [G loss: -98.266] \n",
                        "4032 [D loss: (4.444)(R 4.442, F 4.445)]  [G loss: -98.277] \n",
                        "4033 [D loss: (4.444)(R 4.442, F 4.446)]  [G loss: -98.287] \n",
                        "4034 [D loss: (4.444)(R 4.443, F 4.446)]  [G loss: -98.297] \n",
                        "4035 [D loss: (4.445)(R 4.443, F 4.447)]  [G loss: -98.305] \n",
                        "4036 [D loss: (4.445)(R 4.443, F 4.447)]  [G loss: -98.313] \n",
                        "4037 [D loss: (4.445)(R 4.444, F 4.447)]  [G loss: -98.321] \n",
                        "4038 [D loss: (4.445)(R 4.444, F 4.447)]  [G loss: -98.330] \n",
                        "4039 [D loss: (4.446)(R 4.444, F 4.448)]  [G loss: -98.339] \n",
                        "4040 [D loss: (4.446)(R 4.445, F 4.448)]  [G loss: -98.347] \n",
                        "4041 [D loss: (4.446)(R 4.445, F 4.448)]  [G loss: -98.356] \n",
                        "4042 [D loss: (4.447)(R 4.445, F 4.448)]  [G loss: -98.366] \n",
                        "4043 [D loss: (4.447)(R 4.445, F 4.449)]  [G loss: -98.375] \n",
                        "4044 [D loss: (4.447)(R 4.445, F 4.449)]  [G loss: -98.385] \n",
                        "4045 [D loss: (4.447)(R 4.445, F 4.449)]  [G loss: -98.396] \n",
                        "4046 [D loss: (4.447)(R 4.446, F 4.449)]  [G loss: -98.408] \n",
                        "4047 [D loss: (4.448)(R 4.446, F 4.450)]  [G loss: -98.419] \n",
                        "4048 [D loss: (4.448)(R 4.446, F 4.450)]  [G loss: -98.431] \n",
                        "4049 [D loss: (4.449)(R 4.447, F 4.451)]  [G loss: -98.444] \n",
                        "4050 [D loss: (4.449)(R 4.447, F 4.451)]  [G loss: -98.457] \n",
                        "4051 [D loss: (4.450)(R 4.448, F 4.452)]  [G loss: -98.471] \n",
                        "4052 [D loss: (4.451)(R 4.449, F 4.453)]  [G loss: -98.484] \n",
                        "4053 [D loss: (4.453)(R 4.451, F 4.455)]  [G loss: -98.499] \n",
                        "4054 [D loss: (4.454)(R 4.452, F 4.456)]  [G loss: -98.512] \n",
                        "4055 [D loss: (4.455)(R 4.453, F 4.457)]  [G loss: -98.525] \n",
                        "4056 [D loss: (4.456)(R 4.454, F 4.458)]  [G loss: -98.538] \n",
                        "4057 [D loss: (4.457)(R 4.455, F 4.459)]  [G loss: -98.551] \n",
                        "4058 [D loss: (4.457)(R 4.455, F 4.459)]  [G loss: -98.564] \n",
                        "4059 [D loss: (4.458)(R 4.456, F 4.460)]  [G loss: -98.577] \n",
                        "4060 [D loss: (4.458)(R 4.456, F 4.460)]  [G loss: -98.590] \n",
                        "4061 [D loss: (4.459)(R 4.457, F 4.461)]  [G loss: -98.601] \n",
                        "4062 [D loss: (4.459)(R 4.457, F 4.461)]  [G loss: -98.612] \n",
                        "4063 [D loss: (4.460)(R 4.458, F 4.461)]  [G loss: -98.623] \n",
                        "4064 [D loss: (4.460)(R 4.458, F 4.462)]  [G loss: -98.635] \n",
                        "4065 [D loss: (4.461)(R 4.459, F 4.463)]  [G loss: -98.646] \n",
                        "4066 [D loss: (4.462)(R 4.460, F 4.463)]  [G loss: -98.657] \n",
                        "4067 [D loss: (4.462)(R 4.460, F 4.464)]  [G loss: -98.667] \n",
                        "4068 [D loss: (4.463)(R 4.461, F 4.465)]  [G loss: -98.678] \n",
                        "4069 [D loss: (4.464)(R 4.462, F 4.465)]  [G loss: -98.688] \n",
                        "4070 [D loss: (4.464)(R 4.462, F 4.466)]  [G loss: -98.699] \n",
                        "4071 [D loss: (4.465)(R 4.463, F 4.466)]  [G loss: -98.709] \n",
                        "4072 [D loss: (4.465)(R 4.463, F 4.467)]  [G loss: -98.721] \n",
                        "4073 [D loss: (4.466)(R 4.464, F 4.467)]  [G loss: -98.732] \n",
                        "4074 [D loss: (4.466)(R 4.464, F 4.468)]  [G loss: -98.744] \n",
                        "4075 [D loss: (4.466)(R 4.465, F 4.468)]  [G loss: -98.755] \n",
                        "4076 [D loss: (4.467)(R 4.465, F 4.469)]  [G loss: -98.767] \n",
                        "4077 [D loss: (4.467)(R 4.465, F 4.469)]  [G loss: -98.780] \n",
                        "4078 [D loss: (4.468)(R 4.466, F 4.470)]  [G loss: -98.793] \n",
                        "4079 [D loss: (4.469)(R 4.467, F 4.471)]  [G loss: -98.806] \n",
                        "4080 [D loss: (4.470)(R 4.468, F 4.472)]  [G loss: -98.820] \n",
                        "4081 [D loss: (4.471)(R 4.469, F 4.473)]  [G loss: -98.834] \n",
                        "4082 [D loss: (4.472)(R 4.470, F 4.474)]  [G loss: -98.847] \n",
                        "4083 [D loss: (4.473)(R 4.471, F 4.475)]  [G loss: -98.861] \n",
                        "4084 [D loss: (4.474)(R 4.472, F 4.476)]  [G loss: -98.875] \n",
                        "4085 [D loss: (4.475)(R 4.473, F 4.477)]  [G loss: -98.888] \n",
                        "4086 [D loss: (4.476)(R 4.474, F 4.478)]  [G loss: -98.901] \n",
                        "4087 [D loss: (4.476)(R 4.474, F 4.478)]  [G loss: -98.914] \n",
                        "4088 [D loss: (4.477)(R 4.475, F 4.479)]  [G loss: -98.925] \n",
                        "4089 [D loss: (4.477)(R 4.476, F 4.479)]  [G loss: -98.936] \n",
                        "4090 [D loss: (4.478)(R 4.476, F 4.480)]  [G loss: -98.947] \n",
                        "4091 [D loss: (4.479)(R 4.477, F 4.481)]  [G loss: -98.958] \n",
                        "4092 [D loss: (4.479)(R 4.477, F 4.481)]  [G loss: -98.969] \n",
                        "4093 [D loss: (4.480)(R 4.478, F 4.482)]  [G loss: -98.980] \n",
                        "4094 [D loss: (4.481)(R 4.479, F 4.482)]  [G loss: -98.992] \n",
                        "4095 [D loss: (4.481)(R 4.480, F 4.483)]  [G loss: -99.004] \n",
                        "4096 [D loss: (4.482)(R 4.480, F 4.484)]  [G loss: -99.014] \n",
                        "4097 [D loss: (4.483)(R 4.481, F 4.485)]  [G loss: -99.024] \n",
                        "4098 [D loss: (4.483)(R 4.481, F 4.485)]  [G loss: -99.035] \n",
                        "4099 [D loss: (4.483)(R 4.481, F 4.485)]  [G loss: -99.046] \n",
                        "4100 [D loss: (4.484)(R 4.482, F 4.486)]  [G loss: -99.057] \n",
                        "4101 [D loss: (4.484)(R 4.482, F 4.486)]  [G loss: -99.068] \n",
                        "4102 [D loss: (4.485)(R 4.483, F 4.486)]  [G loss: -99.080] \n",
                        "4103 [D loss: (4.485)(R 4.483, F 4.487)]  [G loss: -99.092] \n",
                        "4104 [D loss: (4.486)(R 4.484, F 4.488)]  [G loss: -99.104] \n",
                        "4105 [D loss: (4.487)(R 4.485, F 4.489)]  [G loss: -99.116] \n",
                        "4106 [D loss: (4.488)(R 4.486, F 4.489)]  [G loss: -99.128] \n",
                        "4107 [D loss: (4.488)(R 4.486, F 4.490)]  [G loss: -99.140] \n",
                        "4108 [D loss: (4.489)(R 4.487, F 4.491)]  [G loss: -99.152] \n",
                        "4109 [D loss: (4.490)(R 4.488, F 4.492)]  [G loss: -99.163] \n",
                        "4110 [D loss: (4.490)(R 4.489, F 4.492)]  [G loss: -99.174] \n",
                        "4111 [D loss: (4.491)(R 4.489, F 4.493)]  [G loss: -99.185] \n",
                        "4112 [D loss: (4.492)(R 4.490, F 4.493)]  [G loss: -99.196] \n",
                        "4113 [D loss: (4.492)(R 4.490, F 4.494)]  [G loss: -99.206] \n",
                        "4114 [D loss: (4.492)(R 4.491, F 4.494)]  [G loss: -99.217] \n",
                        "4115 [D loss: (4.493)(R 4.491, F 4.495)]  [G loss: -99.227] \n",
                        "4116 [D loss: (4.493)(R 4.491, F 4.495)]  [G loss: -99.238] \n",
                        "4117 [D loss: (4.494)(R 4.492, F 4.496)]  [G loss: -99.249] \n",
                        "4118 [D loss: (4.494)(R 4.492, F 4.496)]  [G loss: -99.260] \n",
                        "4119 [D loss: (4.495)(R 4.493, F 4.497)]  [G loss: -99.272] \n",
                        "4120 [D loss: (4.495)(R 4.493, F 4.497)]  [G loss: -99.283] \n",
                        "4121 [D loss: (4.496)(R 4.494, F 4.497)]  [G loss: -99.295] \n",
                        "4122 [D loss: (4.496)(R 4.494, F 4.498)]  [G loss: -99.306] \n",
                        "4123 [D loss: (4.497)(R 4.495, F 4.498)]  [G loss: -99.318] \n",
                        "4124 [D loss: (4.497)(R 4.495, F 4.499)]  [G loss: -99.331] \n",
                        "4125 [D loss: (4.497)(R 4.496, F 4.499)]  [G loss: -99.344] \n",
                        "4126 [D loss: (4.498)(R 4.496, F 4.500)]  [G loss: -99.357] \n",
                        "4127 [D loss: (4.499)(R 4.497, F 4.501)]  [G loss: -99.370] \n",
                        "4128 [D loss: (4.500)(R 4.498, F 4.501)]  [G loss: -99.384] \n",
                        "4129 [D loss: (4.500)(R 4.498, F 4.502)]  [G loss: -99.397] \n",
                        "4130 [D loss: (4.501)(R 4.499, F 4.503)]  [G loss: -99.410] \n",
                        "4131 [D loss: (4.502)(R 4.500, F 4.504)]  [G loss: -99.423] \n",
                        "4132 [D loss: (4.503)(R 4.501, F 4.505)]  [G loss: -99.436] \n",
                        "4133 [D loss: (4.504)(R 4.502, F 4.506)]  [G loss: -99.448] \n",
                        "4134 [D loss: (4.504)(R 4.502, F 4.506)]  [G loss: -99.458] \n",
                        "4135 [D loss: (4.505)(R 4.503, F 4.507)]  [G loss: -99.469] \n",
                        "4136 [D loss: (4.505)(R 4.503, F 4.507)]  [G loss: -99.480] \n",
                        "4137 [D loss: (4.505)(R 4.504, F 4.507)]  [G loss: -99.490] \n",
                        "4138 [D loss: (4.506)(R 4.504, F 4.508)]  [G loss: -99.500] \n",
                        "4139 [D loss: (4.506)(R 4.504, F 4.508)]  [G loss: -99.510] \n",
                        "4140 [D loss: (4.506)(R 4.505, F 4.508)]  [G loss: -99.521] \n",
                        "4141 [D loss: (4.507)(R 4.505, F 4.509)]  [G loss: -99.532] \n",
                        "4142 [D loss: (4.507)(R 4.506, F 4.509)]  [G loss: -99.543] \n",
                        "4143 [D loss: (4.508)(R 4.506, F 4.510)]  [G loss: -99.555] \n",
                        "4144 [D loss: (4.509)(R 4.507, F 4.510)]  [G loss: -99.567] \n",
                        "4145 [D loss: (4.509)(R 4.507, F 4.511)]  [G loss: -99.579] \n",
                        "4146 [D loss: (4.510)(R 4.508, F 4.512)]  [G loss: -99.592] \n",
                        "4147 [D loss: (4.511)(R 4.509, F 4.513)]  [G loss: -99.605] \n",
                        "4148 [D loss: (4.511)(R 4.509, F 4.513)]  [G loss: -99.619] \n",
                        "4149 [D loss: (4.512)(R 4.510, F 4.514)]  [G loss: -99.632] \n",
                        "4150 [D loss: (4.513)(R 4.511, F 4.515)]  [G loss: -99.646] \n",
                        "4151 [D loss: (4.514)(R 4.512, F 4.516)]  [G loss: -99.660] \n",
                        "4152 [D loss: (4.515)(R 4.513, F 4.517)]  [G loss: -99.673] \n",
                        "4153 [D loss: (4.515)(R 4.513, F 4.517)]  [G loss: -99.686] \n",
                        "4154 [D loss: (4.516)(R 4.514, F 4.518)]  [G loss: -99.699] \n",
                        "4155 [D loss: (4.517)(R 4.515, F 4.519)]  [G loss: -99.712] \n",
                        "4156 [D loss: (4.518)(R 4.516, F 4.520)]  [G loss: -99.724] \n",
                        "4157 [D loss: (4.519)(R 4.517, F 4.521)]  [G loss: -99.737] \n",
                        "4158 [D loss: (4.520)(R 4.518, F 4.522)]  [G loss: -99.748] \n",
                        "4159 [D loss: (4.521)(R 4.519, F 4.523)]  [G loss: -99.759] \n",
                        "4160 [D loss: (4.521)(R 4.519, F 4.523)]  [G loss: -99.769] \n",
                        "4161 [D loss: (4.522)(R 4.520, F 4.524)]  [G loss: -99.780] \n",
                        "4162 [D loss: (4.522)(R 4.520, F 4.524)]  [G loss: -99.790] \n",
                        "4163 [D loss: (4.523)(R 4.521, F 4.524)]  [G loss: -99.800] \n",
                        "4164 [D loss: (4.523)(R 4.521, F 4.525)]  [G loss: -99.809] \n",
                        "4165 [D loss: (4.523)(R 4.522, F 4.525)]  [G loss: -99.818] \n",
                        "4166 [D loss: (4.524)(R 4.522, F 4.525)]  [G loss: -99.828] \n",
                        "4167 [D loss: (4.524)(R 4.522, F 4.526)]  [G loss: -99.838] \n",
                        "4168 [D loss: (4.525)(R 4.523, F 4.526)]  [G loss: -99.848] \n",
                        "4169 [D loss: (4.525)(R 4.523, F 4.527)]  [G loss: -99.858] \n",
                        "4170 [D loss: (4.525)(R 4.524, F 4.527)]  [G loss: -99.870] \n",
                        "4171 [D loss: (4.526)(R 4.524, F 4.528)]  [G loss: -99.881] \n",
                        "4172 [D loss: (4.527)(R 4.525, F 4.528)]  [G loss: -99.893] \n",
                        "4173 [D loss: (4.527)(R 4.525, F 4.529)]  [G loss: -99.905] \n",
                        "4174 [D loss: (4.528)(R 4.526, F 4.530)]  [G loss: -99.917] \n",
                        "4175 [D loss: (4.528)(R 4.527, F 4.530)]  [G loss: -99.930] \n",
                        "4176 [D loss: (4.529)(R 4.527, F 4.531)]  [G loss: -99.942] \n",
                        "4177 [D loss: (4.530)(R 4.528, F 4.532)]  [G loss: -99.953] \n",
                        "4178 [D loss: (4.530)(R 4.528, F 4.532)]  [G loss: -99.965] \n",
                        "4179 [D loss: (4.531)(R 4.529, F 4.533)]  [G loss: -99.976] \n",
                        "4180 [D loss: (4.532)(R 4.530, F 4.533)]  [G loss: -99.988] \n",
                        "4181 [D loss: (4.532)(R 4.530, F 4.534)]  [G loss: -99.999] \n",
                        "4182 [D loss: (4.533)(R 4.531, F 4.535)]  [G loss: -100.010] \n",
                        "4183 [D loss: (4.533)(R 4.532, F 4.535)]  [G loss: -100.021] \n",
                        "4184 [D loss: (4.534)(R 4.532, F 4.536)]  [G loss: -100.032] \n",
                        "4185 [D loss: (4.535)(R 4.533, F 4.536)]  [G loss: -100.043] \n",
                        "4186 [D loss: (4.535)(R 4.534, F 4.537)]  [G loss: -100.054] \n",
                        "4187 [D loss: (4.536)(R 4.534, F 4.538)]  [G loss: -100.065] \n",
                        "4188 [D loss: (4.536)(R 4.535, F 4.538)]  [G loss: -100.075] \n",
                        "4189 [D loss: (4.537)(R 4.535, F 4.539)]  [G loss: -100.085] \n",
                        "4190 [D loss: (4.537)(R 4.535, F 4.539)]  [G loss: -100.096] \n",
                        "4191 [D loss: (4.538)(R 4.536, F 4.540)]  [G loss: -100.106] \n",
                        "4192 [D loss: (4.538)(R 4.536, F 4.540)]  [G loss: -100.116] \n",
                        "4193 [D loss: (4.539)(R 4.537, F 4.541)]  [G loss: -100.127] \n",
                        "4194 [D loss: (4.539)(R 4.537, F 4.541)]  [G loss: -100.138] \n",
                        "4195 [D loss: (4.540)(R 4.538, F 4.542)]  [G loss: -100.149] \n",
                        "4196 [D loss: (4.541)(R 4.539, F 4.542)]  [G loss: -100.161] \n",
                        "4197 [D loss: (4.541)(R 4.539, F 4.543)]  [G loss: -100.173] \n",
                        "4198 [D loss: (4.542)(R 4.540, F 4.544)]  [G loss: -100.184] \n",
                        "4199 [D loss: (4.542)(R 4.541, F 4.544)]  [G loss: -100.196] \n",
                        "4200 [D loss: (4.543)(R 4.541, F 4.545)]  [G loss: -100.207] \n",
                        "4201 [D loss: (4.544)(R 4.542, F 4.545)]  [G loss: -100.220] \n",
                        "4202 [D loss: (4.544)(R 4.542, F 4.546)]  [G loss: -100.232] \n",
                        "4203 [D loss: (4.545)(R 4.543, F 4.547)]  [G loss: -100.244] \n",
                        "4204 [D loss: (4.546)(R 4.544, F 4.548)]  [G loss: -100.257] \n",
                        "4205 [D loss: (4.547)(R 4.545, F 4.548)]  [G loss: -100.269] \n",
                        "4206 [D loss: (4.547)(R 4.545, F 4.549)]  [G loss: -100.281] \n",
                        "4207 [D loss: (4.548)(R 4.546, F 4.550)]  [G loss: -100.294] \n",
                        "4208 [D loss: (4.549)(R 4.547, F 4.551)]  [G loss: -100.306] \n",
                        "4209 [D loss: (4.549)(R 4.548, F 4.551)]  [G loss: -100.319] \n",
                        "4210 [D loss: (4.550)(R 4.548, F 4.552)]  [G loss: -100.331] \n",
                        "4211 [D loss: (4.551)(R 4.549, F 4.553)]  [G loss: -100.344] \n",
                        "4212 [D loss: (4.552)(R 4.550, F 4.554)]  [G loss: -100.357] \n",
                        "4213 [D loss: (4.553)(R 4.551, F 4.555)]  [G loss: -100.369] \n",
                        "4214 [D loss: (4.553)(R 4.552, F 4.555)]  [G loss: -100.381] \n",
                        "4215 [D loss: (4.554)(R 4.552, F 4.556)]  [G loss: -100.394] \n",
                        "4216 [D loss: (4.555)(R 4.553, F 4.557)]  [G loss: -100.404] \n",
                        "4217 [D loss: (4.556)(R 4.554, F 4.558)]  [G loss: -100.415] \n",
                        "4218 [D loss: (4.556)(R 4.554, F 4.558)]  [G loss: -100.426] \n",
                        "4219 [D loss: (4.557)(R 4.555, F 4.559)]  [G loss: -100.436] \n",
                        "4220 [D loss: (4.557)(R 4.556, F 4.559)]  [G loss: -100.446] \n",
                        "4221 [D loss: (4.558)(R 4.556, F 4.560)]  [G loss: -100.456] \n",
                        "4222 [D loss: (4.558)(R 4.557, F 4.560)]  [G loss: -100.466] \n",
                        "4223 [D loss: (4.559)(R 4.557, F 4.561)]  [G loss: -100.477] \n",
                        "4224 [D loss: (4.559)(R 4.558, F 4.561)]  [G loss: -100.488] \n",
                        "4225 [D loss: (4.560)(R 4.558, F 4.562)]  [G loss: -100.498] \n",
                        "4226 [D loss: (4.560)(R 4.558, F 4.562)]  [G loss: -100.509] \n",
                        "4227 [D loss: (4.561)(R 4.559, F 4.563)]  [G loss: -100.520] \n",
                        "4228 [D loss: (4.561)(R 4.559, F 4.563)]  [G loss: -100.531] \n",
                        "4229 [D loss: (4.562)(R 4.560, F 4.564)]  [G loss: -100.542] \n",
                        "4230 [D loss: (4.562)(R 4.560, F 4.564)]  [G loss: -100.554] \n",
                        "4231 [D loss: (4.563)(R 4.561, F 4.565)]  [G loss: -100.565] \n",
                        "4232 [D loss: (4.563)(R 4.562, F 4.565)]  [G loss: -100.576] \n",
                        "4233 [D loss: (4.564)(R 4.562, F 4.566)]  [G loss: -100.587] \n",
                        "4234 [D loss: (4.564)(R 4.563, F 4.566)]  [G loss: -100.598] \n",
                        "4235 [D loss: (4.565)(R 4.563, F 4.567)]  [G loss: -100.609] \n",
                        "4236 [D loss: (4.566)(R 4.564, F 4.567)]  [G loss: -100.619] \n",
                        "4237 [D loss: (4.566)(R 4.564, F 4.568)]  [G loss: -100.629] \n",
                        "4238 [D loss: (4.566)(R 4.565, F 4.568)]  [G loss: -100.640] \n",
                        "4239 [D loss: (4.567)(R 4.565, F 4.569)]  [G loss: -100.650] \n",
                        "4240 [D loss: (4.567)(R 4.565, F 4.569)]  [G loss: -100.660] \n",
                        "4241 [D loss: (4.568)(R 4.566, F 4.569)]  [G loss: -100.671] \n",
                        "4242 [D loss: (4.568)(R 4.566, F 4.570)]  [G loss: -100.681] \n",
                        "4243 [D loss: (4.569)(R 4.567, F 4.570)]  [G loss: -100.691] \n",
                        "4244 [D loss: (4.569)(R 4.567, F 4.571)]  [G loss: -100.702] \n",
                        "4245 [D loss: (4.569)(R 4.568, F 4.571)]  [G loss: -100.712] \n",
                        "4246 [D loss: (4.570)(R 4.568, F 4.572)]  [G loss: -100.723] \n",
                        "4247 [D loss: (4.570)(R 4.569, F 4.572)]  [G loss: -100.734] \n",
                        "4248 [D loss: (4.571)(R 4.569, F 4.573)]  [G loss: -100.745] \n",
                        "4249 [D loss: (4.571)(R 4.570, F 4.573)]  [G loss: -100.756] \n",
                        "4250 [D loss: (4.572)(R 4.570, F 4.574)]  [G loss: -100.767] \n",
                        "4251 [D loss: (4.572)(R 4.571, F 4.574)]  [G loss: -100.779] \n",
                        "4252 [D loss: (4.573)(R 4.571, F 4.575)]  [G loss: -100.790] \n",
                        "4253 [D loss: (4.574)(R 4.572, F 4.576)]  [G loss: -100.802] \n",
                        "4254 [D loss: (4.575)(R 4.573, F 4.577)]  [G loss: -100.814] \n",
                        "4255 [D loss: (4.575)(R 4.573, F 4.577)]  [G loss: -100.826] \n",
                        "4256 [D loss: (4.576)(R 4.574, F 4.578)]  [G loss: -100.838] \n",
                        "4257 [D loss: (4.577)(R 4.575, F 4.579)]  [G loss: -100.850] \n",
                        "4258 [D loss: (4.578)(R 4.576, F 4.579)]  [G loss: -100.863] \n",
                        "4259 [D loss: (4.579)(R 4.577, F 4.581)]  [G loss: -100.874] \n",
                        "4260 [D loss: (4.579)(R 4.577, F 4.581)]  [G loss: -100.886] \n",
                        "4261 [D loss: (4.580)(R 4.578, F 4.582)]  [G loss: -100.898] \n",
                        "4262 [D loss: (4.581)(R 4.579, F 4.583)]  [G loss: -100.909] \n",
                        "4263 [D loss: (4.581)(R 4.580, F 4.583)]  [G loss: -100.920] \n",
                        "4264 [D loss: (4.582)(R 4.580, F 4.584)]  [G loss: -100.931] \n",
                        "4265 [D loss: (4.583)(R 4.581, F 4.584)]  [G loss: -100.943] \n",
                        "4266 [D loss: (4.584)(R 4.582, F 4.585)]  [G loss: -100.952] \n",
                        "4267 [D loss: (4.584)(R 4.582, F 4.586)]  [G loss: -100.962] \n",
                        "4268 [D loss: (4.584)(R 4.583, F 4.586)]  [G loss: -100.973] \n",
                        "4269 [D loss: (4.585)(R 4.583, F 4.587)]  [G loss: -100.984] \n",
                        "4270 [D loss: (4.586)(R 4.584, F 4.588)]  [G loss: -100.993] \n",
                        "4271 [D loss: (4.586)(R 4.584, F 4.588)]  [G loss: -101.003] \n",
                        "4272 [D loss: (4.587)(R 4.585, F 4.588)]  [G loss: -101.013] \n",
                        "4273 [D loss: (4.587)(R 4.585, F 4.589)]  [G loss: -101.023] \n",
                        "4274 [D loss: (4.588)(R 4.586, F 4.589)]  [G loss: -101.033] \n",
                        "4275 [D loss: (4.588)(R 4.586, F 4.590)]  [G loss: -101.043] \n",
                        "4276 [D loss: (4.589)(R 4.587, F 4.590)]  [G loss: -101.054] \n",
                        "4277 [D loss: (4.589)(R 4.587, F 4.591)]  [G loss: -101.064] \n",
                        "4278 [D loss: (4.589)(R 4.588, F 4.591)]  [G loss: -101.074] \n",
                        "4279 [D loss: (4.590)(R 4.588, F 4.592)]  [G loss: -101.085] \n",
                        "4280 [D loss: (4.590)(R 4.589, F 4.592)]  [G loss: -101.094] \n",
                        "4281 [D loss: (4.591)(R 4.589, F 4.593)]  [G loss: -101.105] \n",
                        "4282 [D loss: (4.591)(R 4.589, F 4.593)]  [G loss: -101.116] \n",
                        "4283 [D loss: (4.592)(R 4.590, F 4.594)]  [G loss: -101.127] \n",
                        "4284 [D loss: (4.592)(R 4.590, F 4.594)]  [G loss: -101.137] \n",
                        "4285 [D loss: (4.593)(R 4.591, F 4.595)]  [G loss: -101.148] \n",
                        "4286 [D loss: (4.593)(R 4.591, F 4.595)]  [G loss: -101.159] \n",
                        "4287 [D loss: (4.594)(R 4.592, F 4.596)]  [G loss: -101.170] \n",
                        "4288 [D loss: (4.594)(R 4.593, F 4.596)]  [G loss: -101.181] \n",
                        "4289 [D loss: (4.595)(R 4.593, F 4.596)]  [G loss: -101.192] \n",
                        "4290 [D loss: (4.596)(R 4.594, F 4.597)]  [G loss: -101.203] \n",
                        "4291 [D loss: (4.596)(R 4.594, F 4.598)]  [G loss: -101.214] \n",
                        "4292 [D loss: (4.596)(R 4.595, F 4.598)]  [G loss: -101.224] \n",
                        "4293 [D loss: (4.597)(R 4.595, F 4.599)]  [G loss: -101.236] \n",
                        "4294 [D loss: (4.598)(R 4.596, F 4.600)]  [G loss: -101.246] \n",
                        "4295 [D loss: (4.598)(R 4.597, F 4.600)]  [G loss: -101.256] \n",
                        "4296 [D loss: (4.599)(R 4.597, F 4.601)]  [G loss: -101.267] \n",
                        "4297 [D loss: (4.599)(R 4.598, F 4.601)]  [G loss: -101.277] \n",
                        "4298 [D loss: (4.600)(R 4.598, F 4.602)]  [G loss: -101.288] \n",
                        "4299 [D loss: (4.601)(R 4.599, F 4.602)]  [G loss: -101.298] \n",
                        "4300 [D loss: (4.601)(R 4.599, F 4.603)]  [G loss: -101.309] \n",
                        "4301 [D loss: (4.602)(R 4.600, F 4.603)]  [G loss: -101.320] \n",
                        "4302 [D loss: (4.602)(R 4.600, F 4.604)]  [G loss: -101.331] \n",
                        "4303 [D loss: (4.603)(R 4.601, F 4.605)]  [G loss: -101.341] \n",
                        "4304 [D loss: (4.603)(R 4.602, F 4.605)]  [G loss: -101.352] \n",
                        "4305 [D loss: (4.604)(R 4.602, F 4.606)]  [G loss: -101.363] \n",
                        "4306 [D loss: (4.605)(R 4.603, F 4.606)]  [G loss: -101.374] \n",
                        "4307 [D loss: (4.605)(R 4.604, F 4.607)]  [G loss: -101.384] \n",
                        "4308 [D loss: (4.606)(R 4.604, F 4.608)]  [G loss: -101.395] \n",
                        "4309 [D loss: (4.606)(R 4.605, F 4.608)]  [G loss: -101.406] \n",
                        "4310 [D loss: (4.607)(R 4.605, F 4.609)]  [G loss: -101.417] \n",
                        "4311 [D loss: (4.608)(R 4.606, F 4.609)]  [G loss: -101.428] \n",
                        "4312 [D loss: (4.608)(R 4.606, F 4.610)]  [G loss: -101.440] \n",
                        "4313 [D loss: (4.609)(R 4.607, F 4.611)]  [G loss: -101.450] \n",
                        "4314 [D loss: (4.609)(R 4.608, F 4.611)]  [G loss: -101.461] \n",
                        "4315 [D loss: (4.610)(R 4.608, F 4.612)]  [G loss: -101.473] \n",
                        "4316 [D loss: (4.611)(R 4.609, F 4.613)]  [G loss: -101.485] \n",
                        "4317 [D loss: (4.611)(R 4.610, F 4.613)]  [G loss: -101.496] \n",
                        "4318 [D loss: (4.612)(R 4.610, F 4.614)]  [G loss: -101.509] \n",
                        "4319 [D loss: (4.613)(R 4.611, F 4.615)]  [G loss: -101.520] \n",
                        "4320 [D loss: (4.613)(R 4.611, F 4.615)]  [G loss: -101.532] \n",
                        "4321 [D loss: (4.614)(R 4.613, F 4.616)]  [G loss: -101.543] \n",
                        "4322 [D loss: (4.615)(R 4.613, F 4.617)]  [G loss: -101.554] \n",
                        "4323 [D loss: (4.616)(R 4.614, F 4.617)]  [G loss: -101.566] \n",
                        "4324 [D loss: (4.616)(R 4.615, F 4.618)]  [G loss: -101.576] \n",
                        "4325 [D loss: (4.617)(R 4.615, F 4.618)]  [G loss: -101.588] \n",
                        "4326 [D loss: (4.618)(R 4.616, F 4.620)]  [G loss: -101.597] \n",
                        "4327 [D loss: (4.618)(R 4.617, F 4.620)]  [G loss: -101.607] \n",
                        "4328 [D loss: (4.619)(R 4.617, F 4.621)]  [G loss: -101.618] \n",
                        "4329 [D loss: (4.620)(R 4.618, F 4.621)]  [G loss: -101.629] \n",
                        "4330 [D loss: (4.620)(R 4.618, F 4.622)]  [G loss: -101.638] \n",
                        "4331 [D loss: (4.620)(R 4.619, F 4.622)]  [G loss: -101.649] \n",
                        "4332 [D loss: (4.621)(R 4.619, F 4.623)]  [G loss: -101.659] \n",
                        "4333 [D loss: (4.622)(R 4.620, F 4.623)]  [G loss: -101.669] \n",
                        "4334 [D loss: (4.622)(R 4.620, F 4.624)]  [G loss: -101.679] \n",
                        "4335 [D loss: (4.623)(R 4.621, F 4.624)]  [G loss: -101.689] \n",
                        "4336 [D loss: (4.623)(R 4.621, F 4.625)]  [G loss: -101.699] \n",
                        "4337 [D loss: (4.623)(R 4.622, F 4.625)]  [G loss: -101.710] \n",
                        "4338 [D loss: (4.624)(R 4.622, F 4.626)]  [G loss: -101.719] \n",
                        "4339 [D loss: (4.624)(R 4.623, F 4.626)]  [G loss: -101.729] \n",
                        "4340 [D loss: (4.625)(R 4.623, F 4.627)]  [G loss: -101.740] \n",
                        "4341 [D loss: (4.625)(R 4.624, F 4.627)]  [G loss: -101.750] \n",
                        "4342 [D loss: (4.626)(R 4.624, F 4.628)]  [G loss: -101.760] \n",
                        "4343 [D loss: (4.626)(R 4.624, F 4.628)]  [G loss: -101.770] \n",
                        "4344 [D loss: (4.627)(R 4.625, F 4.629)]  [G loss: -101.780] \n",
                        "4345 [D loss: (4.627)(R 4.626, F 4.629)]  [G loss: -101.789] \n",
                        "4346 [D loss: (4.628)(R 4.626, F 4.630)]  [G loss: -101.800] \n",
                        "4347 [D loss: (4.628)(R 4.626, F 4.630)]  [G loss: -101.811] \n",
                        "4348 [D loss: (4.629)(R 4.627, F 4.631)]  [G loss: -101.821] \n",
                        "4349 [D loss: (4.629)(R 4.628, F 4.631)]  [G loss: -101.832] \n",
                        "4350 [D loss: (4.630)(R 4.628, F 4.632)]  [G loss: -101.843] \n",
                        "4351 [D loss: (4.631)(R 4.629, F 4.632)]  [G loss: -101.854] \n",
                        "4352 [D loss: (4.631)(R 4.629, F 4.633)]  [G loss: -101.865] \n",
                        "4353 [D loss: (4.632)(R 4.630, F 4.634)]  [G loss: -101.876] \n",
                        "4354 [D loss: (4.632)(R 4.630, F 4.634)]  [G loss: -101.887] \n",
                        "4355 [D loss: (4.633)(R 4.631, F 4.635)]  [G loss: -101.898] \n",
                        "4356 [D loss: (4.633)(R 4.632, F 4.635)]  [G loss: -101.909] \n",
                        "4357 [D loss: (4.634)(R 4.632, F 4.636)]  [G loss: -101.919] \n",
                        "4358 [D loss: (4.635)(R 4.633, F 4.636)]  [G loss: -101.930] \n",
                        "4359 [D loss: (4.635)(R 4.633, F 4.637)]  [G loss: -101.940] \n",
                        "4360 [D loss: (4.636)(R 4.634, F 4.638)]  [G loss: -101.952] \n",
                        "4361 [D loss: (4.636)(R 4.635, F 4.638)]  [G loss: -101.963] \n",
                        "4362 [D loss: (4.637)(R 4.635, F 4.639)]  [G loss: -101.973] \n",
                        "4363 [D loss: (4.638)(R 4.636, F 4.639)]  [G loss: -101.984] \n",
                        "4364 [D loss: (4.638)(R 4.637, F 4.640)]  [G loss: -101.995] \n",
                        "4365 [D loss: (4.639)(R 4.637, F 4.641)]  [G loss: -102.006] \n",
                        "4366 [D loss: (4.640)(R 4.638, F 4.641)]  [G loss: -102.016] \n",
                        "4367 [D loss: (4.640)(R 4.638, F 4.642)]  [G loss: -102.027] \n",
                        "4368 [D loss: (4.640)(R 4.639, F 4.642)]  [G loss: -102.037] \n",
                        "4369 [D loss: (4.641)(R 4.639, F 4.643)]  [G loss: -102.048] \n",
                        "4370 [D loss: (4.642)(R 4.640, F 4.643)]  [G loss: -102.059] \n",
                        "4371 [D loss: (4.642)(R 4.640, F 4.644)]  [G loss: -102.070] \n",
                        "4372 [D loss: (4.643)(R 4.641, F 4.644)]  [G loss: -102.080] \n",
                        "4373 [D loss: (4.644)(R 4.642, F 4.645)]  [G loss: -102.090] \n",
                        "4374 [D loss: (4.644)(R 4.642, F 4.646)]  [G loss: -102.101] \n",
                        "4375 [D loss: (4.645)(R 4.643, F 4.646)]  [G loss: -102.112] \n",
                        "4376 [D loss: (4.645)(R 4.643, F 4.647)]  [G loss: -102.123] \n",
                        "4377 [D loss: (4.646)(R 4.644, F 4.647)]  [G loss: -102.133] \n",
                        "4378 [D loss: (4.646)(R 4.644, F 4.648)]  [G loss: -102.144] \n",
                        "4379 [D loss: (4.647)(R 4.645, F 4.649)]  [G loss: -102.155] \n",
                        "4380 [D loss: (4.647)(R 4.646, F 4.649)]  [G loss: -102.166] \n",
                        "4381 [D loss: (4.648)(R 4.646, F 4.650)]  [G loss: -102.176] \n",
                        "4382 [D loss: (4.648)(R 4.647, F 4.650)]  [G loss: -102.187] \n",
                        "4383 [D loss: (4.649)(R 4.647, F 4.651)]  [G loss: -102.198] \n",
                        "4384 [D loss: (4.650)(R 4.648, F 4.651)]  [G loss: -102.210] \n",
                        "4385 [D loss: (4.650)(R 4.648, F 4.652)]  [G loss: -102.222] \n",
                        "4386 [D loss: (4.651)(R 4.649, F 4.653)]  [G loss: -102.233] \n",
                        "4387 [D loss: (4.652)(R 4.650, F 4.654)]  [G loss: -102.244] \n",
                        "4388 [D loss: (4.653)(R 4.651, F 4.654)]  [G loss: -102.256] \n",
                        "4389 [D loss: (4.654)(R 4.652, F 4.655)]  [G loss: -102.267] \n",
                        "4390 [D loss: (4.654)(R 4.652, F 4.656)]  [G loss: -102.277] \n",
                        "4391 [D loss: (4.655)(R 4.653, F 4.656)]  [G loss: -102.289] \n",
                        "4392 [D loss: (4.655)(R 4.654, F 4.657)]  [G loss: -102.298] \n",
                        "4393 [D loss: (4.656)(R 4.654, F 4.658)]  [G loss: -102.309] \n",
                        "4394 [D loss: (4.656)(R 4.655, F 4.658)]  [G loss: -102.321] \n",
                        "4395 [D loss: (4.657)(R 4.655, F 4.659)]  [G loss: -102.331] \n",
                        "4396 [D loss: (4.658)(R 4.656, F 4.660)]  [G loss: -102.342] \n",
                        "4397 [D loss: (4.658)(R 4.656, F 4.660)]  [G loss: -102.353] \n",
                        "4398 [D loss: (4.659)(R 4.657, F 4.661)]  [G loss: -102.363] \n",
                        "4399 [D loss: (4.660)(R 4.658, F 4.662)]  [G loss: -102.373] \n",
                        "4400 [D loss: (4.660)(R 4.659, F 4.662)]  [G loss: -102.383] \n",
                        "4401 [D loss: (4.661)(R 4.659, F 4.662)]  [G loss: -102.393] \n",
                        "4402 [D loss: (4.661)(R 4.659, F 4.663)]  [G loss: -102.402] \n",
                        "4403 [D loss: (4.662)(R 4.660, F 4.663)]  [G loss: -102.412] \n",
                        "4404 [D loss: (4.662)(R 4.660, F 4.664)]  [G loss: -102.422] \n",
                        "4405 [D loss: (4.663)(R 4.661, F 4.664)]  [G loss: -102.432] \n",
                        "4406 [D loss: (4.663)(R 4.661, F 4.665)]  [G loss: -102.442] \n",
                        "4407 [D loss: (4.663)(R 4.662, F 4.665)]  [G loss: -102.453] \n",
                        "4408 [D loss: (4.664)(R 4.662, F 4.666)]  [G loss: -102.465] \n",
                        "4409 [D loss: (4.665)(R 4.663, F 4.667)]  [G loss: -102.476] \n",
                        "4410 [D loss: (4.666)(R 4.664, F 4.667)]  [G loss: -102.487] \n",
                        "4411 [D loss: (4.666)(R 4.664, F 4.668)]  [G loss: -102.498] \n",
                        "4412 [D loss: (4.667)(R 4.665, F 4.669)]  [G loss: -102.509] \n",
                        "4413 [D loss: (4.667)(R 4.666, F 4.669)]  [G loss: -102.519] \n",
                        "4414 [D loss: (4.668)(R 4.666, F 4.670)]  [G loss: -102.530] \n",
                        "4415 [D loss: (4.669)(R 4.667, F 4.670)]  [G loss: -102.541] \n",
                        "4416 [D loss: (4.669)(R 4.667, F 4.671)]  [G loss: -102.552] \n",
                        "4417 [D loss: (4.670)(R 4.668, F 4.672)]  [G loss: -102.563] \n",
                        "4418 [D loss: (4.670)(R 4.669, F 4.672)]  [G loss: -102.574] \n",
                        "4419 [D loss: (4.671)(R 4.669, F 4.673)]  [G loss: -102.585] \n",
                        "4420 [D loss: (4.672)(R 4.670, F 4.674)]  [G loss: -102.597] \n",
                        "4421 [D loss: (4.673)(R 4.671, F 4.674)]  [G loss: -102.607] \n",
                        "4422 [D loss: (4.673)(R 4.671, F 4.675)]  [G loss: -102.618] \n",
                        "4423 [D loss: (4.674)(R 4.672, F 4.676)]  [G loss: -102.629] \n",
                        "4424 [D loss: (4.674)(R 4.673, F 4.676)]  [G loss: -102.639] \n",
                        "4425 [D loss: (4.675)(R 4.673, F 4.677)]  [G loss: -102.650] \n",
                        "4426 [D loss: (4.676)(R 4.674, F 4.677)]  [G loss: -102.660] \n",
                        "4427 [D loss: (4.676)(R 4.674, F 4.678)]  [G loss: -102.671] \n",
                        "4428 [D loss: (4.677)(R 4.675, F 4.678)]  [G loss: -102.681] \n",
                        "4429 [D loss: (4.677)(R 4.676, F 4.679)]  [G loss: -102.692] \n",
                        "4430 [D loss: (4.678)(R 4.676, F 4.680)]  [G loss: -102.703] \n",
                        "4431 [D loss: (4.678)(R 4.677, F 4.680)]  [G loss: -102.714] \n",
                        "4432 [D loss: (4.679)(R 4.677, F 4.681)]  [G loss: -102.725] \n",
                        "4433 [D loss: (4.680)(R 4.678, F 4.682)]  [G loss: -102.737] \n",
                        "4434 [D loss: (4.680)(R 4.679, F 4.682)]  [G loss: -102.748] \n",
                        "4435 [D loss: (4.681)(R 4.680, F 4.683)]  [G loss: -102.759] \n",
                        "4436 [D loss: (4.682)(R 4.680, F 4.684)]  [G loss: -102.769] \n",
                        "4437 [D loss: (4.683)(R 4.681, F 4.684)]  [G loss: -102.780] \n",
                        "4438 [D loss: (4.683)(R 4.681, F 4.685)]  [G loss: -102.790] \n",
                        "4439 [D loss: (4.684)(R 4.682, F 4.686)]  [G loss: -102.800] \n",
                        "4440 [D loss: (4.684)(R 4.682, F 4.686)]  [G loss: -102.810] \n",
                        "4441 [D loss: (4.685)(R 4.683, F 4.687)]  [G loss: -102.820] \n",
                        "4442 [D loss: (4.685)(R 4.684, F 4.687)]  [G loss: -102.830] \n",
                        "4443 [D loss: (4.686)(R 4.684, F 4.688)]  [G loss: -102.840] \n",
                        "4444 [D loss: (4.686)(R 4.685, F 4.688)]  [G loss: -102.851] \n",
                        "4445 [D loss: (4.687)(R 4.685, F 4.689)]  [G loss: -102.860] \n",
                        "4446 [D loss: (4.688)(R 4.686, F 4.689)]  [G loss: -102.870] \n",
                        "4447 [D loss: (4.688)(R 4.686, F 4.690)]  [G loss: -102.880] \n",
                        "4448 [D loss: (4.689)(R 4.687, F 4.690)]  [G loss: -102.890] \n",
                        "4449 [D loss: (4.689)(R 4.687, F 4.691)]  [G loss: -102.900] \n",
                        "4450 [D loss: (4.690)(R 4.688, F 4.691)]  [G loss: -102.910] \n",
                        "4451 [D loss: (4.690)(R 4.688, F 4.692)]  [G loss: -102.920] \n",
                        "4452 [D loss: (4.690)(R 4.689, F 4.692)]  [G loss: -102.930] \n",
                        "4453 [D loss: (4.691)(R 4.689, F 4.693)]  [G loss: -102.940] \n",
                        "4454 [D loss: (4.691)(R 4.690, F 4.693)]  [G loss: -102.950] \n",
                        "4455 [D loss: (4.692)(R 4.690, F 4.694)]  [G loss: -102.961] \n",
                        "4456 [D loss: (4.692)(R 4.691, F 4.694)]  [G loss: -102.972] \n",
                        "4457 [D loss: (4.693)(R 4.691, F 4.695)]  [G loss: -102.982] \n",
                        "4458 [D loss: (4.694)(R 4.692, F 4.696)]  [G loss: -102.992] \n",
                        "4459 [D loss: (4.694)(R 4.693, F 4.696)]  [G loss: -103.003] \n",
                        "4460 [D loss: (4.695)(R 4.693, F 4.697)]  [G loss: -103.013] \n",
                        "4461 [D loss: (4.695)(R 4.694, F 4.697)]  [G loss: -103.024] \n",
                        "4462 [D loss: (4.696)(R 4.694, F 4.698)]  [G loss: -103.034] \n",
                        "4463 [D loss: (4.696)(R 4.695, F 4.698)]  [G loss: -103.045] \n",
                        "4464 [D loss: (4.697)(R 4.695, F 4.699)]  [G loss: -103.055] \n",
                        "4465 [D loss: (4.698)(R 4.696, F 4.699)]  [G loss: -103.065] \n",
                        "4466 [D loss: (4.698)(R 4.696, F 4.700)]  [G loss: -103.075] \n",
                        "4467 [D loss: (4.699)(R 4.697, F 4.700)]  [G loss: -103.085] \n",
                        "4468 [D loss: (4.699)(R 4.697, F 4.701)]  [G loss: -103.096] \n",
                        "4469 [D loss: (4.700)(R 4.698, F 4.701)]  [G loss: -103.105] \n",
                        "4470 [D loss: (4.700)(R 4.698, F 4.702)]  [G loss: -103.116] \n",
                        "4471 [D loss: (4.700)(R 4.699, F 4.702)]  [G loss: -103.127] \n",
                        "4472 [D loss: (4.701)(R 4.699, F 4.703)]  [G loss: -103.138] \n",
                        "4473 [D loss: (4.702)(R 4.700, F 4.704)]  [G loss: -103.147] \n",
                        "4474 [D loss: (4.703)(R 4.701, F 4.704)]  [G loss: -103.158] \n",
                        "4475 [D loss: (4.703)(R 4.701, F 4.705)]  [G loss: -103.168] \n",
                        "4476 [D loss: (4.703)(R 4.702, F 4.705)]  [G loss: -103.179] \n",
                        "4477 [D loss: (4.704)(R 4.702, F 4.706)]  [G loss: -103.190] \n",
                        "4478 [D loss: (4.705)(R 4.703, F 4.706)]  [G loss: -103.202] \n",
                        "4479 [D loss: (4.705)(R 4.704, F 4.707)]  [G loss: -103.212] \n",
                        "4480 [D loss: (4.706)(R 4.704, F 4.708)]  [G loss: -103.222] \n",
                        "4481 [D loss: (4.706)(R 4.705, F 4.708)]  [G loss: -103.232] \n",
                        "4482 [D loss: (4.707)(R 4.705, F 4.709)]  [G loss: -103.242] \n",
                        "4483 [D loss: (4.707)(R 4.706, F 4.709)]  [G loss: -103.252] \n",
                        "4484 [D loss: (4.708)(R 4.706, F 4.709)]  [G loss: -103.263] \n",
                        "4485 [D loss: (4.708)(R 4.706, F 4.710)]  [G loss: -103.274] \n",
                        "4486 [D loss: (4.709)(R 4.707, F 4.710)]  [G loss: -103.285] \n",
                        "4487 [D loss: (4.709)(R 4.708, F 4.711)]  [G loss: -103.297] \n",
                        "4488 [D loss: (4.710)(R 4.708, F 4.712)]  [G loss: -103.307] \n",
                        "4489 [D loss: (4.711)(R 4.709, F 4.713)]  [G loss: -103.318] \n",
                        "4490 [D loss: (4.711)(R 4.710, F 4.713)]  [G loss: -103.329] \n",
                        "4491 [D loss: (4.712)(R 4.710, F 4.714)]  [G loss: -103.338] \n",
                        "4492 [D loss: (4.712)(R 4.711, F 4.714)]  [G loss: -103.348] \n",
                        "4493 [D loss: (4.713)(R 4.711, F 4.715)]  [G loss: -103.358] \n",
                        "4494 [D loss: (4.713)(R 4.712, F 4.715)]  [G loss: -103.369] \n",
                        "4495 [D loss: (4.714)(R 4.712, F 4.716)]  [G loss: -103.379] \n",
                        "4496 [D loss: (4.714)(R 4.713, F 4.716)]  [G loss: -103.390] \n",
                        "4497 [D loss: (4.715)(R 4.713, F 4.717)]  [G loss: -103.400] \n",
                        "4498 [D loss: (4.716)(R 4.714, F 4.717)]  [G loss: -103.411] \n",
                        "4499 [D loss: (4.716)(R 4.714, F 4.718)]  [G loss: -103.421] \n",
                        "4500 [D loss: (4.717)(R 4.715, F 4.718)]  [G loss: -103.432] \n",
                        "4501 [D loss: (4.717)(R 4.716, F 4.719)]  [G loss: -103.442] \n",
                        "4502 [D loss: (4.718)(R 4.716, F 4.720)]  [G loss: -103.452] \n",
                        "4503 [D loss: (4.718)(R 4.716, F 4.720)]  [G loss: -103.462] \n",
                        "4504 [D loss: (4.719)(R 4.717, F 4.721)]  [G loss: -103.473] \n",
                        "4505 [D loss: (4.719)(R 4.718, F 4.721)]  [G loss: -103.483] \n",
                        "4506 [D loss: (4.720)(R 4.718, F 4.722)]  [G loss: -103.495] \n",
                        "4507 [D loss: (4.721)(R 4.719, F 4.722)]  [G loss: -103.506] \n",
                        "4508 [D loss: (4.721)(R 4.719, F 4.723)]  [G loss: -103.518] \n",
                        "4509 [D loss: (4.722)(R 4.720, F 4.724)]  [G loss: -103.529] \n",
                        "4510 [D loss: (4.723)(R 4.721, F 4.725)]  [G loss: -103.539] \n",
                        "4511 [D loss: (4.723)(R 4.722, F 4.725)]  [G loss: -103.549] \n",
                        "4512 [D loss: (4.724)(R 4.722, F 4.726)]  [G loss: -103.560] \n",
                        "4513 [D loss: (4.724)(R 4.723, F 4.726)]  [G loss: -103.569] \n",
                        "4514 [D loss: (4.725)(R 4.723, F 4.727)]  [G loss: -103.579] \n",
                        "4515 [D loss: (4.725)(R 4.724, F 4.727)]  [G loss: -103.589] \n",
                        "4516 [D loss: (4.726)(R 4.724, F 4.728)]  [G loss: -103.598] \n",
                        "4517 [D loss: (4.726)(R 4.725, F 4.728)]  [G loss: -103.608] \n",
                        "4518 [D loss: (4.727)(R 4.725, F 4.728)]  [G loss: -103.617] \n",
                        "4519 [D loss: (4.727)(R 4.725, F 4.729)]  [G loss: -103.627] \n",
                        "4520 [D loss: (4.727)(R 4.726, F 4.729)]  [G loss: -103.637] \n",
                        "4521 [D loss: (4.728)(R 4.726, F 4.730)]  [G loss: -103.647] \n",
                        "4522 [D loss: (4.729)(R 4.727, F 4.730)]  [G loss: -103.656] \n",
                        "4523 [D loss: (4.729)(R 4.727, F 4.731)]  [G loss: -103.666] \n",
                        "4524 [D loss: (4.730)(R 4.728, F 4.731)]  [G loss: -103.675] \n",
                        "4525 [D loss: (4.730)(R 4.728, F 4.731)]  [G loss: -103.684] \n",
                        "4526 [D loss: (4.730)(R 4.728, F 4.732)]  [G loss: -103.694] \n",
                        "4527 [D loss: (4.730)(R 4.729, F 4.732)]  [G loss: -103.704] \n",
                        "4528 [D loss: (4.731)(R 4.729, F 4.733)]  [G loss: -103.716] \n",
                        "4529 [D loss: (4.732)(R 4.730, F 4.733)]  [G loss: -103.727] \n",
                        "4530 [D loss: (4.732)(R 4.730, F 4.734)]  [G loss: -103.739] \n",
                        "4531 [D loss: (4.733)(R 4.731, F 4.735)]  [G loss: -103.750] \n",
                        "4532 [D loss: (4.734)(R 4.732, F 4.736)]  [G loss: -103.760] \n",
                        "4533 [D loss: (4.735)(R 4.733, F 4.736)]  [G loss: -103.769] \n",
                        "4534 [D loss: (4.735)(R 4.733, F 4.737)]  [G loss: -103.779] \n",
                        "4535 [D loss: (4.735)(R 4.734, F 4.737)]  [G loss: -103.788] \n",
                        "4536 [D loss: (4.736)(R 4.734, F 4.738)]  [G loss: -103.798] \n",
                        "4537 [D loss: (4.736)(R 4.735, F 4.738)]  [G loss: -103.808] \n",
                        "4538 [D loss: (4.737)(R 4.735, F 4.738)]  [G loss: -103.817] \n",
                        "4539 [D loss: (4.737)(R 4.735, F 4.739)]  [G loss: -103.827] \n",
                        "4540 [D loss: (4.737)(R 4.736, F 4.739)]  [G loss: -103.837] \n",
                        "4541 [D loss: (4.738)(R 4.736, F 4.740)]  [G loss: -103.847] \n",
                        "4542 [D loss: (4.738)(R 4.737, F 4.740)]  [G loss: -103.857] \n",
                        "4543 [D loss: (4.739)(R 4.737, F 4.741)]  [G loss: -103.867] \n",
                        "4544 [D loss: (4.740)(R 4.738, F 4.741)]  [G loss: -103.878] \n",
                        "4545 [D loss: (4.740)(R 4.738, F 4.742)]  [G loss: -103.888] \n",
                        "4546 [D loss: (4.741)(R 4.739, F 4.742)]  [G loss: -103.898] \n",
                        "4547 [D loss: (4.741)(R 4.739, F 4.743)]  [G loss: -103.907] \n",
                        "4548 [D loss: (4.742)(R 4.740, F 4.743)]  [G loss: -103.918] \n",
                        "4549 [D loss: (4.742)(R 4.740, F 4.744)]  [G loss: -103.928] \n",
                        "4550 [D loss: (4.742)(R 4.741, F 4.744)]  [G loss: -103.939] \n",
                        "4551 [D loss: (4.743)(R 4.741, F 4.745)]  [G loss: -103.948] \n",
                        "4552 [D loss: (4.744)(R 4.742, F 4.745)]  [G loss: -103.959] \n",
                        "4553 [D loss: (4.744)(R 4.742, F 4.746)]  [G loss: -103.969] \n",
                        "4554 [D loss: (4.745)(R 4.743, F 4.746)]  [G loss: -103.980] \n",
                        "4555 [D loss: (4.745)(R 4.744, F 4.747)]  [G loss: -103.990] \n",
                        "4556 [D loss: (4.746)(R 4.744, F 4.747)]  [G loss: -104.000] \n",
                        "4557 [D loss: (4.746)(R 4.744, F 4.748)]  [G loss: -104.009] \n",
                        "4558 [D loss: (4.747)(R 4.745, F 4.748)]  [G loss: -104.019] \n",
                        "4559 [D loss: (4.747)(R 4.745, F 4.749)]  [G loss: -104.029] \n",
                        "4560 [D loss: (4.747)(R 4.746, F 4.749)]  [G loss: -104.039] \n",
                        "4561 [D loss: (4.748)(R 4.746, F 4.750)]  [G loss: -104.049] \n",
                        "4562 [D loss: (4.748)(R 4.747, F 4.750)]  [G loss: -104.059] \n",
                        "4563 [D loss: (4.749)(R 4.747, F 4.751)]  [G loss: -104.070] \n",
                        "4564 [D loss: (4.750)(R 4.748, F 4.751)]  [G loss: -104.079] \n",
                        "4565 [D loss: (4.750)(R 4.748, F 4.752)]  [G loss: -104.088] \n",
                        "4566 [D loss: (4.751)(R 4.749, F 4.752)]  [G loss: -104.098] \n",
                        "4567 [D loss: (4.751)(R 4.749, F 4.753)]  [G loss: -104.107] \n",
                        "4568 [D loss: (4.751)(R 4.749, F 4.753)]  [G loss: -104.116] \n",
                        "4569 [D loss: (4.751)(R 4.750, F 4.753)]  [G loss: -104.126] \n",
                        "4570 [D loss: (4.752)(R 4.750, F 4.753)]  [G loss: -104.137] \n",
                        "4571 [D loss: (4.752)(R 4.750, F 4.754)]  [G loss: -104.148] \n",
                        "4572 [D loss: (4.753)(R 4.751, F 4.755)]  [G loss: -104.159] \n",
                        "4573 [D loss: (4.754)(R 4.752, F 4.755)]  [G loss: -104.171] \n",
                        "4574 [D loss: (4.754)(R 4.753, F 4.756)]  [G loss: -104.182] \n",
                        "4575 [D loss: (4.755)(R 4.753, F 4.757)]  [G loss: -104.192] \n",
                        "4576 [D loss: (4.756)(R 4.754, F 4.757)]  [G loss: -104.200] \n",
                        "4577 [D loss: (4.756)(R 4.754, F 4.758)]  [G loss: -104.209] \n",
                        "4578 [D loss: (4.756)(R 4.755, F 4.758)]  [G loss: -104.218] \n",
                        "4579 [D loss: (4.757)(R 4.755, F 4.758)]  [G loss: -104.227] \n",
                        "4580 [D loss: (4.757)(R 4.755, F 4.759)]  [G loss: -104.236] \n",
                        "4581 [D loss: (4.758)(R 4.756, F 4.759)]  [G loss: -104.246] \n",
                        "4582 [D loss: (4.758)(R 4.756, F 4.760)]  [G loss: -104.255] \n",
                        "4583 [D loss: (4.758)(R 4.757, F 4.760)]  [G loss: -104.266] \n",
                        "4584 [D loss: (4.759)(R 4.757, F 4.761)]  [G loss: -104.276] \n",
                        "4585 [D loss: (4.759)(R 4.758, F 4.761)]  [G loss: -104.286] \n",
                        "4586 [D loss: (4.760)(R 4.758, F 4.761)]  [G loss: -104.296] \n",
                        "4587 [D loss: (4.760)(R 4.758, F 4.762)]  [G loss: -104.307] \n",
                        "4588 [D loss: (4.761)(R 4.759, F 4.763)]  [G loss: -104.317] \n",
                        "4589 [D loss: (4.761)(R 4.759, F 4.763)]  [G loss: -104.329] \n",
                        "4590 [D loss: (4.762)(R 4.760, F 4.764)]  [G loss: -104.339] \n",
                        "4591 [D loss: (4.763)(R 4.761, F 4.764)]  [G loss: -104.350] \n",
                        "4592 [D loss: (4.763)(R 4.762, F 4.765)]  [G loss: -104.360] \n",
                        "4593 [D loss: (4.764)(R 4.762, F 4.765)]  [G loss: -104.370] \n",
                        "4594 [D loss: (4.764)(R 4.762, F 4.766)]  [G loss: -104.380] \n",
                        "4595 [D loss: (4.765)(R 4.763, F 4.766)]  [G loss: -104.389] \n",
                        "4596 [D loss: (4.765)(R 4.763, F 4.767)]  [G loss: -104.399] \n",
                        "4597 [D loss: (4.766)(R 4.764, F 4.767)]  [G loss: -104.409] \n",
                        "4598 [D loss: (4.766)(R 4.764, F 4.768)]  [G loss: -104.418] \n",
                        "4599 [D loss: (4.767)(R 4.765, F 4.768)]  [G loss: -104.428] \n",
                        "4600 [D loss: (4.767)(R 4.765, F 4.769)]  [G loss: -104.438] \n",
                        "4601 [D loss: (4.768)(R 4.766, F 4.769)]  [G loss: -104.448] \n",
                        "4602 [D loss: (4.768)(R 4.766, F 4.770)]  [G loss: -104.458] \n",
                        "4603 [D loss: (4.769)(R 4.767, F 4.770)]  [G loss: -104.468] \n",
                        "4604 [D loss: (4.769)(R 4.767, F 4.771)]  [G loss: -104.478] \n",
                        "4605 [D loss: (4.770)(R 4.768, F 4.771)]  [G loss: -104.489] \n",
                        "4606 [D loss: (4.770)(R 4.768, F 4.772)]  [G loss: -104.500] \n",
                        "4607 [D loss: (4.771)(R 4.769, F 4.772)]  [G loss: -104.510] \n",
                        "4608 [D loss: (4.771)(R 4.769, F 4.773)]  [G loss: -104.522] \n",
                        "4609 [D loss: (4.772)(R 4.770, F 4.774)]  [G loss: -104.533] \n",
                        "4610 [D loss: (4.773)(R 4.771, F 4.774)]  [G loss: -104.543] \n",
                        "4611 [D loss: (4.773)(R 4.772, F 4.775)]  [G loss: -104.553] \n",
                        "4612 [D loss: (4.774)(R 4.772, F 4.776)]  [G loss: -104.563] \n",
                        "4613 [D loss: (4.774)(R 4.773, F 4.776)]  [G loss: -104.573] \n",
                        "4614 [D loss: (4.775)(R 4.773, F 4.776)]  [G loss: -104.583] \n",
                        "4615 [D loss: (4.775)(R 4.774, F 4.777)]  [G loss: -104.592] \n",
                        "4616 [D loss: (4.776)(R 4.774, F 4.778)]  [G loss: -104.603] \n",
                        "4617 [D loss: (4.776)(R 4.775, F 4.778)]  [G loss: -104.613] \n",
                        "4618 [D loss: (4.777)(R 4.775, F 4.779)]  [G loss: -104.624] \n",
                        "4619 [D loss: (4.778)(R 4.776, F 4.779)]  [G loss: -104.634] \n",
                        "4620 [D loss: (4.778)(R 4.776, F 4.780)]  [G loss: -104.644] \n",
                        "4621 [D loss: (4.779)(R 4.777, F 4.780)]  [G loss: -104.654] \n",
                        "4622 [D loss: (4.779)(R 4.777, F 4.781)]  [G loss: -104.664] \n",
                        "4623 [D loss: (4.780)(R 4.778, F 4.782)]  [G loss: -104.674] \n",
                        "4624 [D loss: (4.780)(R 4.779, F 4.782)]  [G loss: -104.685] \n",
                        "4625 [D loss: (4.781)(R 4.779, F 4.783)]  [G loss: -104.695] \n",
                        "4626 [D loss: (4.782)(R 4.780, F 4.783)]  [G loss: -104.705] \n",
                        "4627 [D loss: (4.782)(R 4.780, F 4.784)]  [G loss: -104.715] \n",
                        "4628 [D loss: (4.783)(R 4.781, F 4.784)]  [G loss: -104.723] \n",
                        "4629 [D loss: (4.783)(R 4.781, F 4.785)]  [G loss: -104.733] \n",
                        "4630 [D loss: (4.783)(R 4.782, F 4.785)]  [G loss: -104.744] \n",
                        "4631 [D loss: (4.784)(R 4.782, F 4.786)]  [G loss: -104.754] \n",
                        "4632 [D loss: (4.785)(R 4.783, F 4.786)]  [G loss: -104.764] \n",
                        "4633 [D loss: (4.785)(R 4.783, F 4.787)]  [G loss: -104.775] \n",
                        "4634 [D loss: (4.786)(R 4.784, F 4.788)]  [G loss: -104.786] \n",
                        "4635 [D loss: (4.786)(R 4.785, F 4.788)]  [G loss: -104.797] \n",
                        "4636 [D loss: (4.787)(R 4.785, F 4.789)]  [G loss: -104.808] \n",
                        "4637 [D loss: (4.788)(R 4.786, F 4.790)]  [G loss: -104.818] \n",
                        "4638 [D loss: (4.789)(R 4.787, F 4.790)]  [G loss: -104.827] \n",
                        "4639 [D loss: (4.789)(R 4.787, F 4.791)]  [G loss: -104.837] \n",
                        "4640 [D loss: (4.790)(R 4.788, F 4.791)]  [G loss: -104.847] \n",
                        "4641 [D loss: (4.790)(R 4.789, F 4.792)]  [G loss: -104.857] \n",
                        "4642 [D loss: (4.791)(R 4.789, F 4.792)]  [G loss: -104.866] \n",
                        "4643 [D loss: (4.791)(R 4.789, F 4.793)]  [G loss: -104.876] \n",
                        "4644 [D loss: (4.792)(R 4.790, F 4.793)]  [G loss: -104.885] \n",
                        "4645 [D loss: (4.792)(R 4.790, F 4.794)]  [G loss: -104.895] \n",
                        "4646 [D loss: (4.792)(R 4.791, F 4.794)]  [G loss: -104.905] \n",
                        "4647 [D loss: (4.793)(R 4.791, F 4.795)]  [G loss: -104.915] \n",
                        "4648 [D loss: (4.793)(R 4.792, F 4.795)]  [G loss: -104.926] \n",
                        "4649 [D loss: (4.794)(R 4.792, F 4.796)]  [G loss: -104.937] \n",
                        "4650 [D loss: (4.795)(R 4.793, F 4.796)]  [G loss: -104.947] \n",
                        "4651 [D loss: (4.795)(R 4.793, F 4.797)]  [G loss: -104.958] \n",
                        "4652 [D loss: (4.796)(R 4.794, F 4.797)]  [G loss: -104.968] \n",
                        "4653 [D loss: (4.796)(R 4.794, F 4.798)]  [G loss: -104.978] \n",
                        "4654 [D loss: (4.797)(R 4.795, F 4.799)]  [G loss: -104.988] \n",
                        "4655 [D loss: (4.797)(R 4.796, F 4.799)]  [G loss: -104.998] \n",
                        "4656 [D loss: (4.798)(R 4.796, F 4.799)]  [G loss: -105.008] \n",
                        "4657 [D loss: (4.798)(R 4.797, F 4.800)]  [G loss: -105.019] \n",
                        "4658 [D loss: (4.799)(R 4.797, F 4.801)]  [G loss: -105.029] \n",
                        "4659 [D loss: (4.799)(R 4.798, F 4.801)]  [G loss: -105.040] \n",
                        "4660 [D loss: (4.800)(R 4.798, F 4.802)]  [G loss: -105.050] \n",
                        "4661 [D loss: (4.801)(R 4.799, F 4.802)]  [G loss: -105.061] \n",
                        "4662 [D loss: (4.801)(R 4.799, F 4.803)]  [G loss: -105.072] \n",
                        "4663 [D loss: (4.802)(R 4.800, F 4.804)]  [G loss: -105.082] \n",
                        "4664 [D loss: (4.802)(R 4.801, F 4.804)]  [G loss: -105.093] \n",
                        "4665 [D loss: (4.803)(R 4.801, F 4.805)]  [G loss: -105.104] \n",
                        "4666 [D loss: (4.804)(R 4.802, F 4.806)]  [G loss: -105.114] \n",
                        "4667 [D loss: (4.804)(R 4.803, F 4.806)]  [G loss: -105.125] \n",
                        "4668 [D loss: (4.805)(R 4.803, F 4.807)]  [G loss: -105.135] \n",
                        "4669 [D loss: (4.806)(R 4.804, F 4.807)]  [G loss: -105.145] \n",
                        "4670 [D loss: (4.806)(R 4.805, F 4.808)]  [G loss: -105.155] \n",
                        "4671 [D loss: (4.807)(R 4.805, F 4.808)]  [G loss: -105.166] \n",
                        "4672 [D loss: (4.807)(R 4.806, F 4.809)]  [G loss: -105.175] \n",
                        "4673 [D loss: (4.808)(R 4.806, F 4.809)]  [G loss: -105.185] \n",
                        "4674 [D loss: (4.808)(R 4.807, F 4.810)]  [G loss: -105.195] \n",
                        "4675 [D loss: (4.809)(R 4.807, F 4.811)]  [G loss: -105.205] \n",
                        "4676 [D loss: (4.809)(R 4.808, F 4.811)]  [G loss: -105.214] \n",
                        "4677 [D loss: (4.810)(R 4.808, F 4.812)]  [G loss: -105.224] \n",
                        "4678 [D loss: (4.810)(R 4.809, F 4.812)]  [G loss: -105.234] \n",
                        "4679 [D loss: (4.811)(R 4.809, F 4.813)]  [G loss: -105.245] \n",
                        "4680 [D loss: (4.812)(R 4.810, F 4.813)]  [G loss: -105.254] \n",
                        "4681 [D loss: (4.812)(R 4.810, F 4.814)]  [G loss: -105.265] \n",
                        "4682 [D loss: (4.812)(R 4.811, F 4.814)]  [G loss: -105.275] \n",
                        "4683 [D loss: (4.813)(R 4.811, F 4.815)]  [G loss: -105.285] \n",
                        "4684 [D loss: (4.814)(R 4.812, F 4.815)]  [G loss: -105.295] \n",
                        "4685 [D loss: (4.814)(R 4.812, F 4.816)]  [G loss: -105.305] \n",
                        "4686 [D loss: (4.815)(R 4.813, F 4.816)]  [G loss: -105.316] \n",
                        "4687 [D loss: (4.815)(R 4.814, F 4.817)]  [G loss: -105.325] \n",
                        "4688 [D loss: (4.816)(R 4.814, F 4.817)]  [G loss: -105.335] \n",
                        "4689 [D loss: (4.816)(R 4.815, F 4.818)]  [G loss: -105.345] \n",
                        "4690 [D loss: (4.817)(R 4.815, F 4.818)]  [G loss: -105.355] \n",
                        "4691 [D loss: (4.817)(R 4.816, F 4.819)]  [G loss: -105.365] \n",
                        "4692 [D loss: (4.818)(R 4.816, F 4.820)]  [G loss: -105.375] \n",
                        "4693 [D loss: (4.818)(R 4.816, F 4.820)]  [G loss: -105.385] \n",
                        "4694 [D loss: (4.819)(R 4.817, F 4.821)]  [G loss: -105.395] \n",
                        "4695 [D loss: (4.819)(R 4.818, F 4.821)]  [G loss: -105.405] \n",
                        "4696 [D loss: (4.820)(R 4.818, F 4.822)]  [G loss: -105.415] \n",
                        "4697 [D loss: (4.820)(R 4.819, F 4.822)]  [G loss: -105.426] \n",
                        "4698 [D loss: (4.821)(R 4.819, F 4.823)]  [G loss: -105.436] \n",
                        "4699 [D loss: (4.822)(R 4.820, F 4.823)]  [G loss: -105.446] \n",
                        "4700 [D loss: (4.822)(R 4.820, F 4.824)]  [G loss: -105.456] \n",
                        "4701 [D loss: (4.823)(R 4.821, F 4.824)]  [G loss: -105.466] \n",
                        "4702 [D loss: (4.823)(R 4.822, F 4.825)]  [G loss: -105.477] \n",
                        "4703 [D loss: (4.824)(R 4.822, F 4.825)]  [G loss: -105.487] \n",
                        "4704 [D loss: (4.824)(R 4.823, F 4.826)]  [G loss: -105.497] \n",
                        "4705 [D loss: (4.825)(R 4.823, F 4.827)]  [G loss: -105.508] \n",
                        "4706 [D loss: (4.825)(R 4.824, F 4.827)]  [G loss: -105.518] \n",
                        "4707 [D loss: (4.826)(R 4.824, F 4.828)]  [G loss: -105.528] \n",
                        "4708 [D loss: (4.827)(R 4.825, F 4.828)]  [G loss: -105.538] \n",
                        "4709 [D loss: (4.827)(R 4.825, F 4.829)]  [G loss: -105.549] \n",
                        "4710 [D loss: (4.828)(R 4.826, F 4.829)]  [G loss: -105.559] \n",
                        "4711 [D loss: (4.828)(R 4.826, F 4.830)]  [G loss: -105.569] \n",
                        "4712 [D loss: (4.829)(R 4.827, F 4.831)]  [G loss: -105.580] \n",
                        "4713 [D loss: (4.829)(R 4.828, F 4.831)]  [G loss: -105.590] \n",
                        "4714 [D loss: (4.830)(R 4.828, F 4.832)]  [G loss: -105.600] \n",
                        "4715 [D loss: (4.831)(R 4.829, F 4.832)]  [G loss: -105.611] \n",
                        "4716 [D loss: (4.831)(R 4.830, F 4.833)]  [G loss: -105.621] \n",
                        "4717 [D loss: (4.832)(R 4.830, F 4.834)]  [G loss: -105.631] \n",
                        "4718 [D loss: (4.832)(R 4.831, F 4.834)]  [G loss: -105.641] \n",
                        "4719 [D loss: (4.833)(R 4.831, F 4.835)]  [G loss: -105.651] \n",
                        "4720 [D loss: (4.833)(R 4.832, F 4.835)]  [G loss: -105.662] \n",
                        "4721 [D loss: (4.834)(R 4.832, F 4.836)]  [G loss: -105.672] \n",
                        "4722 [D loss: (4.835)(R 4.833, F 4.836)]  [G loss: -105.682] \n",
                        "4723 [D loss: (4.835)(R 4.834, F 4.837)]  [G loss: -105.692] \n",
                        "4724 [D loss: (4.836)(R 4.834, F 4.837)]  [G loss: -105.703] \n",
                        "4725 [D loss: (4.836)(R 4.835, F 4.838)]  [G loss: -105.713] \n",
                        "4726 [D loss: (4.837)(R 4.835, F 4.839)]  [G loss: -105.723] \n",
                        "4727 [D loss: (4.837)(R 4.836, F 4.839)]  [G loss: -105.733] \n",
                        "4728 [D loss: (4.838)(R 4.836, F 4.840)]  [G loss: -105.743] \n",
                        "4729 [D loss: (4.839)(R 4.837, F 4.840)]  [G loss: -105.753] \n",
                        "4730 [D loss: (4.839)(R 4.837, F 4.841)]  [G loss: -105.763] \n",
                        "4731 [D loss: (4.840)(R 4.838, F 4.841)]  [G loss: -105.774] \n",
                        "4732 [D loss: (4.840)(R 4.839, F 4.842)]  [G loss: -105.784] \n",
                        "4733 [D loss: (4.841)(R 4.839, F 4.843)]  [G loss: -105.793] \n",
                        "4734 [D loss: (4.841)(R 4.840, F 4.843)]  [G loss: -105.803] \n",
                        "4735 [D loss: (4.842)(R 4.840, F 4.844)]  [G loss: -105.813] \n",
                        "4736 [D loss: (4.843)(R 4.841, F 4.844)]  [G loss: -105.823] \n",
                        "4737 [D loss: (4.843)(R 4.841, F 4.845)]  [G loss: -105.833] \n",
                        "4738 [D loss: (4.844)(R 4.842, F 4.845)]  [G loss: -105.843] \n",
                        "4739 [D loss: (4.844)(R 4.843, F 4.846)]  [G loss: -105.852] \n",
                        "4740 [D loss: (4.845)(R 4.843, F 4.846)]  [G loss: -105.862] \n",
                        "4741 [D loss: (4.845)(R 4.843, F 4.847)]  [G loss: -105.873] \n",
                        "4742 [D loss: (4.846)(R 4.844, F 4.848)]  [G loss: -105.882] \n",
                        "4743 [D loss: (4.846)(R 4.845, F 4.848)]  [G loss: -105.892] \n",
                        "4744 [D loss: (4.847)(R 4.845, F 4.849)]  [G loss: -105.902] \n",
                        "4745 [D loss: (4.847)(R 4.846, F 4.849)]  [G loss: -105.911] \n",
                        "4746 [D loss: (4.848)(R 4.846, F 4.850)]  [G loss: -105.921] \n",
                        "4747 [D loss: (4.848)(R 4.847, F 4.850)]  [G loss: -105.931] \n",
                        "4748 [D loss: (4.849)(R 4.847, F 4.851)]  [G loss: -105.941] \n",
                        "4749 [D loss: (4.849)(R 4.848, F 4.851)]  [G loss: -105.951] \n",
                        "4750 [D loss: (4.850)(R 4.848, F 4.852)]  [G loss: -105.960] \n",
                        "4751 [D loss: (4.850)(R 4.849, F 4.852)]  [G loss: -105.971] \n",
                        "4752 [D loss: (4.851)(R 4.850, F 4.853)]  [G loss: -105.981] \n",
                        "4753 [D loss: (4.852)(R 4.850, F 4.854)]  [G loss: -105.991] \n",
                        "4754 [D loss: (4.852)(R 4.851, F 4.854)]  [G loss: -106.001] \n",
                        "4755 [D loss: (4.853)(R 4.851, F 4.855)]  [G loss: -106.011] \n",
                        "4756 [D loss: (4.854)(R 4.852, F 4.855)]  [G loss: -106.021] \n",
                        "4757 [D loss: (4.854)(R 4.852, F 4.856)]  [G loss: -106.031] \n",
                        "4758 [D loss: (4.855)(R 4.853, F 4.856)]  [G loss: -106.041] \n",
                        "4759 [D loss: (4.855)(R 4.854, F 4.857)]  [G loss: -106.051] \n",
                        "4760 [D loss: (4.856)(R 4.854, F 4.857)]  [G loss: -106.061] \n",
                        "4761 [D loss: (4.856)(R 4.855, F 4.858)]  [G loss: -106.071] \n",
                        "4762 [D loss: (4.857)(R 4.855, F 4.858)]  [G loss: -106.081] \n",
                        "4763 [D loss: (4.858)(R 4.856, F 4.859)]  [G loss: -106.090] \n",
                        "4764 [D loss: (4.858)(R 4.856, F 4.860)]  [G loss: -106.100] \n",
                        "4765 [D loss: (4.858)(R 4.857, F 4.860)]  [G loss: -106.110] \n",
                        "4766 [D loss: (4.859)(R 4.857, F 4.861)]  [G loss: -106.120] \n",
                        "4767 [D loss: (4.860)(R 4.858, F 4.861)]  [G loss: -106.130] \n",
                        "4768 [D loss: (4.860)(R 4.858, F 4.862)]  [G loss: -106.140] \n",
                        "4769 [D loss: (4.861)(R 4.859, F 4.862)]  [G loss: -106.149] \n",
                        "4770 [D loss: (4.861)(R 4.859, F 4.863)]  [G loss: -106.160] \n",
                        "4771 [D loss: (4.862)(R 4.860, F 4.863)]  [G loss: -106.169] \n",
                        "4772 [D loss: (4.862)(R 4.861, F 4.864)]  [G loss: -106.179] \n",
                        "4773 [D loss: (4.863)(R 4.861, F 4.864)]  [G loss: -106.190] \n",
                        "4774 [D loss: (4.863)(R 4.862, F 4.865)]  [G loss: -106.200] \n",
                        "4775 [D loss: (4.864)(R 4.862, F 4.865)]  [G loss: -106.209] \n",
                        "4776 [D loss: (4.865)(R 4.863, F 4.866)]  [G loss: -106.219] \n",
                        "4777 [D loss: (4.865)(R 4.863, F 4.867)]  [G loss: -106.229] \n",
                        "4778 [D loss: (4.866)(R 4.864, F 4.867)]  [G loss: -106.239] \n",
                        "4779 [D loss: (4.866)(R 4.864, F 4.868)]  [G loss: -106.250] \n",
                        "4780 [D loss: (4.867)(R 4.865, F 4.869)]  [G loss: -106.260] \n",
                        "4781 [D loss: (4.867)(R 4.866, F 4.869)]  [G loss: -106.270] \n",
                        "4782 [D loss: (4.868)(R 4.866, F 4.870)]  [G loss: -106.281] \n",
                        "4783 [D loss: (4.869)(R 4.867, F 4.870)]  [G loss: -106.291] \n",
                        "4784 [D loss: (4.869)(R 4.868, F 4.871)]  [G loss: -106.301] \n",
                        "4785 [D loss: (4.870)(R 4.868, F 4.872)]  [G loss: -106.312] \n",
                        "4786 [D loss: (4.871)(R 4.869, F 4.872)]  [G loss: -106.322] \n",
                        "4787 [D loss: (4.871)(R 4.869, F 4.873)]  [G loss: -106.333] \n",
                        "4788 [D loss: (4.872)(R 4.870, F 4.874)]  [G loss: -106.344] \n",
                        "4789 [D loss: (4.873)(R 4.871, F 4.874)]  [G loss: -106.354] \n",
                        "4790 [D loss: (4.873)(R 4.871, F 4.875)]  [G loss: -106.364] \n",
                        "4791 [D loss: (4.874)(R 4.872, F 4.875)]  [G loss: -106.375] \n",
                        "4792 [D loss: (4.875)(R 4.873, F 4.876)]  [G loss: -106.384] \n",
                        "4793 [D loss: (4.875)(R 4.874, F 4.877)]  [G loss: -106.394] \n",
                        "4794 [D loss: (4.876)(R 4.874, F 4.877)]  [G loss: -106.405] \n",
                        "4795 [D loss: (4.877)(R 4.875, F 4.878)]  [G loss: -106.413] \n",
                        "4796 [D loss: (4.877)(R 4.875, F 4.879)]  [G loss: -106.422] \n",
                        "4797 [D loss: (4.877)(R 4.876, F 4.879)]  [G loss: -106.432] \n",
                        "4798 [D loss: (4.878)(R 4.876, F 4.880)]  [G loss: -106.441] \n",
                        "4799 [D loss: (4.879)(R 4.877, F 4.880)]  [G loss: -106.450] \n",
                        "4800 [D loss: (4.879)(R 4.877, F 4.881)]  [G loss: -106.459] \n",
                        "4801 [D loss: (4.879)(R 4.878, F 4.881)]  [G loss: -106.467] \n",
                        "4802 [D loss: (4.880)(R 4.878, F 4.881)]  [G loss: -106.477] \n",
                        "4803 [D loss: (4.880)(R 4.879, F 4.882)]  [G loss: -106.485] \n",
                        "4804 [D loss: (4.881)(R 4.879, F 4.882)]  [G loss: -106.493] \n",
                        "4805 [D loss: (4.881)(R 4.879, F 4.883)]  [G loss: -106.503] \n",
                        "4806 [D loss: (4.882)(R 4.880, F 4.883)]  [G loss: -106.512] \n",
                        "4807 [D loss: (4.882)(R 4.880, F 4.884)]  [G loss: -106.521] \n",
                        "4808 [D loss: (4.882)(R 4.881, F 4.884)]  [G loss: -106.530] \n",
                        "4809 [D loss: (4.883)(R 4.881, F 4.884)]  [G loss: -106.539] \n",
                        "4810 [D loss: (4.883)(R 4.882, F 4.885)]  [G loss: -106.549] \n",
                        "4811 [D loss: (4.884)(R 4.882, F 4.885)]  [G loss: -106.558] \n",
                        "4812 [D loss: (4.884)(R 4.882, F 4.886)]  [G loss: -106.567] \n",
                        "4813 [D loss: (4.884)(R 4.883, F 4.886)]  [G loss: -106.577] \n",
                        "4814 [D loss: (4.885)(R 4.883, F 4.887)]  [G loss: -106.587] \n",
                        "4815 [D loss: (4.885)(R 4.884, F 4.887)]  [G loss: -106.597] \n",
                        "4816 [D loss: (4.886)(R 4.884, F 4.888)]  [G loss: -106.607] \n",
                        "4817 [D loss: (4.886)(R 4.885, F 4.888)]  [G loss: -106.616] \n",
                        "4818 [D loss: (4.887)(R 4.885, F 4.889)]  [G loss: -106.626] \n",
                        "4819 [D loss: (4.887)(R 4.886, F 4.889)]  [G loss: -106.636] \n",
                        "4820 [D loss: (4.888)(R 4.886, F 4.890)]  [G loss: -106.646] \n",
                        "4821 [D loss: (4.888)(R 4.887, F 4.890)]  [G loss: -106.656] \n",
                        "4822 [D loss: (4.889)(R 4.887, F 4.891)]  [G loss: -106.666] \n",
                        "4823 [D loss: (4.890)(R 4.888, F 4.891)]  [G loss: -106.676] \n",
                        "4824 [D loss: (4.890)(R 4.888, F 4.892)]  [G loss: -106.687] \n",
                        "4825 [D loss: (4.891)(R 4.889, F 4.892)]  [G loss: -106.697] \n",
                        "4826 [D loss: (4.891)(R 4.890, F 4.893)]  [G loss: -106.707] \n",
                        "4827 [D loss: (4.892)(R 4.890, F 4.894)]  [G loss: -106.718] \n",
                        "4828 [D loss: (4.892)(R 4.891, F 4.894)]  [G loss: -106.728] \n",
                        "4829 [D loss: (4.893)(R 4.891, F 4.895)]  [G loss: -106.740] \n",
                        "4830 [D loss: (4.894)(R 4.892, F 4.896)]  [G loss: -106.750] \n",
                        "4831 [D loss: (4.895)(R 4.893, F 4.896)]  [G loss: -106.760] \n",
                        "4832 [D loss: (4.895)(R 4.893, F 4.897)]  [G loss: -106.771] \n",
                        "4833 [D loss: (4.896)(R 4.894, F 4.898)]  [G loss: -106.783] \n",
                        "4834 [D loss: (4.897)(R 4.895, F 4.898)]  [G loss: -106.793] \n",
                        "4835 [D loss: (4.897)(R 4.896, F 4.899)]  [G loss: -106.803] \n",
                        "4836 [D loss: (4.898)(R 4.896, F 4.900)]  [G loss: -106.813] \n",
                        "4837 [D loss: (4.899)(R 4.897, F 4.900)]  [G loss: -106.824] \n",
                        "4838 [D loss: (4.899)(R 4.898, F 4.901)]  [G loss: -106.832] \n",
                        "4839 [D loss: (4.900)(R 4.898, F 4.901)]  [G loss: -106.843] \n",
                        "4840 [D loss: (4.900)(R 4.899, F 4.902)]  [G loss: -106.852] \n",
                        "4841 [D loss: (4.901)(R 4.899, F 4.903)]  [G loss: -106.860] \n",
                        "4842 [D loss: (4.901)(R 4.900, F 4.903)]  [G loss: -106.870] \n",
                        "4843 [D loss: (4.902)(R 4.900, F 4.904)]  [G loss: -106.879] \n",
                        "4844 [D loss: (4.902)(R 4.901, F 4.904)]  [G loss: -106.887] \n",
                        "4845 [D loss: (4.903)(R 4.901, F 4.904)]  [G loss: -106.897] \n",
                        "4846 [D loss: (4.903)(R 4.902, F 4.905)]  [G loss: -106.905] \n",
                        "4847 [D loss: (4.904)(R 4.902, F 4.905)]  [G loss: -106.915] \n",
                        "4848 [D loss: (4.904)(R 4.903, F 4.906)]  [G loss: -106.924] \n",
                        "4849 [D loss: (4.905)(R 4.903, F 4.906)]  [G loss: -106.933] \n",
                        "4850 [D loss: (4.905)(R 4.903, F 4.907)]  [G loss: -106.942] \n",
                        "4851 [D loss: (4.905)(R 4.904, F 4.907)]  [G loss: -106.951] \n",
                        "4852 [D loss: (4.906)(R 4.904, F 4.908)]  [G loss: -106.960] \n",
                        "4853 [D loss: (4.906)(R 4.905, F 4.908)]  [G loss: -106.970] \n",
                        "4854 [D loss: (4.907)(R 4.905, F 4.908)]  [G loss: -106.979] \n",
                        "4855 [D loss: (4.907)(R 4.906, F 4.909)]  [G loss: -106.988] \n",
                        "4856 [D loss: (4.908)(R 4.906, F 4.909)]  [G loss: -106.997] \n",
                        "4857 [D loss: (4.908)(R 4.906, F 4.910)]  [G loss: -107.007] \n",
                        "4858 [D loss: (4.908)(R 4.907, F 4.910)]  [G loss: -107.016] \n",
                        "4859 [D loss: (4.909)(R 4.907, F 4.910)]  [G loss: -107.025] \n",
                        "4860 [D loss: (4.909)(R 4.908, F 4.911)]  [G loss: -107.035] \n",
                        "4861 [D loss: (4.910)(R 4.908, F 4.911)]  [G loss: -107.045] \n",
                        "4862 [D loss: (4.910)(R 4.909, F 4.912)]  [G loss: -107.055] \n",
                        "4863 [D loss: (4.911)(R 4.909, F 4.913)]  [G loss: -107.065] \n",
                        "4864 [D loss: (4.911)(R 4.910, F 4.913)]  [G loss: -107.075] \n",
                        "4865 [D loss: (4.912)(R 4.910, F 4.913)]  [G loss: -107.085] \n",
                        "4866 [D loss: (4.913)(R 4.911, F 4.914)]  [G loss: -107.095] \n",
                        "4867 [D loss: (4.913)(R 4.911, F 4.915)]  [G loss: -107.106] \n",
                        "4868 [D loss: (4.914)(R 4.912, F 4.915)]  [G loss: -107.117] \n",
                        "4869 [D loss: (4.914)(R 4.913, F 4.916)]  [G loss: -107.127] \n",
                        "4870 [D loss: (4.915)(R 4.913, F 4.917)]  [G loss: -107.138] \n",
                        "4871 [D loss: (4.916)(R 4.914, F 4.918)]  [G loss: -107.148] \n",
                        "4872 [D loss: (4.916)(R 4.915, F 4.918)]  [G loss: -107.158] \n",
                        "4873 [D loss: (4.917)(R 4.915, F 4.919)]  [G loss: -107.169] \n",
                        "4874 [D loss: (4.918)(R 4.916, F 4.919)]  [G loss: -107.180] \n",
                        "4875 [D loss: (4.918)(R 4.917, F 4.920)]  [G loss: -107.190] \n",
                        "4876 [D loss: (4.919)(R 4.917, F 4.921)]  [G loss: -107.200] \n",
                        "4877 [D loss: (4.920)(R 4.918, F 4.921)]  [G loss: -107.211] \n",
                        "4878 [D loss: (4.920)(R 4.919, F 4.922)]  [G loss: -107.220] \n",
                        "4879 [D loss: (4.921)(R 4.919, F 4.922)]  [G loss: -107.231] \n",
                        "4880 [D loss: (4.922)(R 4.920, F 4.923)]  [G loss: -107.241] \n",
                        "4881 [D loss: (4.922)(R 4.921, F 4.924)]  [G loss: -107.249] \n",
                        "4882 [D loss: (4.923)(R 4.921, F 4.924)]  [G loss: -107.258] \n",
                        "4883 [D loss: (4.923)(R 4.922, F 4.925)]  [G loss: -107.267] \n",
                        "4884 [D loss: (4.924)(R 4.922, F 4.925)]  [G loss: -107.277] \n",
                        "4885 [D loss: (4.924)(R 4.923, F 4.926)]  [G loss: -107.285] \n",
                        "4886 [D loss: (4.925)(R 4.923, F 4.926)]  [G loss: -107.294] \n",
                        "4887 [D loss: (4.925)(R 4.923, F 4.927)]  [G loss: -107.303] \n",
                        "4888 [D loss: (4.926)(R 4.924, F 4.927)]  [G loss: -107.311] \n",
                        "4889 [D loss: (4.926)(R 4.924, F 4.927)]  [G loss: -107.320] \n",
                        "4890 [D loss: (4.926)(R 4.925, F 4.928)]  [G loss: -107.329] \n",
                        "4891 [D loss: (4.927)(R 4.925, F 4.928)]  [G loss: -107.337] \n",
                        "4892 [D loss: (4.927)(R 4.925, F 4.929)]  [G loss: -107.346] \n",
                        "4893 [D loss: (4.928)(R 4.926, F 4.929)]  [G loss: -107.355] \n",
                        "4894 [D loss: (4.928)(R 4.926, F 4.930)]  [G loss: -107.365] \n",
                        "4895 [D loss: (4.928)(R 4.927, F 4.930)]  [G loss: -107.373] \n",
                        "4896 [D loss: (4.929)(R 4.927, F 4.930)]  [G loss: -107.383] \n",
                        "4897 [D loss: (4.929)(R 4.928, F 4.931)]  [G loss: -107.392] \n",
                        "4898 [D loss: (4.930)(R 4.928, F 4.931)]  [G loss: -107.402] \n",
                        "4899 [D loss: (4.930)(R 4.928, F 4.932)]  [G loss: -107.411] \n",
                        "4900 [D loss: (4.931)(R 4.929, F 4.932)]  [G loss: -107.421] \n",
                        "4901 [D loss: (4.931)(R 4.929, F 4.933)]  [G loss: -107.431] \n",
                        "4902 [D loss: (4.932)(R 4.930, F 4.933)]  [G loss: -107.440] \n",
                        "4903 [D loss: (4.932)(R 4.930, F 4.934)]  [G loss: -107.450] \n",
                        "4904 [D loss: (4.933)(R 4.931, F 4.934)]  [G loss: -107.460] \n",
                        "4905 [D loss: (4.933)(R 4.931, F 4.935)]  [G loss: -107.469] \n",
                        "4906 [D loss: (4.934)(R 4.932, F 4.935)]  [G loss: -107.479] \n",
                        "4907 [D loss: (4.934)(R 4.932, F 4.936)]  [G loss: -107.489] \n",
                        "4908 [D loss: (4.935)(R 4.933, F 4.936)]  [G loss: -107.498] \n",
                        "4909 [D loss: (4.935)(R 4.934, F 4.937)]  [G loss: -107.508] \n",
                        "4910 [D loss: (4.936)(R 4.934, F 4.937)]  [G loss: -107.517] \n",
                        "4911 [D loss: (4.936)(R 4.934, F 4.938)]  [G loss: -107.527] \n",
                        "4912 [D loss: (4.937)(R 4.935, F 4.938)]  [G loss: -107.537] \n",
                        "4913 [D loss: (4.937)(R 4.936, F 4.939)]  [G loss: -107.547] \n",
                        "4914 [D loss: (4.938)(R 4.936, F 4.939)]  [G loss: -107.559] \n",
                        "4915 [D loss: (4.939)(R 4.937, F 4.940)]  [G loss: -107.568] \n",
                        "4916 [D loss: (4.939)(R 4.937, F 4.941)]  [G loss: -107.579] \n",
                        "4917 [D loss: (4.940)(R 4.938, F 4.941)]  [G loss: -107.589] \n",
                        "4918 [D loss: (4.940)(R 4.939, F 4.942)]  [G loss: -107.599] \n",
                        "4919 [D loss: (4.941)(R 4.939, F 4.943)]  [G loss: -107.609] \n",
                        "4920 [D loss: (4.942)(R 4.940, F 4.943)]  [G loss: -107.619] \n",
                        "4921 [D loss: (4.942)(R 4.941, F 4.944)]  [G loss: -107.628] \n",
                        "4922 [D loss: (4.943)(R 4.941, F 4.944)]  [G loss: -107.639] \n",
                        "4923 [D loss: (4.943)(R 4.942, F 4.945)]  [G loss: -107.647] \n",
                        "4924 [D loss: (4.944)(R 4.942, F 4.945)]  [G loss: -107.656] \n",
                        "4925 [D loss: (4.944)(R 4.943, F 4.946)]  [G loss: -107.665] \n",
                        "4926 [D loss: (4.945)(R 4.943, F 4.946)]  [G loss: -107.673] \n",
                        "4927 [D loss: (4.945)(R 4.943, F 4.947)]  [G loss: -107.682] \n",
                        "4928 [D loss: (4.945)(R 4.944, F 4.947)]  [G loss: -107.691] \n",
                        "4929 [D loss: (4.946)(R 4.944, F 4.947)]  [G loss: -107.699] \n",
                        "4930 [D loss: (4.946)(R 4.945, F 4.948)]  [G loss: -107.708] \n",
                        "4931 [D loss: (4.947)(R 4.945, F 4.948)]  [G loss: -107.717] \n",
                        "4932 [D loss: (4.947)(R 4.945, F 4.949)]  [G loss: -107.726] \n",
                        "4933 [D loss: (4.947)(R 4.946, F 4.949)]  [G loss: -107.735] \n",
                        "4934 [D loss: (4.948)(R 4.946, F 4.949)]  [G loss: -107.743] \n",
                        "4935 [D loss: (4.948)(R 4.947, F 4.950)]  [G loss: -107.753] \n",
                        "4936 [D loss: (4.949)(R 4.947, F 4.950)]  [G loss: -107.761] \n",
                        "4937 [D loss: (4.949)(R 4.947, F 4.951)]  [G loss: -107.770] \n",
                        "4938 [D loss: (4.949)(R 4.948, F 4.951)]  [G loss: -107.779] \n",
                        "4939 [D loss: (4.950)(R 4.948, F 4.951)]  [G loss: -107.788] \n",
                        "4940 [D loss: (4.950)(R 4.949, F 4.952)]  [G loss: -107.797] \n",
                        "4941 [D loss: (4.951)(R 4.949, F 4.952)]  [G loss: -107.806] \n",
                        "4942 [D loss: (4.951)(R 4.949, F 4.953)]  [G loss: -107.815] \n",
                        "4943 [D loss: (4.952)(R 4.950, F 4.953)]  [G loss: -107.825] \n",
                        "4944 [D loss: (4.952)(R 4.950, F 4.954)]  [G loss: -107.834] \n",
                        "4945 [D loss: (4.952)(R 4.951, F 4.954)]  [G loss: -107.844] \n",
                        "4946 [D loss: (4.953)(R 4.951, F 4.955)]  [G loss: -107.853] \n",
                        "4947 [D loss: (4.953)(R 4.952, F 4.955)]  [G loss: -107.863] \n",
                        "4948 [D loss: (4.954)(R 4.952, F 4.956)]  [G loss: -107.873] \n",
                        "4949 [D loss: (4.955)(R 4.953, F 4.956)]  [G loss: -107.882] \n",
                        "4950 [D loss: (4.955)(R 4.953, F 4.957)]  [G loss: -107.893] \n",
                        "4951 [D loss: (4.956)(R 4.954, F 4.957)]  [G loss: -107.902] \n",
                        "4952 [D loss: (4.956)(R 4.954, F 4.958)]  [G loss: -107.912] \n",
                        "4953 [D loss: (4.957)(R 4.955, F 4.958)]  [G loss: -107.921] \n",
                        "4954 [D loss: (4.957)(R 4.955, F 4.959)]  [G loss: -107.931] \n",
                        "4955 [D loss: (4.958)(R 4.956, F 4.959)]  [G loss: -107.939] \n",
                        "4956 [D loss: (4.958)(R 4.956, F 4.960)]  [G loss: -107.949] \n",
                        "4957 [D loss: (4.959)(R 4.957, F 4.960)]  [G loss: -107.958] \n",
                        "4958 [D loss: (4.959)(R 4.958, F 4.961)]  [G loss: -107.967] \n",
                        "4959 [D loss: (4.960)(R 4.958, F 4.961)]  [G loss: -107.976] \n",
                        "4960 [D loss: (4.960)(R 4.958, F 4.962)]  [G loss: -107.985] \n",
                        "4961 [D loss: (4.961)(R 4.959, F 4.962)]  [G loss: -107.994] \n",
                        "4962 [D loss: (4.961)(R 4.959, F 4.963)]  [G loss: -108.003] \n",
                        "4963 [D loss: (4.961)(R 4.960, F 4.963)]  [G loss: -108.012] \n",
                        "4964 [D loss: (4.962)(R 4.960, F 4.963)]  [G loss: -108.020] \n",
                        "4965 [D loss: (4.962)(R 4.961, F 4.964)]  [G loss: -108.029] \n",
                        "4966 [D loss: (4.963)(R 4.961, F 4.964)]  [G loss: -108.038] \n",
                        "4967 [D loss: (4.963)(R 4.961, F 4.965)]  [G loss: -108.047] \n",
                        "4968 [D loss: (4.963)(R 4.962, F 4.965)]  [G loss: -108.056] \n",
                        "4969 [D loss: (4.964)(R 4.962, F 4.965)]  [G loss: -108.065] \n",
                        "4970 [D loss: (4.964)(R 4.963, F 4.966)]  [G loss: -108.074] \n",
                        "4971 [D loss: (4.965)(R 4.963, F 4.966)]  [G loss: -108.082] \n",
                        "4972 [D loss: (4.965)(R 4.964, F 4.967)]  [G loss: -108.091] \n",
                        "4973 [D loss: (4.966)(R 4.964, F 4.967)]  [G loss: -108.100] \n",
                        "4974 [D loss: (4.966)(R 4.964, F 4.967)]  [G loss: -108.109] \n",
                        "4975 [D loss: (4.966)(R 4.965, F 4.968)]  [G loss: -108.118] \n",
                        "4976 [D loss: (4.967)(R 4.965, F 4.968)]  [G loss: -108.127] \n",
                        "4977 [D loss: (4.967)(R 4.965, F 4.969)]  [G loss: -108.136] \n",
                        "4978 [D loss: (4.968)(R 4.966, F 4.969)]  [G loss: -108.145] \n",
                        "4979 [D loss: (4.968)(R 4.966, F 4.970)]  [G loss: -108.155] \n",
                        "4980 [D loss: (4.969)(R 4.967, F 4.970)]  [G loss: -108.164] \n",
                        "4981 [D loss: (4.969)(R 4.967, F 4.971)]  [G loss: -108.173] \n",
                        "4982 [D loss: (4.969)(R 4.968, F 4.971)]  [G loss: -108.183] \n",
                        "4983 [D loss: (4.970)(R 4.969, F 4.972)]  [G loss: -108.191] \n",
                        "4984 [D loss: (4.970)(R 4.969, F 4.972)]  [G loss: -108.200] \n",
                        "4985 [D loss: (4.971)(R 4.969, F 4.973)]  [G loss: -108.209] \n",
                        "4986 [D loss: (4.971)(R 4.970, F 4.973)]  [G loss: -108.219] \n",
                        "4987 [D loss: (4.972)(R 4.970, F 4.974)]  [G loss: -108.227] \n",
                        "4988 [D loss: (4.972)(R 4.971, F 4.974)]  [G loss: -108.237] \n",
                        "4989 [D loss: (4.973)(R 4.971, F 4.974)]  [G loss: -108.245] \n",
                        "4990 [D loss: (4.973)(R 4.972, F 4.975)]  [G loss: -108.254] \n",
                        "4991 [D loss: (4.974)(R 4.972, F 4.975)]  [G loss: -108.264] \n",
                        "4992 [D loss: (4.974)(R 4.973, F 4.976)]  [G loss: -108.272] \n",
                        "4993 [D loss: (4.975)(R 4.973, F 4.976)]  [G loss: -108.281] \n",
                        "4994 [D loss: (4.975)(R 4.973, F 4.977)]  [G loss: -108.290] \n",
                        "4995 [D loss: (4.976)(R 4.974, F 4.977)]  [G loss: -108.299] \n",
                        "4996 [D loss: (4.976)(R 4.974, F 4.978)]  [G loss: -108.308] \n",
                        "4997 [D loss: (4.976)(R 4.975, F 4.978)]  [G loss: -108.316] \n",
                        "4998 [D loss: (4.977)(R 4.975, F 4.978)]  [G loss: -108.326] \n",
                        "4999 [D loss: (4.977)(R 4.976, F 4.979)]  [G loss: -108.334] \n",
                        "5000 [D loss: (4.978)(R 4.976, F 4.979)]  [G loss: -108.343] \n",
                        "5001 [D loss: (4.978)(R 4.976, F 4.980)]  [G loss: -108.352] \n",
                        "5002 [D loss: (4.978)(R 4.977, F 4.980)]  [G loss: -108.361] \n",
                        "5003 [D loss: (4.979)(R 4.977, F 4.980)]  [G loss: -108.370] \n",
                        "5004 [D loss: (4.979)(R 4.978, F 4.981)]  [G loss: -108.379] \n",
                        "5005 [D loss: (4.980)(R 4.978, F 4.981)]  [G loss: -108.388] \n",
                        "5006 [D loss: (4.980)(R 4.979, F 4.982)]  [G loss: -108.397] \n",
                        "5007 [D loss: (4.981)(R 4.979, F 4.982)]  [G loss: -108.406] \n",
                        "5008 [D loss: (4.981)(R 4.979, F 4.983)]  [G loss: -108.415] \n",
                        "5009 [D loss: (4.981)(R 4.980, F 4.983)]  [G loss: -108.424] \n",
                        "5010 [D loss: (4.982)(R 4.980, F 4.984)]  [G loss: -108.433] \n",
                        "5011 [D loss: (4.982)(R 4.981, F 4.984)]  [G loss: -108.443] \n",
                        "5012 [D loss: (4.983)(R 4.981, F 4.984)]  [G loss: -108.452] \n",
                        "5013 [D loss: (4.983)(R 4.982, F 4.985)]  [G loss: -108.460] \n",
                        "5014 [D loss: (4.984)(R 4.982, F 4.985)]  [G loss: -108.470] \n",
                        "5015 [D loss: (4.984)(R 4.983, F 4.986)]  [G loss: -108.479] \n",
                        "5016 [D loss: (4.985)(R 4.983, F 4.986)]  [G loss: -108.488] \n",
                        "5017 [D loss: (4.985)(R 4.984, F 4.987)]  [G loss: -108.497] \n",
                        "5018 [D loss: (4.985)(R 4.984, F 4.987)]  [G loss: -108.506] \n",
                        "5019 [D loss: (4.986)(R 4.985, F 4.988)]  [G loss: -108.514] \n",
                        "5020 [D loss: (4.987)(R 4.985, F 4.988)]  [G loss: -108.523] \n",
                        "5021 [D loss: (4.987)(R 4.985, F 4.988)]  [G loss: -108.532] \n",
                        "5022 [D loss: (4.987)(R 4.986, F 4.989)]  [G loss: -108.542] \n",
                        "5023 [D loss: (4.988)(R 4.986, F 4.989)]  [G loss: -108.551] \n",
                        "5024 [D loss: (4.988)(R 4.987, F 4.990)]  [G loss: -108.560] \n",
                        "5025 [D loss: (4.989)(R 4.987, F 4.990)]  [G loss: -108.570] \n",
                        "5026 [D loss: (4.989)(R 4.988, F 4.991)]  [G loss: -108.579] \n",
                        "5027 [D loss: (4.990)(R 4.988, F 4.991)]  [G loss: -108.589] \n",
                        "5028 [D loss: (4.990)(R 4.989, F 4.992)]  [G loss: -108.597] \n",
                        "5029 [D loss: (4.991)(R 4.989, F 4.992)]  [G loss: -108.607] \n",
                        "5030 [D loss: (4.991)(R 4.990, F 4.993)]  [G loss: -108.615] \n",
                        "5031 [D loss: (4.992)(R 4.990, F 4.993)]  [G loss: -108.625] \n",
                        "5032 [D loss: (4.992)(R 4.991, F 4.994)]  [G loss: -108.634] \n",
                        "5033 [D loss: (4.993)(R 4.991, F 4.994)]  [G loss: -108.642] \n",
                        "5034 [D loss: (4.993)(R 4.992, F 4.995)]  [G loss: -108.651] \n",
                        "5035 [D loss: (4.994)(R 4.992, F 4.995)]  [G loss: -108.661] \n",
                        "5036 [D loss: (4.994)(R 4.992, F 4.996)]  [G loss: -108.669] \n",
                        "5037 [D loss: (4.994)(R 4.993, F 4.996)]  [G loss: -108.678] \n",
                        "5038 [D loss: (4.995)(R 4.993, F 4.996)]  [G loss: -108.687] \n",
                        "5039 [D loss: (4.995)(R 4.994, F 4.997)]  [G loss: -108.696] \n",
                        "5040 [D loss: (4.996)(R 4.994, F 4.997)]  [G loss: -108.705] \n",
                        "5041 [D loss: (4.996)(R 4.995, F 4.998)]  [G loss: -108.713] \n",
                        "5042 [D loss: (4.997)(R 4.995, F 4.998)]  [G loss: -108.722] \n",
                        "5043 [D loss: (4.997)(R 4.995, F 4.999)]  [G loss: -108.731] \n",
                        "5044 [D loss: (4.997)(R 4.996, F 4.999)]  [G loss: -108.741] \n",
                        "5045 [D loss: (4.998)(R 4.996, F 4.999)]  [G loss: -108.750] \n",
                        "5046 [D loss: (4.998)(R 4.997, F 5.000)]  [G loss: -108.759] \n",
                        "5047 [D loss: (4.999)(R 4.997, F 5.000)]  [G loss: -108.767] \n",
                        "5048 [D loss: (4.999)(R 4.997, F 5.001)]  [G loss: -108.776] \n",
                        "5049 [D loss: (5.000)(R 4.998, F 5.001)]  [G loss: -108.785] \n",
                        "5050 [D loss: (5.000)(R 4.998, F 5.002)]  [G loss: -108.794] \n",
                        "5051 [D loss: (5.000)(R 4.999, F 5.002)]  [G loss: -108.803] \n",
                        "5052 [D loss: (5.001)(R 4.999, F 5.002)]  [G loss: -108.812] \n",
                        "5053 [D loss: (5.001)(R 5.000, F 5.003)]  [G loss: -108.821] \n",
                        "5054 [D loss: (5.002)(R 5.000, F 5.003)]  [G loss: -108.831] \n",
                        "5055 [D loss: (5.002)(R 5.000, F 5.004)]  [G loss: -108.840] \n",
                        "5056 [D loss: (5.002)(R 5.001, F 5.004)]  [G loss: -108.850] \n",
                        "5057 [D loss: (5.003)(R 5.001, F 5.005)]  [G loss: -108.860] \n",
                        "5058 [D loss: (5.004)(R 5.002, F 5.005)]  [G loss: -108.869] \n",
                        "5059 [D loss: (5.004)(R 5.003, F 5.006)]  [G loss: -108.879] \n",
                        "5060 [D loss: (5.005)(R 5.003, F 5.006)]  [G loss: -108.889] \n",
                        "5061 [D loss: (5.005)(R 5.004, F 5.007)]  [G loss: -108.899] \n",
                        "5062 [D loss: (5.006)(R 5.004, F 5.008)]  [G loss: -108.908] \n",
                        "5063 [D loss: (5.006)(R 5.005, F 5.008)]  [G loss: -108.918] \n",
                        "5064 [D loss: (5.007)(R 5.005, F 5.009)]  [G loss: -108.927] \n",
                        "5065 [D loss: (5.008)(R 5.006, F 5.009)]  [G loss: -108.935] \n",
                        "5066 [D loss: (5.008)(R 5.006, F 5.009)]  [G loss: -108.945] \n",
                        "5067 [D loss: (5.009)(R 5.007, F 5.010)]  [G loss: -108.953] \n",
                        "5068 [D loss: (5.009)(R 5.007, F 5.010)]  [G loss: -108.962] \n",
                        "5069 [D loss: (5.009)(R 5.008, F 5.011)]  [G loss: -108.971] \n",
                        "5070 [D loss: (5.010)(R 5.008, F 5.011)]  [G loss: -108.979] \n",
                        "5071 [D loss: (5.010)(R 5.009, F 5.012)]  [G loss: -108.988] \n",
                        "5072 [D loss: (5.011)(R 5.009, F 5.012)]  [G loss: -108.995] \n",
                        "5073 [D loss: (5.011)(R 5.009, F 5.012)]  [G loss: -109.004] \n",
                        "5074 [D loss: (5.011)(R 5.010, F 5.013)]  [G loss: -109.012] \n",
                        "5075 [D loss: (5.012)(R 5.010, F 5.013)]  [G loss: -109.020] \n",
                        "5076 [D loss: (5.012)(R 5.011, F 5.014)]  [G loss: -109.029] \n",
                        "5077 [D loss: (5.012)(R 5.011, F 5.014)]  [G loss: -109.037] \n",
                        "5078 [D loss: (5.013)(R 5.011, F 5.014)]  [G loss: -109.045] \n",
                        "5079 [D loss: (5.013)(R 5.011, F 5.015)]  [G loss: -109.054] \n",
                        "5080 [D loss: (5.013)(R 5.012, F 5.015)]  [G loss: -109.062] \n",
                        "5081 [D loss: (5.014)(R 5.012, F 5.015)]  [G loss: -109.071] \n",
                        "5082 [D loss: (5.014)(R 5.012, F 5.016)]  [G loss: -109.080] \n",
                        "5083 [D loss: (5.014)(R 5.013, F 5.016)]  [G loss: -109.088] \n",
                        "5084 [D loss: (5.015)(R 5.013, F 5.016)]  [G loss: -109.097] \n",
                        "5085 [D loss: (5.015)(R 5.014, F 5.017)]  [G loss: -109.105] \n",
                        "5086 [D loss: (5.016)(R 5.014, F 5.017)]  [G loss: -109.114] \n",
                        "5087 [D loss: (5.016)(R 5.014, F 5.017)]  [G loss: -109.122] \n",
                        "5088 [D loss: (5.016)(R 5.015, F 5.018)]  [G loss: -109.131] \n",
                        "5089 [D loss: (5.017)(R 5.015, F 5.018)]  [G loss: -109.140] \n",
                        "5090 [D loss: (5.017)(R 5.015, F 5.018)]  [G loss: -109.149] \n",
                        "5091 [D loss: (5.017)(R 5.016, F 5.019)]  [G loss: -109.158] \n",
                        "5092 [D loss: (5.018)(R 5.016, F 5.019)]  [G loss: -109.168] \n",
                        "5093 [D loss: (5.018)(R 5.016, F 5.020)]  [G loss: -109.177] \n",
                        "5094 [D loss: (5.019)(R 5.017, F 5.020)]  [G loss: -109.187] \n",
                        "5095 [D loss: (5.019)(R 5.017, F 5.021)]  [G loss: -109.197] \n",
                        "5096 [D loss: (5.019)(R 5.018, F 5.021)]  [G loss: -109.207] \n",
                        "5097 [D loss: (5.020)(R 5.018, F 5.022)]  [G loss: -109.216] \n",
                        "5098 [D loss: (5.020)(R 5.019, F 5.022)]  [G loss: -109.226] \n",
                        "5099 [D loss: (5.021)(R 5.019, F 5.023)]  [G loss: -109.236] \n",
                        "5100 [D loss: (5.022)(R 5.020, F 5.023)]  [G loss: -109.245] \n",
                        "5101 [D loss: (5.022)(R 5.020, F 5.024)]  [G loss: -109.255] \n",
                        "5102 [D loss: (5.023)(R 5.021, F 5.024)]  [G loss: -109.264] \n",
                        "5103 [D loss: (5.023)(R 5.021, F 5.025)]  [G loss: -109.274] \n",
                        "5104 [D loss: (5.023)(R 5.022, F 5.025)]  [G loss: -109.284] \n",
                        "5105 [D loss: (5.024)(R 5.023, F 5.026)]  [G loss: -109.293] \n",
                        "5106 [D loss: (5.025)(R 5.023, F 5.026)]  [G loss: -109.302] \n",
                        "5107 [D loss: (5.025)(R 5.023, F 5.027)]  [G loss: -109.311] \n",
                        "5108 [D loss: (5.026)(R 5.024, F 5.027)]  [G loss: -109.320] \n",
                        "5109 [D loss: (5.026)(R 5.024, F 5.027)]  [G loss: -109.330] \n",
                        "5110 [D loss: (5.027)(R 5.025, F 5.028)]  [G loss: -109.337] \n",
                        "5111 [D loss: (5.027)(R 5.025, F 5.029)]  [G loss: -109.344] \n",
                        "5112 [D loss: (5.027)(R 5.026, F 5.029)]  [G loss: -109.353] \n",
                        "5113 [D loss: (5.028)(R 5.026, F 5.029)]  [G loss: -109.361] \n",
                        "5114 [D loss: (5.028)(R 5.027, F 5.030)]  [G loss: -109.368] \n",
                        "5115 [D loss: (5.028)(R 5.027, F 5.030)]  [G loss: -109.376] \n",
                        "5116 [D loss: (5.029)(R 5.027, F 5.030)]  [G loss: -109.383] \n",
                        "5117 [D loss: (5.029)(R 5.027, F 5.030)]  [G loss: -109.391] \n",
                        "5118 [D loss: (5.029)(R 5.027, F 5.030)]  [G loss: -109.398] \n",
                        "5119 [D loss: (5.029)(R 5.028, F 5.031)]  [G loss: -109.406] \n",
                        "5120 [D loss: (5.029)(R 5.028, F 5.031)]  [G loss: -109.414] \n",
                        "5121 [D loss: (5.030)(R 5.028, F 5.031)]  [G loss: -109.422] \n",
                        "5122 [D loss: (5.030)(R 5.028, F 5.031)]  [G loss: -109.430] \n",
                        "5123 [D loss: (5.030)(R 5.029, F 5.032)]  [G loss: -109.438] \n",
                        "5124 [D loss: (5.030)(R 5.029, F 5.032)]  [G loss: -109.446] \n",
                        "5125 [D loss: (5.031)(R 5.029, F 5.032)]  [G loss: -109.454] \n",
                        "5126 [D loss: (5.031)(R 5.029, F 5.033)]  [G loss: -109.463] \n",
                        "5127 [D loss: (5.031)(R 5.030, F 5.033)]  [G loss: -109.471] \n",
                        "5128 [D loss: (5.032)(R 5.030, F 5.033)]  [G loss: -109.480] \n",
                        "5129 [D loss: (5.032)(R 5.031, F 5.034)]  [G loss: -109.489] \n",
                        "5130 [D loss: (5.032)(R 5.031, F 5.034)]  [G loss: -109.497] \n",
                        "5131 [D loss: (5.033)(R 5.031, F 5.034)]  [G loss: -109.506] \n",
                        "5132 [D loss: (5.033)(R 5.031, F 5.035)]  [G loss: -109.515] \n",
                        "5133 [D loss: (5.034)(R 5.032, F 5.035)]  [G loss: -109.523] \n",
                        "5134 [D loss: (5.034)(R 5.032, F 5.036)]  [G loss: -109.533] \n",
                        "5135 [D loss: (5.034)(R 5.033, F 5.036)]  [G loss: -109.542] \n",
                        "5136 [D loss: (5.035)(R 5.033, F 5.036)]  [G loss: -109.552] \n",
                        "5137 [D loss: (5.035)(R 5.034, F 5.037)]  [G loss: -109.561] \n",
                        "5138 [D loss: (5.036)(R 5.034, F 5.037)]  [G loss: -109.570] \n",
                        "5139 [D loss: (5.036)(R 5.035, F 5.038)]  [G loss: -109.580] \n",
                        "5140 [D loss: (5.037)(R 5.035, F 5.039)]  [G loss: -109.589] \n",
                        "5141 [D loss: (5.037)(R 5.036, F 5.039)]  [G loss: -109.599] \n",
                        "5142 [D loss: (5.038)(R 5.036, F 5.039)]  [G loss: -109.609] \n",
                        "5143 [D loss: (5.038)(R 5.037, F 5.040)]  [G loss: -109.619] \n",
                        "5144 [D loss: (5.039)(R 5.037, F 5.041)]  [G loss: -109.629] \n",
                        "5145 [D loss: (5.040)(R 5.038, F 5.041)]  [G loss: -109.639] \n",
                        "5146 [D loss: (5.040)(R 5.038, F 5.042)]  [G loss: -109.648] \n",
                        "5147 [D loss: (5.041)(R 5.039, F 5.042)]  [G loss: -109.658] \n",
                        "5148 [D loss: (5.041)(R 5.040, F 5.043)]  [G loss: -109.667] \n",
                        "5149 [D loss: (5.042)(R 5.040, F 5.043)]  [G loss: -109.677] \n",
                        "5150 [D loss: (5.042)(R 5.041, F 5.044)]  [G loss: -109.685] \n",
                        "5151 [D loss: (5.043)(R 5.041, F 5.044)]  [G loss: -109.695] \n",
                        "5152 [D loss: (5.043)(R 5.042, F 5.045)]  [G loss: -109.703] \n",
                        "5153 [D loss: (5.043)(R 5.042, F 5.045)]  [G loss: -109.713] \n",
                        "5154 [D loss: (5.044)(R 5.042, F 5.046)]  [G loss: -109.721] \n",
                        "5155 [D loss: (5.044)(R 5.043, F 5.046)]  [G loss: -109.729] \n",
                        "5156 [D loss: (5.045)(R 5.043, F 5.046)]  [G loss: -109.737] \n",
                        "5157 [D loss: (5.045)(R 5.044, F 5.047)]  [G loss: -109.745] \n",
                        "5158 [D loss: (5.046)(R 5.044, F 5.047)]  [G loss: -109.754] \n",
                        "5159 [D loss: (5.046)(R 5.044, F 5.048)]  [G loss: -109.761] \n",
                        "5160 [D loss: (5.046)(R 5.045, F 5.048)]  [G loss: -109.770] \n",
                        "5161 [D loss: (5.047)(R 5.045, F 5.048)]  [G loss: -109.778] \n",
                        "5162 [D loss: (5.047)(R 5.046, F 5.049)]  [G loss: -109.784] \n",
                        "5163 [D loss: (5.047)(R 5.046, F 5.049)]  [G loss: -109.792] \n",
                        "5164 [D loss: (5.048)(R 5.046, F 5.049)]  [G loss: -109.800] \n",
                        "5165 [D loss: (5.048)(R 5.047, F 5.050)]  [G loss: -109.808] \n",
                        "5166 [D loss: (5.048)(R 5.047, F 5.050)]  [G loss: -109.816] \n",
                        "5167 [D loss: (5.049)(R 5.047, F 5.050)]  [G loss: -109.824] \n",
                        "5168 [D loss: (5.049)(R 5.047, F 5.050)]  [G loss: -109.833] \n",
                        "5169 [D loss: (5.050)(R 5.048, F 5.051)]  [G loss: -109.841] \n",
                        "5170 [D loss: (5.050)(R 5.048, F 5.051)]  [G loss: -109.849] \n",
                        "5171 [D loss: (5.050)(R 5.049, F 5.052)]  [G loss: -109.857] \n",
                        "5172 [D loss: (5.051)(R 5.049, F 5.052)]  [G loss: -109.866] \n",
                        "5173 [D loss: (5.051)(R 5.049, F 5.052)]  [G loss: -109.874] \n",
                        "5174 [D loss: (5.051)(R 5.050, F 5.053)]  [G loss: -109.883] \n",
                        "5175 [D loss: (5.052)(R 5.050, F 5.053)]  [G loss: -109.892] \n",
                        "5176 [D loss: (5.052)(R 5.050, F 5.054)]  [G loss: -109.901] \n",
                        "5177 [D loss: (5.052)(R 5.051, F 5.054)]  [G loss: -109.910] \n",
                        "5178 [D loss: (5.053)(R 5.051, F 5.054)]  [G loss: -109.919] \n",
                        "5179 [D loss: (5.053)(R 5.052, F 5.055)]  [G loss: -109.928] \n",
                        "5180 [D loss: (5.054)(R 5.052, F 5.055)]  [G loss: -109.937] \n",
                        "5181 [D loss: (5.054)(R 5.053, F 5.056)]  [G loss: -109.947] \n",
                        "5182 [D loss: (5.055)(R 5.053, F 5.056)]  [G loss: -109.956] \n",
                        "5183 [D loss: (5.055)(R 5.054, F 5.057)]  [G loss: -109.965] \n",
                        "5184 [D loss: (5.056)(R 5.054, F 5.057)]  [G loss: -109.975] \n",
                        "5185 [D loss: (5.056)(R 5.055, F 5.058)]  [G loss: -109.984] \n",
                        "5186 [D loss: (5.057)(R 5.055, F 5.058)]  [G loss: -109.994] \n",
                        "5187 [D loss: (5.057)(R 5.056, F 5.059)]  [G loss: -110.003] \n",
                        "5188 [D loss: (5.058)(R 5.056, F 5.059)]  [G loss: -110.013] \n",
                        "5189 [D loss: (5.058)(R 5.057, F 5.060)]  [G loss: -110.022] \n",
                        "5190 [D loss: (5.059)(R 5.057, F 5.060)]  [G loss: -110.031] \n",
                        "5191 [D loss: (5.059)(R 5.058, F 5.061)]  [G loss: -110.040] \n",
                        "5192 [D loss: (5.060)(R 5.058, F 5.061)]  [G loss: -110.049] \n",
                        "5193 [D loss: (5.060)(R 5.059, F 5.062)]  [G loss: -110.058] \n",
                        "5194 [D loss: (5.061)(R 5.059, F 5.062)]  [G loss: -110.066] \n",
                        "5195 [D loss: (5.061)(R 5.059, F 5.063)]  [G loss: -110.075] \n",
                        "5196 [D loss: (5.061)(R 5.060, F 5.063)]  [G loss: -110.084] \n",
                        "5197 [D loss: (5.062)(R 5.060, F 5.063)]  [G loss: -110.092] \n",
                        "5198 [D loss: (5.062)(R 5.061, F 5.064)]  [G loss: -110.101] \n",
                        "5199 [D loss: (5.063)(R 5.061, F 5.064)]  [G loss: -110.109] \n",
                        "5200 [D loss: (5.063)(R 5.061, F 5.065)]  [G loss: -110.118] \n",
                        "5201 [D loss: (5.063)(R 5.062, F 5.065)]  [G loss: -110.126] \n",
                        "5202 [D loss: (5.064)(R 5.062, F 5.065)]  [G loss: -110.134] \n",
                        "5203 [D loss: (5.064)(R 5.063, F 5.066)]  [G loss: -110.142] \n",
                        "5204 [D loss: (5.064)(R 5.063, F 5.066)]  [G loss: -110.151] \n",
                        "5205 [D loss: (5.065)(R 5.063, F 5.066)]  [G loss: -110.158] \n",
                        "5206 [D loss: (5.065)(R 5.064, F 5.067)]  [G loss: -110.166] \n",
                        "5207 [D loss: (5.065)(R 5.064, F 5.067)]  [G loss: -110.175] \n",
                        "5208 [D loss: (5.066)(R 5.064, F 5.067)]  [G loss: -110.184] \n",
                        "5209 [D loss: (5.066)(R 5.065, F 5.068)]  [G loss: -110.192] \n",
                        "5210 [D loss: (5.066)(R 5.065, F 5.068)]  [G loss: -110.201] \n",
                        "5211 [D loss: (5.067)(R 5.065, F 5.069)]  [G loss: -110.211] \n",
                        "5212 [D loss: (5.068)(R 5.066, F 5.069)]  [G loss: -110.219] \n",
                        "5213 [D loss: (5.068)(R 5.067, F 5.070)]  [G loss: -110.227] \n",
                        "5214 [D loss: (5.068)(R 5.067, F 5.070)]  [G loss: -110.236] \n",
                        "5215 [D loss: (5.069)(R 5.067, F 5.070)]  [G loss: -110.245] \n",
                        "5216 [D loss: (5.069)(R 5.068, F 5.071)]  [G loss: -110.254] \n",
                        "5217 [D loss: (5.070)(R 5.068, F 5.071)]  [G loss: -110.262] \n",
                        "5218 [D loss: (5.070)(R 5.069, F 5.072)]  [G loss: -110.270] \n",
                        "5219 [D loss: (5.071)(R 5.069, F 5.072)]  [G loss: -110.279] \n",
                        "5220 [D loss: (5.071)(R 5.070, F 5.073)]  [G loss: -110.287] \n",
                        "5221 [D loss: (5.071)(R 5.070, F 5.073)]  [G loss: -110.296] \n",
                        "5222 [D loss: (5.072)(R 5.070, F 5.073)]  [G loss: -110.304] \n",
                        "5223 [D loss: (5.072)(R 5.071, F 5.074)]  [G loss: -110.313] \n",
                        "5224 [D loss: (5.073)(R 5.071, F 5.074)]  [G loss: -110.321] \n",
                        "5225 [D loss: (5.073)(R 5.071, F 5.074)]  [G loss: -110.329] \n",
                        "5226 [D loss: (5.073)(R 5.072, F 5.075)]  [G loss: -110.337] \n",
                        "5227 [D loss: (5.074)(R 5.072, F 5.075)]  [G loss: -110.346] \n",
                        "5228 [D loss: (5.074)(R 5.072, F 5.076)]  [G loss: -110.354] \n",
                        "5229 [D loss: (5.074)(R 5.073, F 5.076)]  [G loss: -110.363] \n",
                        "5230 [D loss: (5.075)(R 5.073, F 5.076)]  [G loss: -110.371] \n",
                        "5231 [D loss: (5.075)(R 5.073, F 5.077)]  [G loss: -110.380] \n",
                        "5232 [D loss: (5.075)(R 5.074, F 5.077)]  [G loss: -110.388] \n",
                        "5233 [D loss: (5.076)(R 5.074, F 5.077)]  [G loss: -110.397] \n",
                        "5234 [D loss: (5.076)(R 5.075, F 5.078)]  [G loss: -110.406] \n",
                        "5235 [D loss: (5.077)(R 5.075, F 5.078)]  [G loss: -110.414] \n",
                        "5236 [D loss: (5.077)(R 5.075, F 5.078)]  [G loss: -110.423] \n",
                        "5237 [D loss: (5.077)(R 5.076, F 5.079)]  [G loss: -110.431] \n",
                        "5238 [D loss: (5.078)(R 5.076, F 5.079)]  [G loss: -110.440] \n",
                        "5239 [D loss: (5.078)(R 5.077, F 5.080)]  [G loss: -110.448] \n",
                        "5240 [D loss: (5.079)(R 5.077, F 5.080)]  [G loss: -110.457] \n",
                        "5241 [D loss: (5.079)(R 5.077, F 5.080)]  [G loss: -110.466] \n",
                        "5242 [D loss: (5.079)(R 5.078, F 5.081)]  [G loss: -110.475] \n",
                        "5243 [D loss: (5.080)(R 5.078, F 5.081)]  [G loss: -110.484] \n",
                        "5244 [D loss: (5.080)(R 5.079, F 5.082)]  [G loss: -110.493] \n",
                        "5245 [D loss: (5.081)(R 5.079, F 5.082)]  [G loss: -110.502] \n",
                        "5246 [D loss: (5.081)(R 5.079, F 5.083)]  [G loss: -110.511] \n",
                        "5247 [D loss: (5.081)(R 5.080, F 5.083)]  [G loss: -110.520] \n",
                        "5248 [D loss: (5.082)(R 5.080, F 5.084)]  [G loss: -110.529] \n",
                        "5249 [D loss: (5.082)(R 5.081, F 5.084)]  [G loss: -110.538] \n",
                        "5250 [D loss: (5.083)(R 5.081, F 5.085)]  [G loss: -110.546] \n",
                        "5251 [D loss: (5.083)(R 5.082, F 5.085)]  [G loss: -110.555] \n",
                        "5252 [D loss: (5.084)(R 5.082, F 5.085)]  [G loss: -110.564] \n",
                        "5253 [D loss: (5.084)(R 5.083, F 5.086)]  [G loss: -110.573] \n",
                        "5254 [D loss: (5.085)(R 5.083, F 5.087)]  [G loss: -110.581] \n",
                        "5255 [D loss: (5.085)(R 5.084, F 5.087)]  [G loss: -110.589] \n",
                        "5256 [D loss: (5.086)(R 5.084, F 5.087)]  [G loss: -110.598] \n",
                        "5257 [D loss: (5.086)(R 5.085, F 5.088)]  [G loss: -110.606] \n",
                        "5258 [D loss: (5.087)(R 5.085, F 5.088)]  [G loss: -110.614] \n",
                        "5259 [D loss: (5.087)(R 5.085, F 5.088)]  [G loss: -110.622] \n",
                        "5260 [D loss: (5.087)(R 5.086, F 5.089)]  [G loss: -110.630] \n",
                        "5261 [D loss: (5.088)(R 5.086, F 5.089)]  [G loss: -110.638] \n",
                        "5262 [D loss: (5.088)(R 5.086, F 5.090)]  [G loss: -110.646] \n",
                        "5263 [D loss: (5.088)(R 5.087, F 5.090)]  [G loss: -110.654] \n",
                        "5264 [D loss: (5.089)(R 5.087, F 5.090)]  [G loss: -110.662] \n",
                        "5265 [D loss: (5.089)(R 5.087, F 5.091)]  [G loss: -110.671] \n",
                        "5266 [D loss: (5.089)(R 5.088, F 5.091)]  [G loss: -110.679] \n",
                        "5267 [D loss: (5.090)(R 5.088, F 5.091)]  [G loss: -110.687] \n",
                        "5268 [D loss: (5.090)(R 5.089, F 5.092)]  [G loss: -110.695] \n",
                        "5269 [D loss: (5.090)(R 5.089, F 5.092)]  [G loss: -110.703] \n",
                        "5270 [D loss: (5.091)(R 5.089, F 5.092)]  [G loss: -110.711] \n",
                        "5271 [D loss: (5.091)(R 5.089, F 5.092)]  [G loss: -110.719] \n",
                        "5272 [D loss: (5.091)(R 5.090, F 5.093)]  [G loss: -110.727] \n",
                        "5273 [D loss: (5.092)(R 5.090, F 5.093)]  [G loss: -110.736] \n",
                        "5274 [D loss: (5.092)(R 5.090, F 5.094)]  [G loss: -110.745] \n",
                        "5275 [D loss: (5.092)(R 5.091, F 5.094)]  [G loss: -110.753] \n",
                        "5276 [D loss: (5.093)(R 5.091, F 5.094)]  [G loss: -110.762] \n",
                        "5277 [D loss: (5.093)(R 5.092, F 5.095)]  [G loss: -110.771] \n",
                        "5278 [D loss: (5.094)(R 5.092, F 5.095)]  [G loss: -110.780] \n",
                        "5279 [D loss: (5.094)(R 5.092, F 5.096)]  [G loss: -110.789] \n",
                        "5280 [D loss: (5.094)(R 5.093, F 5.096)]  [G loss: -110.798] \n",
                        "5281 [D loss: (5.095)(R 5.093, F 5.096)]  [G loss: -110.807] \n",
                        "5282 [D loss: (5.095)(R 5.094, F 5.097)]  [G loss: -110.816] \n",
                        "5283 [D loss: (5.096)(R 5.094, F 5.097)]  [G loss: -110.826] \n",
                        "5284 [D loss: (5.096)(R 5.095, F 5.098)]  [G loss: -110.834] \n",
                        "5285 [D loss: (5.097)(R 5.095, F 5.098)]  [G loss: -110.843] \n",
                        "5286 [D loss: (5.097)(R 5.096, F 5.099)]  [G loss: -110.853] \n",
                        "5287 [D loss: (5.098)(R 5.096, F 5.099)]  [G loss: -110.861] \n",
                        "5288 [D loss: (5.098)(R 5.097, F 5.100)]  [G loss: -110.871] \n",
                        "5289 [D loss: (5.099)(R 5.097, F 5.100)]  [G loss: -110.879] \n",
                        "5290 [D loss: (5.099)(R 5.098, F 5.101)]  [G loss: -110.888] \n",
                        "5291 [D loss: (5.100)(R 5.098, F 5.101)]  [G loss: -110.897] \n",
                        "5292 [D loss: (5.100)(R 5.099, F 5.102)]  [G loss: -110.905] \n",
                        "5293 [D loss: (5.101)(R 5.099, F 5.102)]  [G loss: -110.913] \n",
                        "5294 [D loss: (5.101)(R 5.099, F 5.102)]  [G loss: -110.922] \n",
                        "5295 [D loss: (5.101)(R 5.100, F 5.103)]  [G loss: -110.930] \n",
                        "5296 [D loss: (5.102)(R 5.100, F 5.103)]  [G loss: -110.938] \n",
                        "5297 [D loss: (5.102)(R 5.101, F 5.104)]  [G loss: -110.947] \n",
                        "5298 [D loss: (5.103)(R 5.101, F 5.104)]  [G loss: -110.955] \n",
                        "5299 [D loss: (5.103)(R 5.102, F 5.105)]  [G loss: -110.964] \n",
                        "5300 [D loss: (5.104)(R 5.102, F 5.105)]  [G loss: -110.972] \n",
                        "5301 [D loss: (5.104)(R 5.102, F 5.105)]  [G loss: -110.980] \n",
                        "5302 [D loss: (5.104)(R 5.103, F 5.106)]  [G loss: -110.989] \n",
                        "5303 [D loss: (5.105)(R 5.103, F 5.106)]  [G loss: -110.997] \n",
                        "5304 [D loss: (5.105)(R 5.104, F 5.107)]  [G loss: -111.005] \n",
                        "5305 [D loss: (5.105)(R 5.104, F 5.107)]  [G loss: -111.013] \n",
                        "5306 [D loss: (5.106)(R 5.104, F 5.107)]  [G loss: -111.021] \n",
                        "5307 [D loss: (5.106)(R 5.105, F 5.108)]  [G loss: -111.029] \n",
                        "5308 [D loss: (5.106)(R 5.105, F 5.108)]  [G loss: -111.038] \n",
                        "5309 [D loss: (5.107)(R 5.105, F 5.108)]  [G loss: -111.046] \n",
                        "5310 [D loss: (5.107)(R 5.106, F 5.109)]  [G loss: -111.054] \n",
                        "5311 [D loss: (5.107)(R 5.106, F 5.109)]  [G loss: -111.063] \n",
                        "5312 [D loss: (5.108)(R 5.106, F 5.109)]  [G loss: -111.071] \n",
                        "5313 [D loss: (5.108)(R 5.107, F 5.110)]  [G loss: -111.080] \n",
                        "5314 [D loss: (5.109)(R 5.107, F 5.110)]  [G loss: -111.088] \n",
                        "5315 [D loss: (5.109)(R 5.108, F 5.111)]  [G loss: -111.097] \n",
                        "5316 [D loss: (5.109)(R 5.108, F 5.111)]  [G loss: -111.105] \n",
                        "5317 [D loss: (5.110)(R 5.108, F 5.111)]  [G loss: -111.114] \n",
                        "5318 [D loss: (5.110)(R 5.109, F 5.112)]  [G loss: -111.123] \n",
                        "5319 [D loss: (5.111)(R 5.109, F 5.112)]  [G loss: -111.132] \n",
                        "5320 [D loss: (5.111)(R 5.110, F 5.113)]  [G loss: -111.140] \n",
                        "5321 [D loss: (5.112)(R 5.110, F 5.113)]  [G loss: -111.149] \n",
                        "5322 [D loss: (5.112)(R 5.110, F 5.114)]  [G loss: -111.159] \n",
                        "5323 [D loss: (5.113)(R 5.111, F 5.114)]  [G loss: -111.167] \n",
                        "5324 [D loss: (5.113)(R 5.112, F 5.115)]  [G loss: -111.176] \n",
                        "5325 [D loss: (5.114)(R 5.112, F 5.115)]  [G loss: -111.185] \n",
                        "5326 [D loss: (5.114)(R 5.112, F 5.116)]  [G loss: -111.195] \n",
                        "5327 [D loss: (5.115)(R 5.113, F 5.116)]  [G loss: -111.203] \n",
                        "5328 [D loss: (5.115)(R 5.114, F 5.117)]  [G loss: -111.212] \n",
                        "5329 [D loss: (5.116)(R 5.114, F 5.117)]  [G loss: -111.221] \n",
                        "5330 [D loss: (5.116)(R 5.115, F 5.118)]  [G loss: -111.230] \n",
                        "5331 [D loss: (5.117)(R 5.115, F 5.118)]  [G loss: -111.239] \n",
                        "5332 [D loss: (5.117)(R 5.116, F 5.119)]  [G loss: -111.247] \n",
                        "5333 [D loss: (5.118)(R 5.116, F 5.119)]  [G loss: -111.256] \n",
                        "5334 [D loss: (5.118)(R 5.117, F 5.120)]  [G loss: -111.264] \n",
                        "5335 [D loss: (5.119)(R 5.117, F 5.120)]  [G loss: -111.272] \n",
                        "5336 [D loss: (5.119)(R 5.117, F 5.120)]  [G loss: -111.280] \n",
                        "5337 [D loss: (5.119)(R 5.118, F 5.121)]  [G loss: -111.288] \n",
                        "5338 [D loss: (5.120)(R 5.118, F 5.121)]  [G loss: -111.296] \n",
                        "5339 [D loss: (5.120)(R 5.119, F 5.122)]  [G loss: -111.304] \n",
                        "5340 [D loss: (5.120)(R 5.119, F 5.122)]  [G loss: -111.312] \n",
                        "5341 [D loss: (5.121)(R 5.119, F 5.122)]  [G loss: -111.320] \n",
                        "5342 [D loss: (5.121)(R 5.120, F 5.123)]  [G loss: -111.328] \n",
                        "5343 [D loss: (5.121)(R 5.120, F 5.123)]  [G loss: -111.337] \n",
                        "5344 [D loss: (5.122)(R 5.120, F 5.123)]  [G loss: -111.345] \n",
                        "5345 [D loss: (5.122)(R 5.121, F 5.124)]  [G loss: -111.353] \n",
                        "5346 [D loss: (5.123)(R 5.121, F 5.124)]  [G loss: -111.361] \n",
                        "5347 [D loss: (5.123)(R 5.121, F 5.125)]  [G loss: -111.369] \n",
                        "5348 [D loss: (5.123)(R 5.122, F 5.125)]  [G loss: -111.378] \n",
                        "5349 [D loss: (5.124)(R 5.122, F 5.125)]  [G loss: -111.386] \n",
                        "5350 [D loss: (5.124)(R 5.123, F 5.126)]  [G loss: -111.395] \n",
                        "5351 [D loss: (5.125)(R 5.123, F 5.126)]  [G loss: -111.403] \n",
                        "5352 [D loss: (5.125)(R 5.123, F 5.126)]  [G loss: -111.412] \n",
                        "5353 [D loss: (5.125)(R 5.124, F 5.127)]  [G loss: -111.420] \n",
                        "5354 [D loss: (5.126)(R 5.124, F 5.127)]  [G loss: -111.429] \n",
                        "5355 [D loss: (5.126)(R 5.125, F 5.128)]  [G loss: -111.438] \n",
                        "5356 [D loss: (5.127)(R 5.125, F 5.128)]  [G loss: -111.447] \n",
                        "5357 [D loss: (5.127)(R 5.126, F 5.129)]  [G loss: -111.456] \n",
                        "5358 [D loss: (5.128)(R 5.126, F 5.129)]  [G loss: -111.465] \n",
                        "5359 [D loss: (5.128)(R 5.126, F 5.130)]  [G loss: -111.475] \n",
                        "5360 [D loss: (5.129)(R 5.127, F 5.130)]  [G loss: -111.484] \n",
                        "5361 [D loss: (5.129)(R 5.128, F 5.131)]  [G loss: -111.493] \n",
                        "5362 [D loss: (5.130)(R 5.128, F 5.131)]  [G loss: -111.502] \n",
                        "5363 [D loss: (5.130)(R 5.129, F 5.132)]  [G loss: -111.511] \n",
                        "5364 [D loss: (5.131)(R 5.129, F 5.133)]  [G loss: -111.520] \n",
                        "5365 [D loss: (5.131)(R 5.130, F 5.133)]  [G loss: -111.528] \n",
                        "5366 [D loss: (5.132)(R 5.130, F 5.133)]  [G loss: -111.537] \n",
                        "5367 [D loss: (5.132)(R 5.131, F 5.134)]  [G loss: -111.546] \n",
                        "5368 [D loss: (5.133)(R 5.131, F 5.134)]  [G loss: -111.553] \n",
                        "5369 [D loss: (5.133)(R 5.132, F 5.135)]  [G loss: -111.561] \n",
                        "5370 [D loss: (5.134)(R 5.132, F 5.135)]  [G loss: -111.568] \n",
                        "5371 [D loss: (5.134)(R 5.133, F 5.136)]  [G loss: -111.576] \n",
                        "5372 [D loss: (5.134)(R 5.133, F 5.136)]  [G loss: -111.583] \n",
                        "5373 [D loss: (5.135)(R 5.133, F 5.136)]  [G loss: -111.591] \n",
                        "5374 [D loss: (5.135)(R 5.134, F 5.136)]  [G loss: -111.598] \n",
                        "5375 [D loss: (5.135)(R 5.134, F 5.137)]  [G loss: -111.605] \n",
                        "5376 [D loss: (5.136)(R 5.134, F 5.137)]  [G loss: -111.613] \n",
                        "5377 [D loss: (5.136)(R 5.134, F 5.137)]  [G loss: -111.620] \n",
                        "5378 [D loss: (5.136)(R 5.135, F 5.137)]  [G loss: -111.628] \n",
                        "5379 [D loss: (5.136)(R 5.135, F 5.138)]  [G loss: -111.636] \n",
                        "5380 [D loss: (5.137)(R 5.135, F 5.138)]  [G loss: -111.643] \n",
                        "5381 [D loss: (5.137)(R 5.135, F 5.138)]  [G loss: -111.650] \n",
                        "5382 [D loss: (5.137)(R 5.136, F 5.139)]  [G loss: -111.658] \n",
                        "5383 [D loss: (5.137)(R 5.136, F 5.139)]  [G loss: -111.666] \n",
                        "5384 [D loss: (5.138)(R 5.136, F 5.139)]  [G loss: -111.674] \n",
                        "5385 [D loss: (5.138)(R 5.137, F 5.140)]  [G loss: -111.682] \n",
                        "5386 [D loss: (5.139)(R 5.137, F 5.140)]  [G loss: -111.690] \n",
                        "5387 [D loss: (5.139)(R 5.137, F 5.140)]  [G loss: -111.699] \n",
                        "5388 [D loss: (5.139)(R 5.138, F 5.141)]  [G loss: -111.707] \n",
                        "5389 [D loss: (5.140)(R 5.138, F 5.141)]  [G loss: -111.715] \n",
                        "5390 [D loss: (5.140)(R 5.138, F 5.142)]  [G loss: -111.724] \n",
                        "5391 [D loss: (5.140)(R 5.139, F 5.142)]  [G loss: -111.732] \n",
                        "5392 [D loss: (5.141)(R 5.139, F 5.142)]  [G loss: -111.741] \n",
                        "5393 [D loss: (5.141)(R 5.140, F 5.143)]  [G loss: -111.750] \n",
                        "5394 [D loss: (5.142)(R 5.140, F 5.143)]  [G loss: -111.759] \n",
                        "5395 [D loss: (5.142)(R 5.140, F 5.143)]  [G loss: -111.768] \n",
                        "5396 [D loss: (5.142)(R 5.141, F 5.144)]  [G loss: -111.778] \n",
                        "5397 [D loss: (5.143)(R 5.142, F 5.145)]  [G loss: -111.787] \n",
                        "5398 [D loss: (5.144)(R 5.142, F 5.145)]  [G loss: -111.796] \n",
                        "5399 [D loss: (5.144)(R 5.142, F 5.146)]  [G loss: -111.805] \n",
                        "5400 [D loss: (5.145)(R 5.143, F 5.146)]  [G loss: -111.814] \n",
                        "5401 [D loss: (5.145)(R 5.144, F 5.147)]  [G loss: -111.823] \n",
                        "5402 [D loss: (5.146)(R 5.144, F 5.147)]  [G loss: -111.831] \n",
                        "5403 [D loss: (5.146)(R 5.144, F 5.148)]  [G loss: -111.840] \n",
                        "5404 [D loss: (5.146)(R 5.145, F 5.148)]  [G loss: -111.849] \n",
                        "5405 [D loss: (5.147)(R 5.145, F 5.148)]  [G loss: -111.858] \n",
                        "5406 [D loss: (5.148)(R 5.146, F 5.149)]  [G loss: -111.865] \n",
                        "5407 [D loss: (5.148)(R 5.146, F 5.149)]  [G loss: -111.873] \n",
                        "5408 [D loss: (5.148)(R 5.147, F 5.150)]  [G loss: -111.881] \n",
                        "5409 [D loss: (5.149)(R 5.147, F 5.150)]  [G loss: -111.889] \n",
                        "5410 [D loss: (5.149)(R 5.148, F 5.151)]  [G loss: -111.896] \n",
                        "5411 [D loss: (5.149)(R 5.148, F 5.151)]  [G loss: -111.903] \n",
                        "5412 [D loss: (5.150)(R 5.148, F 5.151)]  [G loss: -111.911] \n",
                        "5413 [D loss: (5.150)(R 5.148, F 5.151)]  [G loss: -111.918] \n",
                        "5414 [D loss: (5.150)(R 5.149, F 5.152)]  [G loss: -111.925] \n",
                        "5415 [D loss: (5.150)(R 5.149, F 5.152)]  [G loss: -111.933] \n",
                        "5416 [D loss: (5.151)(R 5.149, F 5.152)]  [G loss: -111.940] \n",
                        "5417 [D loss: (5.151)(R 5.149, F 5.152)]  [G loss: -111.948] \n",
                        "5418 [D loss: (5.151)(R 5.150, F 5.153)]  [G loss: -111.955] \n",
                        "5419 [D loss: (5.151)(R 5.150, F 5.153)]  [G loss: -111.962] \n",
                        "5420 [D loss: (5.152)(R 5.150, F 5.153)]  [G loss: -111.970] \n",
                        "5421 [D loss: (5.152)(R 5.151, F 5.153)]  [G loss: -111.978] \n",
                        "5422 [D loss: (5.152)(R 5.151, F 5.154)]  [G loss: -111.986] \n",
                        "5423 [D loss: (5.153)(R 5.151, F 5.154)]  [G loss: -111.993] \n",
                        "5424 [D loss: (5.153)(R 5.152, F 5.155)]  [G loss: -112.001] \n",
                        "5425 [D loss: (5.153)(R 5.152, F 5.155)]  [G loss: -112.009] \n",
                        "5426 [D loss: (5.154)(R 5.152, F 5.155)]  [G loss: -112.017] \n",
                        "5427 [D loss: (5.154)(R 5.153, F 5.156)]  [G loss: -112.025] \n",
                        "5428 [D loss: (5.154)(R 5.153, F 5.156)]  [G loss: -112.033] \n",
                        "5429 [D loss: (5.155)(R 5.153, F 5.156)]  [G loss: -112.042] \n",
                        "5430 [D loss: (5.155)(R 5.154, F 5.157)]  [G loss: -112.050] \n",
                        "5431 [D loss: (5.156)(R 5.154, F 5.157)]  [G loss: -112.059] \n",
                        "5432 [D loss: (5.156)(R 5.155, F 5.158)]  [G loss: -112.067] \n",
                        "5433 [D loss: (5.157)(R 5.155, F 5.158)]  [G loss: -112.076] \n",
                        "5434 [D loss: (5.157)(R 5.156, F 5.159)]  [G loss: -112.084] \n",
                        "5435 [D loss: (5.157)(R 5.156, F 5.159)]  [G loss: -112.093] \n",
                        "5436 [D loss: (5.158)(R 5.156, F 5.159)]  [G loss: -112.101] \n",
                        "5437 [D loss: (5.158)(R 5.157, F 5.160)]  [G loss: -112.110] \n",
                        "5438 [D loss: (5.159)(R 5.157, F 5.160)]  [G loss: -112.119] \n",
                        "5439 [D loss: (5.159)(R 5.158, F 5.161)]  [G loss: -112.127] \n",
                        "5440 [D loss: (5.160)(R 5.158, F 5.161)]  [G loss: -112.136] \n",
                        "5441 [D loss: (5.160)(R 5.158, F 5.162)]  [G loss: -112.144] \n",
                        "5442 [D loss: (5.160)(R 5.159, F 5.162)]  [G loss: -112.153] \n",
                        "5443 [D loss: (5.161)(R 5.159, F 5.163)]  [G loss: -112.161] \n",
                        "5444 [D loss: (5.161)(R 5.160, F 5.163)]  [G loss: -112.170] \n",
                        "5445 [D loss: (5.162)(R 5.160, F 5.163)]  [G loss: -112.178] \n",
                        "5446 [D loss: (5.162)(R 5.161, F 5.164)]  [G loss: -112.186] \n",
                        "5447 [D loss: (5.163)(R 5.161, F 5.164)]  [G loss: -112.194] \n",
                        "5448 [D loss: (5.163)(R 5.162, F 5.165)]  [G loss: -112.202] \n",
                        "5449 [D loss: (5.163)(R 5.162, F 5.165)]  [G loss: -112.210] \n",
                        "5450 [D loss: (5.164)(R 5.162, F 5.165)]  [G loss: -112.217] \n",
                        "5451 [D loss: (5.164)(R 5.163, F 5.166)]  [G loss: -112.225] \n",
                        "5452 [D loss: (5.164)(R 5.163, F 5.166)]  [G loss: -112.232] \n",
                        "5453 [D loss: (5.165)(R 5.163, F 5.166)]  [G loss: -112.240] \n",
                        "5454 [D loss: (5.165)(R 5.164, F 5.167)]  [G loss: -112.248] \n",
                        "5455 [D loss: (5.165)(R 5.164, F 5.167)]  [G loss: -112.255] \n",
                        "5456 [D loss: (5.166)(R 5.164, F 5.167)]  [G loss: -112.263] \n",
                        "5457 [D loss: (5.166)(R 5.164, F 5.167)]  [G loss: -112.271] \n",
                        "5458 [D loss: (5.166)(R 5.165, F 5.168)]  [G loss: -112.278] \n",
                        "5459 [D loss: (5.167)(R 5.165, F 5.168)]  [G loss: -112.286] \n",
                        "5460 [D loss: (5.167)(R 5.165, F 5.168)]  [G loss: -112.294] \n",
                        "5461 [D loss: (5.167)(R 5.166, F 5.169)]  [G loss: -112.302] \n",
                        "5462 [D loss: (5.168)(R 5.166, F 5.169)]  [G loss: -112.310] \n",
                        "5463 [D loss: (5.168)(R 5.166, F 5.169)]  [G loss: -112.318] \n",
                        "5464 [D loss: (5.168)(R 5.167, F 5.170)]  [G loss: -112.326] \n",
                        "5465 [D loss: (5.169)(R 5.167, F 5.170)]  [G loss: -112.334] \n",
                        "5466 [D loss: (5.169)(R 5.167, F 5.170)]  [G loss: -112.342] \n",
                        "5467 [D loss: (5.169)(R 5.168, F 5.171)]  [G loss: -112.351] \n",
                        "5468 [D loss: (5.170)(R 5.168, F 5.171)]  [G loss: -112.359] \n",
                        "5469 [D loss: (5.170)(R 5.169, F 5.172)]  [G loss: -112.367] \n",
                        "5470 [D loss: (5.171)(R 5.169, F 5.172)]  [G loss: -112.376] \n",
                        "5471 [D loss: (5.171)(R 5.169, F 5.172)]  [G loss: -112.385] \n",
                        "5472 [D loss: (5.171)(R 5.170, F 5.173)]  [G loss: -112.394] \n",
                        "5473 [D loss: (5.172)(R 5.170, F 5.174)]  [G loss: -112.402] \n",
                        "5474 [D loss: (5.172)(R 5.171, F 5.174)]  [G loss: -112.411] \n",
                        "5475 [D loss: (5.173)(R 5.171, F 5.174)]  [G loss: -112.420] \n",
                        "5476 [D loss: (5.173)(R 5.172, F 5.175)]  [G loss: -112.429] \n",
                        "5477 [D loss: (5.174)(R 5.172, F 5.175)]  [G loss: -112.438] \n",
                        "5478 [D loss: (5.174)(R 5.173, F 5.176)]  [G loss: -112.448] \n",
                        "5479 [D loss: (5.175)(R 5.173, F 5.177)]  [G loss: -112.455] \n",
                        "5480 [D loss: (5.175)(R 5.174, F 5.177)]  [G loss: -112.464] \n",
                        "5481 [D loss: (5.176)(R 5.174, F 5.177)]  [G loss: -112.472] \n",
                        "5482 [D loss: (5.176)(R 5.175, F 5.178)]  [G loss: -112.479] \n",
                        "5483 [D loss: (5.177)(R 5.175, F 5.178)]  [G loss: -112.487] \n",
                        "5484 [D loss: (5.177)(R 5.176, F 5.178)]  [G loss: -112.495] \n",
                        "5485 [D loss: (5.177)(R 5.176, F 5.179)]  [G loss: -112.502] \n",
                        "5486 [D loss: (5.178)(R 5.176, F 5.179)]  [G loss: -112.509] \n",
                        "5487 [D loss: (5.178)(R 5.176, F 5.179)]  [G loss: -112.517] \n",
                        "5488 [D loss: (5.178)(R 5.177, F 5.180)]  [G loss: -112.524] \n",
                        "5489 [D loss: (5.179)(R 5.177, F 5.180)]  [G loss: -112.531] \n",
                        "5490 [D loss: (5.179)(R 5.177, F 5.180)]  [G loss: -112.538] \n",
                        "5491 [D loss: (5.179)(R 5.178, F 5.181)]  [G loss: -112.546] \n",
                        "5492 [D loss: (5.179)(R 5.178, F 5.181)]  [G loss: -112.553] \n",
                        "5493 [D loss: (5.180)(R 5.178, F 5.181)]  [G loss: -112.561] \n",
                        "5494 [D loss: (5.180)(R 5.179, F 5.182)]  [G loss: -112.568] \n",
                        "5495 [D loss: (5.180)(R 5.179, F 5.182)]  [G loss: -112.576] \n",
                        "5496 [D loss: (5.181)(R 5.179, F 5.182)]  [G loss: -112.584] \n",
                        "5497 [D loss: (5.181)(R 5.179, F 5.182)]  [G loss: -112.591] \n",
                        "5498 [D loss: (5.181)(R 5.180, F 5.183)]  [G loss: -112.599] \n",
                        "5499 [D loss: (5.181)(R 5.180, F 5.183)]  [G loss: -112.607] \n",
                        "5500 [D loss: (5.182)(R 5.180, F 5.183)]  [G loss: -112.615] \n",
                        "5501 [D loss: (5.182)(R 5.181, F 5.184)]  [G loss: -112.623] \n",
                        "5502 [D loss: (5.182)(R 5.181, F 5.184)]  [G loss: -112.631] \n",
                        "5503 [D loss: (5.183)(R 5.181, F 5.184)]  [G loss: -112.639] \n",
                        "5504 [D loss: (5.183)(R 5.182, F 5.185)]  [G loss: -112.648] \n",
                        "5505 [D loss: (5.184)(R 5.182, F 5.185)]  [G loss: -112.656] \n",
                        "5506 [D loss: (5.184)(R 5.182, F 5.185)]  [G loss: -112.665] \n",
                        "5507 [D loss: (5.185)(R 5.183, F 5.186)]  [G loss: -112.673] \n",
                        "5508 [D loss: (5.185)(R 5.184, F 5.187)]  [G loss: -112.682] \n",
                        "5509 [D loss: (5.186)(R 5.184, F 5.187)]  [G loss: -112.690] \n",
                        "5510 [D loss: (5.186)(R 5.184, F 5.187)]  [G loss: -112.699] \n",
                        "5511 [D loss: (5.186)(R 5.185, F 5.188)]  [G loss: -112.707] \n",
                        "5512 [D loss: (5.187)(R 5.185, F 5.188)]  [G loss: -112.715] \n",
                        "5513 [D loss: (5.187)(R 5.186, F 5.189)]  [G loss: -112.724] \n",
                        "5514 [D loss: (5.188)(R 5.186, F 5.189)]  [G loss: -112.732] \n",
                        "5515 [D loss: (5.188)(R 5.187, F 5.190)]  [G loss: -112.740] \n",
                        "5516 [D loss: (5.189)(R 5.187, F 5.190)]  [G loss: -112.749] \n",
                        "5517 [D loss: (5.189)(R 5.187, F 5.190)]  [G loss: -112.757] \n",
                        "5518 [D loss: (5.189)(R 5.188, F 5.191)]  [G loss: -112.765] \n",
                        "5519 [D loss: (5.190)(R 5.188, F 5.191)]  [G loss: -112.773] \n",
                        "5520 [D loss: (5.190)(R 5.189, F 5.192)]  [G loss: -112.781] \n",
                        "5521 [D loss: (5.191)(R 5.189, F 5.192)]  [G loss: -112.788] \n",
                        "5522 [D loss: (5.191)(R 5.189, F 5.192)]  [G loss: -112.796] \n",
                        "5523 [D loss: (5.192)(R 5.190, F 5.193)]  [G loss: -112.804] \n",
                        "5524 [D loss: (5.192)(R 5.190, F 5.193)]  [G loss: -112.811] \n",
                        "5525 [D loss: (5.192)(R 5.191, F 5.194)]  [G loss: -112.818] \n",
                        "5526 [D loss: (5.192)(R 5.191, F 5.194)]  [G loss: -112.826] \n",
                        "5527 [D loss: (5.193)(R 5.191, F 5.194)]  [G loss: -112.834] \n",
                        "5528 [D loss: (5.193)(R 5.192, F 5.195)]  [G loss: -112.841] \n",
                        "5529 [D loss: (5.193)(R 5.192, F 5.195)]  [G loss: -112.849] \n",
                        "5530 [D loss: (5.194)(R 5.192, F 5.195)]  [G loss: -112.856] \n",
                        "5531 [D loss: (5.194)(R 5.193, F 5.196)]  [G loss: -112.864] \n",
                        "5532 [D loss: (5.194)(R 5.193, F 5.196)]  [G loss: -112.872] \n",
                        "5533 [D loss: (5.195)(R 5.193, F 5.196)]  [G loss: -112.880] \n",
                        "5534 [D loss: (5.195)(R 5.194, F 5.197)]  [G loss: -112.887] \n",
                        "5535 [D loss: (5.195)(R 5.194, F 5.197)]  [G loss: -112.895] \n",
                        "5536 [D loss: (5.196)(R 5.194, F 5.197)]  [G loss: -112.903] \n",
                        "5537 [D loss: (5.196)(R 5.195, F 5.198)]  [G loss: -112.911] \n",
                        "5538 [D loss: (5.197)(R 5.195, F 5.198)]  [G loss: -112.919] \n",
                        "5539 [D loss: (5.197)(R 5.195, F 5.198)]  [G loss: -112.927] \n",
                        "5540 [D loss: (5.197)(R 5.196, F 5.199)]  [G loss: -112.935] \n",
                        "5541 [D loss: (5.198)(R 5.196, F 5.199)]  [G loss: -112.943] \n",
                        "5542 [D loss: (5.198)(R 5.196, F 5.199)]  [G loss: -112.951] \n",
                        "5543 [D loss: (5.198)(R 5.197, F 5.200)]  [G loss: -112.959] \n",
                        "5544 [D loss: (5.199)(R 5.197, F 5.200)]  [G loss: -112.967] \n",
                        "5545 [D loss: (5.199)(R 5.198, F 5.201)]  [G loss: -112.976] \n",
                        "5546 [D loss: (5.200)(R 5.198, F 5.201)]  [G loss: -112.983] \n",
                        "5547 [D loss: (5.200)(R 5.199, F 5.201)]  [G loss: -112.990] \n",
                        "5548 [D loss: (5.200)(R 5.199, F 5.202)]  [G loss: -112.998] \n",
                        "5549 [D loss: (5.201)(R 5.199, F 5.202)]  [G loss: -113.006] \n",
                        "5550 [D loss: (5.201)(R 5.200, F 5.203)]  [G loss: -113.014] \n",
                        "5551 [D loss: (5.202)(R 5.200, F 5.203)]  [G loss: -113.021] \n",
                        "5552 [D loss: (5.202)(R 5.200, F 5.203)]  [G loss: -113.029] \n",
                        "5553 [D loss: (5.202)(R 5.201, F 5.204)]  [G loss: -113.038] \n",
                        "5554 [D loss: (5.203)(R 5.201, F 5.204)]  [G loss: -113.046] \n",
                        "5555 [D loss: (5.203)(R 5.201, F 5.204)]  [G loss: -113.054] \n",
                        "5556 [D loss: (5.203)(R 5.202, F 5.205)]  [G loss: -113.062] \n",
                        "5557 [D loss: (5.204)(R 5.202, F 5.205)]  [G loss: -113.070] \n",
                        "5558 [D loss: (5.204)(R 5.203, F 5.206)]  [G loss: -113.078] \n",
                        "5559 [D loss: (5.205)(R 5.203, F 5.206)]  [G loss: -113.087] \n",
                        "5560 [D loss: (5.205)(R 5.204, F 5.207)]  [G loss: -113.094] \n",
                        "5561 [D loss: (5.206)(R 5.204, F 5.207)]  [G loss: -113.102] \n",
                        "5562 [D loss: (5.206)(R 5.204, F 5.207)]  [G loss: -113.110] \n",
                        "5563 [D loss: (5.206)(R 5.205, F 5.208)]  [G loss: -113.118] \n",
                        "5564 [D loss: (5.207)(R 5.205, F 5.208)]  [G loss: -113.126] \n",
                        "5565 [D loss: (5.207)(R 5.206, F 5.209)]  [G loss: -113.134] \n",
                        "5566 [D loss: (5.207)(R 5.206, F 5.209)]  [G loss: -113.142] \n",
                        "5567 [D loss: (5.208)(R 5.206, F 5.209)]  [G loss: -113.149] \n",
                        "5568 [D loss: (5.208)(R 5.207, F 5.210)]  [G loss: -113.157] \n",
                        "5569 [D loss: (5.209)(R 5.207, F 5.210)]  [G loss: -113.164] \n",
                        "5570 [D loss: (5.209)(R 5.207, F 5.210)]  [G loss: -113.171] \n",
                        "5571 [D loss: (5.209)(R 5.208, F 5.211)]  [G loss: -113.179] \n",
                        "5572 [D loss: (5.209)(R 5.208, F 5.211)]  [G loss: -113.186] \n",
                        "5573 [D loss: (5.210)(R 5.208, F 5.211)]  [G loss: -113.193] \n",
                        "5574 [D loss: (5.210)(R 5.208, F 5.211)]  [G loss: -113.201] \n",
                        "5575 [D loss: (5.210)(R 5.209, F 5.212)]  [G loss: -113.208] \n",
                        "5576 [D loss: (5.211)(R 5.209, F 5.212)]  [G loss: -113.216] \n",
                        "5577 [D loss: (5.211)(R 5.209, F 5.212)]  [G loss: -113.223] \n",
                        "5578 [D loss: (5.211)(R 5.210, F 5.213)]  [G loss: -113.231] \n",
                        "5579 [D loss: (5.211)(R 5.210, F 5.213)]  [G loss: -113.239] \n",
                        "5580 [D loss: (5.212)(R 5.210, F 5.213)]  [G loss: -113.247] \n",
                        "5581 [D loss: (5.212)(R 5.211, F 5.214)]  [G loss: -113.255] \n",
                        "5582 [D loss: (5.212)(R 5.211, F 5.214)]  [G loss: -113.264] \n",
                        "5583 [D loss: (5.213)(R 5.211, F 5.214)]  [G loss: -113.272] \n",
                        "5584 [D loss: (5.213)(R 5.212, F 5.215)]  [G loss: -113.280] \n",
                        "5585 [D loss: (5.213)(R 5.212, F 5.215)]  [G loss: -113.289] \n",
                        "5586 [D loss: (5.214)(R 5.212, F 5.216)]  [G loss: -113.298] \n",
                        "5587 [D loss: (5.215)(R 5.213, F 5.216)]  [G loss: -113.306] \n",
                        "5588 [D loss: (5.215)(R 5.213, F 5.216)]  [G loss: -113.314] \n",
                        "5589 [D loss: (5.215)(R 5.214, F 5.217)]  [G loss: -113.323] \n",
                        "5590 [D loss: (5.216)(R 5.214, F 5.217)]  [G loss: -113.332] \n",
                        "5591 [D loss: (5.216)(R 5.215, F 5.218)]  [G loss: -113.340] \n",
                        "5592 [D loss: (5.217)(R 5.215, F 5.218)]  [G loss: -113.348] \n",
                        "5593 [D loss: (5.217)(R 5.216, F 5.219)]  [G loss: -113.356] \n",
                        "5594 [D loss: (5.218)(R 5.216, F 5.219)]  [G loss: -113.364] \n",
                        "5595 [D loss: (5.218)(R 5.217, F 5.220)]  [G loss: -113.372] \n",
                        "5596 [D loss: (5.218)(R 5.217, F 5.220)]  [G loss: -113.380] \n",
                        "5597 [D loss: (5.219)(R 5.217, F 5.220)]  [G loss: -113.387] \n",
                        "5598 [D loss: (5.219)(R 5.218, F 5.221)]  [G loss: -113.395] \n",
                        "5599 [D loss: (5.219)(R 5.218, F 5.221)]  [G loss: -113.402] \n",
                        "5600 [D loss: (5.220)(R 5.218, F 5.221)]  [G loss: -113.410] \n",
                        "5601 [D loss: (5.220)(R 5.219, F 5.222)]  [G loss: -113.417] \n",
                        "5602 [D loss: (5.220)(R 5.219, F 5.222)]  [G loss: -113.424] \n",
                        "5603 [D loss: (5.221)(R 5.219, F 5.222)]  [G loss: -113.431] \n",
                        "5604 [D loss: (5.221)(R 5.219, F 5.222)]  [G loss: -113.439] \n",
                        "5605 [D loss: (5.221)(R 5.220, F 5.223)]  [G loss: -113.446] \n",
                        "5606 [D loss: (5.222)(R 5.220, F 5.223)]  [G loss: -113.454] \n",
                        "5607 [D loss: (5.222)(R 5.220, F 5.223)]  [G loss: -113.461] \n",
                        "5608 [D loss: (5.222)(R 5.221, F 5.223)]  [G loss: -113.469] \n",
                        "5609 [D loss: (5.222)(R 5.221, F 5.224)]  [G loss: -113.477] \n",
                        "5610 [D loss: (5.223)(R 5.221, F 5.224)]  [G loss: -113.485] \n",
                        "5611 [D loss: (5.223)(R 5.221, F 5.224)]  [G loss: -113.493] \n",
                        "5612 [D loss: (5.223)(R 5.222, F 5.225)]  [G loss: -113.501] \n",
                        "5613 [D loss: (5.223)(R 5.222, F 5.225)]  [G loss: -113.510] \n",
                        "5614 [D loss: (5.224)(R 5.223, F 5.225)]  [G loss: -113.518] \n",
                        "5615 [D loss: (5.224)(R 5.223, F 5.226)]  [G loss: -113.526] \n",
                        "5616 [D loss: (5.225)(R 5.223, F 5.226)]  [G loss: -113.534] \n",
                        "5617 [D loss: (5.225)(R 5.224, F 5.227)]  [G loss: -113.542] \n",
                        "5618 [D loss: (5.225)(R 5.224, F 5.227)]  [G loss: -113.550] \n",
                        "5619 [D loss: (5.226)(R 5.224, F 5.227)]  [G loss: -113.559] \n",
                        "5620 [D loss: (5.226)(R 5.225, F 5.228)]  [G loss: -113.567] \n",
                        "5621 [D loss: (5.227)(R 5.225, F 5.228)]  [G loss: -113.575] \n",
                        "5622 [D loss: (5.227)(R 5.226, F 5.229)]  [G loss: -113.583] \n",
                        "5623 [D loss: (5.227)(R 5.226, F 5.229)]  [G loss: -113.592] \n",
                        "5624 [D loss: (5.228)(R 5.227, F 5.229)]  [G loss: -113.599] \n",
                        "5625 [D loss: (5.228)(R 5.227, F 5.230)]  [G loss: -113.607] \n",
                        "5626 [D loss: (5.229)(R 5.227, F 5.230)]  [G loss: -113.615] \n",
                        "5627 [D loss: (5.229)(R 5.228, F 5.230)]  [G loss: -113.623] \n",
                        "5628 [D loss: (5.229)(R 5.228, F 5.231)]  [G loss: -113.630] \n",
                        "5629 [D loss: (5.230)(R 5.228, F 5.231)]  [G loss: -113.638] \n",
                        "5630 [D loss: (5.230)(R 5.229, F 5.232)]  [G loss: -113.645] \n",
                        "5631 [D loss: (5.230)(R 5.229, F 5.232)]  [G loss: -113.653] \n",
                        "5632 [D loss: (5.231)(R 5.229, F 5.232)]  [G loss: -113.660] \n",
                        "5633 [D loss: (5.231)(R 5.229, F 5.232)]  [G loss: -113.667] \n",
                        "5634 [D loss: (5.231)(R 5.230, F 5.233)]  [G loss: -113.675] \n",
                        "5635 [D loss: (5.231)(R 5.230, F 5.233)]  [G loss: -113.682] \n",
                        "5636 [D loss: (5.232)(R 5.230, F 5.233)]  [G loss: -113.690] \n",
                        "5637 [D loss: (5.232)(R 5.231, F 5.233)]  [G loss: -113.697] \n",
                        "5638 [D loss: (5.232)(R 5.231, F 5.234)]  [G loss: -113.704] \n",
                        "5639 [D loss: (5.233)(R 5.231, F 5.234)]  [G loss: -113.712] \n",
                        "5640 [D loss: (5.233)(R 5.231, F 5.234)]  [G loss: -113.719] \n",
                        "5641 [D loss: (5.233)(R 5.232, F 5.235)]  [G loss: -113.727] \n",
                        "5642 [D loss: (5.233)(R 5.232, F 5.235)]  [G loss: -113.734] \n",
                        "5643 [D loss: (5.234)(R 5.232, F 5.235)]  [G loss: -113.742] \n",
                        "5644 [D loss: (5.234)(R 5.232, F 5.235)]  [G loss: -113.750] \n",
                        "5645 [D loss: (5.234)(R 5.233, F 5.236)]  [G loss: -113.758] \n",
                        "5646 [D loss: (5.234)(R 5.233, F 5.236)]  [G loss: -113.766] \n",
                        "5647 [D loss: (5.235)(R 5.233, F 5.236)]  [G loss: -113.774] \n",
                        "5648 [D loss: (5.235)(R 5.234, F 5.237)]  [G loss: -113.783] \n",
                        "5649 [D loss: (5.236)(R 5.234, F 5.237)]  [G loss: -113.792] \n",
                        "5650 [D loss: (5.236)(R 5.235, F 5.238)]  [G loss: -113.800] \n",
                        "5651 [D loss: (5.237)(R 5.235, F 5.238)]  [G loss: -113.809] \n",
                        "5652 [D loss: (5.237)(R 5.236, F 5.239)]  [G loss: -113.817] \n",
                        "5653 [D loss: (5.238)(R 5.236, F 5.239)]  [G loss: -113.825] \n",
                        "5654 [D loss: (5.238)(R 5.237, F 5.240)]  [G loss: -113.834] \n",
                        "5655 [D loss: (5.239)(R 5.237, F 5.240)]  [G loss: -113.842] \n",
                        "5656 [D loss: (5.239)(R 5.237, F 5.240)]  [G loss: -113.850] \n",
                        "5657 [D loss: (5.239)(R 5.238, F 5.241)]  [G loss: -113.858] \n",
                        "5658 [D loss: (5.240)(R 5.238, F 5.241)]  [G loss: -113.867] \n",
                        "5659 [D loss: (5.240)(R 5.239, F 5.242)]  [G loss: -113.874] \n",
                        "5660 [D loss: (5.241)(R 5.239, F 5.242)]  [G loss: -113.882] \n",
                        "5661 [D loss: (5.241)(R 5.240, F 5.242)]  [G loss: -113.890] \n",
                        "5662 [D loss: (5.241)(R 5.240, F 5.243)]  [G loss: -113.897] \n",
                        "5663 [D loss: (5.242)(R 5.240, F 5.243)]  [G loss: -113.905] \n",
                        "5664 [D loss: (5.242)(R 5.241, F 5.244)]  [G loss: -113.912] \n",
                        "5665 [D loss: (5.242)(R 5.241, F 5.244)]  [G loss: -113.918] \n",
                        "5666 [D loss: (5.243)(R 5.241, F 5.244)]  [G loss: -113.926] \n",
                        "5667 [D loss: (5.243)(R 5.242, F 5.245)]  [G loss: -113.931] \n",
                        "5668 [D loss: (5.243)(R 5.242, F 5.245)]  [G loss: -113.938] \n",
                        "5669 [D loss: (5.243)(R 5.242, F 5.245)]  [G loss: -113.944] \n",
                        "5670 [D loss: (5.244)(R 5.242, F 5.245)]  [G loss: -113.951] \n",
                        "5671 [D loss: (5.244)(R 5.242, F 5.245)]  [G loss: -113.958] \n",
                        "5672 [D loss: (5.244)(R 5.243, F 5.245)]  [G loss: -113.965] \n",
                        "5673 [D loss: (5.244)(R 5.243, F 5.246)]  [G loss: -113.972] \n",
                        "5674 [D loss: (5.244)(R 5.243, F 5.246)]  [G loss: -113.979] \n",
                        "5675 [D loss: (5.245)(R 5.243, F 5.246)]  [G loss: -113.986] \n",
                        "5676 [D loss: (5.245)(R 5.244, F 5.246)]  [G loss: -113.993] \n",
                        "5677 [D loss: (5.245)(R 5.244, F 5.247)]  [G loss: -114.001] \n",
                        "5678 [D loss: (5.246)(R 5.244, F 5.247)]  [G loss: -114.008] \n",
                        "5679 [D loss: (5.246)(R 5.244, F 5.247)]  [G loss: -114.015] \n",
                        "5680 [D loss: (5.246)(R 5.245, F 5.247)]  [G loss: -114.023] \n",
                        "5681 [D loss: (5.246)(R 5.245, F 5.248)]  [G loss: -114.031] \n",
                        "5682 [D loss: (5.247)(R 5.245, F 5.248)]  [G loss: -114.039] \n",
                        "5683 [D loss: (5.247)(R 5.246, F 5.249)]  [G loss: -114.047] \n",
                        "5684 [D loss: (5.248)(R 5.246, F 5.249)]  [G loss: -114.055] \n",
                        "5685 [D loss: (5.248)(R 5.246, F 5.249)]  [G loss: -114.064] \n",
                        "5686 [D loss: (5.248)(R 5.247, F 5.250)]  [G loss: -114.072] \n",
                        "5687 [D loss: (5.249)(R 5.247, F 5.250)]  [G loss: -114.081] \n",
                        "5688 [D loss: (5.249)(R 5.248, F 5.251)]  [G loss: -114.089] \n",
                        "5689 [D loss: (5.250)(R 5.248, F 5.251)]  [G loss: -114.098] \n",
                        "5690 [D loss: (5.250)(R 5.249, F 5.252)]  [G loss: -114.106] \n",
                        "5691 [D loss: (5.251)(R 5.249, F 5.252)]  [G loss: -114.114] \n",
                        "5692 [D loss: (5.251)(R 5.250, F 5.253)]  [G loss: -114.122] \n",
                        "5693 [D loss: (5.251)(R 5.250, F 5.253)]  [G loss: -114.130] \n",
                        "5694 [D loss: (5.252)(R 5.250, F 5.253)]  [G loss: -114.139] \n",
                        "5695 [D loss: (5.252)(R 5.251, F 5.254)]  [G loss: -114.146] \n",
                        "5696 [D loss: (5.253)(R 5.251, F 5.254)]  [G loss: -114.154] \n",
                        "5697 [D loss: (5.253)(R 5.252, F 5.255)]  [G loss: -114.162] \n",
                        "5698 [D loss: (5.254)(R 5.252, F 5.255)]  [G loss: -114.170] \n",
                        "5699 [D loss: (5.254)(R 5.252, F 5.255)]  [G loss: -114.178] \n",
                        "5700 [D loss: (5.254)(R 5.253, F 5.256)]  [G loss: -114.184] \n",
                        "5701 [D loss: (5.255)(R 5.253, F 5.256)]  [G loss: -114.191] \n",
                        "5702 [D loss: (5.255)(R 5.253, F 5.256)]  [G loss: -114.198] \n",
                        "5703 [D loss: (5.255)(R 5.254, F 5.257)]  [G loss: -114.204] \n",
                        "5704 [D loss: (5.255)(R 5.254, F 5.257)]  [G loss: -114.210] \n",
                        "5705 [D loss: (5.255)(R 5.254, F 5.257)]  [G loss: -114.217] \n",
                        "5706 [D loss: (5.256)(R 5.254, F 5.257)]  [G loss: -114.224] \n",
                        "5707 [D loss: (5.256)(R 5.255, F 5.257)]  [G loss: -114.230] \n",
                        "5708 [D loss: (5.256)(R 5.255, F 5.257)]  [G loss: -114.236] \n",
                        "5709 [D loss: (5.256)(R 5.255, F 5.257)]  [G loss: -114.243] \n",
                        "5710 [D loss: (5.256)(R 5.255, F 5.258)]  [G loss: -114.250] \n",
                        "5711 [D loss: (5.256)(R 5.255, F 5.258)]  [G loss: -114.256] \n",
                        "5712 [D loss: (5.257)(R 5.255, F 5.258)]  [G loss: -114.263] \n",
                        "5713 [D loss: (5.257)(R 5.255, F 5.258)]  [G loss: -114.270] \n",
                        "5714 [D loss: (5.257)(R 5.256, F 5.258)]  [G loss: -114.277] \n",
                        "5715 [D loss: (5.257)(R 5.256, F 5.259)]  [G loss: -114.285] \n",
                        "5716 [D loss: (5.257)(R 5.256, F 5.259)]  [G loss: -114.292] \n",
                        "5717 [D loss: (5.258)(R 5.256, F 5.259)]  [G loss: -114.300] \n",
                        "5718 [D loss: (5.258)(R 5.257, F 5.260)]  [G loss: -114.307] \n",
                        "5719 [D loss: (5.258)(R 5.257, F 5.260)]  [G loss: -114.315] \n",
                        "5720 [D loss: (5.259)(R 5.257, F 5.260)]  [G loss: -114.323] \n",
                        "5721 [D loss: (5.259)(R 5.258, F 5.261)]  [G loss: -114.331] \n",
                        "5722 [D loss: (5.259)(R 5.258, F 5.261)]  [G loss: -114.339] \n",
                        "5723 [D loss: (5.260)(R 5.258, F 5.261)]  [G loss: -114.347] \n",
                        "5724 [D loss: (5.260)(R 5.259, F 5.262)]  [G loss: -114.355] \n",
                        "5725 [D loss: (5.261)(R 5.259, F 5.262)]  [G loss: -114.364] \n",
                        "5726 [D loss: (5.261)(R 5.259, F 5.262)]  [G loss: -114.372] \n",
                        "5727 [D loss: (5.261)(R 5.260, F 5.263)]  [G loss: -114.381] \n",
                        "5728 [D loss: (5.262)(R 5.260, F 5.263)]  [G loss: -114.389] \n",
                        "5729 [D loss: (5.262)(R 5.261, F 5.264)]  [G loss: -114.398] \n",
                        "5730 [D loss: (5.263)(R 5.261, F 5.264)]  [G loss: -114.407] \n",
                        "5731 [D loss: (5.263)(R 5.262, F 5.265)]  [G loss: -114.416] \n",
                        "5732 [D loss: (5.264)(R 5.262, F 5.265)]  [G loss: -114.425] \n",
                        "5733 [D loss: (5.264)(R 5.263, F 5.266)]  [G loss: -114.433] \n",
                        "5734 [D loss: (5.265)(R 5.263, F 5.266)]  [G loss: -114.441] \n",
                        "5735 [D loss: (5.265)(R 5.264, F 5.267)]  [G loss: -114.449] \n",
                        "5736 [D loss: (5.266)(R 5.264, F 5.267)]  [G loss: -114.456] \n",
                        "5737 [D loss: (5.266)(R 5.265, F 5.268)]  [G loss: -114.464] \n",
                        "5738 [D loss: (5.267)(R 5.265, F 5.268)]  [G loss: -114.472] \n",
                        "5739 [D loss: (5.267)(R 5.266, F 5.268)]  [G loss: -114.479] \n",
                        "5740 [D loss: (5.267)(R 5.266, F 5.269)]  [G loss: -114.486] \n",
                        "5741 [D loss: (5.268)(R 5.266, F 5.269)]  [G loss: -114.494] \n",
                        "5742 [D loss: (5.268)(R 5.267, F 5.269)]  [G loss: -114.501] \n",
                        "5743 [D loss: (5.268)(R 5.267, F 5.270)]  [G loss: -114.508] \n",
                        "5744 [D loss: (5.269)(R 5.267, F 5.270)]  [G loss: -114.515] \n",
                        "5745 [D loss: (5.269)(R 5.268, F 5.270)]  [G loss: -114.521] \n",
                        "5746 [D loss: (5.269)(R 5.268, F 5.271)]  [G loss: -114.528] \n",
                        "5747 [D loss: (5.269)(R 5.268, F 5.271)]  [G loss: -114.534] \n",
                        "5748 [D loss: (5.270)(R 5.268, F 5.271)]  [G loss: -114.541] \n",
                        "5749 [D loss: (5.270)(R 5.268, F 5.271)]  [G loss: -114.547] \n",
                        "5750 [D loss: (5.270)(R 5.269, F 5.271)]  [G loss: -114.553] \n",
                        "5751 [D loss: (5.270)(R 5.269, F 5.272)]  [G loss: -114.560] \n",
                        "5752 [D loss: (5.270)(R 5.269, F 5.272)]  [G loss: -114.566] \n",
                        "5753 [D loss: (5.271)(R 5.269, F 5.272)]  [G loss: -114.573] \n",
                        "5754 [D loss: (5.271)(R 5.269, F 5.272)]  [G loss: -114.579] \n",
                        "5755 [D loss: (5.271)(R 5.269, F 5.272)]  [G loss: -114.586] \n",
                        "5756 [D loss: (5.271)(R 5.270, F 5.272)]  [G loss: -114.592] \n",
                        "5757 [D loss: (5.271)(R 5.270, F 5.273)]  [G loss: -114.599] \n",
                        "5758 [D loss: (5.271)(R 5.270, F 5.273)]  [G loss: -114.606] \n",
                        "5759 [D loss: (5.272)(R 5.270, F 5.273)]  [G loss: -114.613] \n",
                        "5760 [D loss: (5.272)(R 5.270, F 5.273)]  [G loss: -114.620] \n",
                        "5761 [D loss: (5.272)(R 5.270, F 5.273)]  [G loss: -114.627] \n",
                        "5762 [D loss: (5.272)(R 5.271, F 5.274)]  [G loss: -114.635] \n",
                        "5763 [D loss: (5.273)(R 5.271, F 5.274)]  [G loss: -114.642] \n",
                        "5764 [D loss: (5.273)(R 5.272, F 5.274)]  [G loss: -114.650] \n",
                        "5765 [D loss: (5.273)(R 5.272, F 5.275)]  [G loss: -114.658] \n",
                        "5766 [D loss: (5.274)(R 5.272, F 5.275)]  [G loss: -114.667] \n",
                        "5767 [D loss: (5.274)(R 5.273, F 5.276)]  [G loss: -114.676] \n",
                        "5768 [D loss: (5.275)(R 5.273, F 5.276)]  [G loss: -114.684] \n",
                        "5769 [D loss: (5.275)(R 5.274, F 5.277)]  [G loss: -114.693] \n",
                        "5770 [D loss: (5.276)(R 5.274, F 5.277)]  [G loss: -114.701] \n",
                        "5771 [D loss: (5.276)(R 5.275, F 5.278)]  [G loss: -114.709] \n",
                        "5772 [D loss: (5.277)(R 5.275, F 5.278)]  [G loss: -114.717] \n",
                        "5773 [D loss: (5.277)(R 5.275, F 5.278)]  [G loss: -114.725] \n",
                        "5774 [D loss: (5.277)(R 5.276, F 5.279)]  [G loss: -114.732] \n",
                        "5775 [D loss: (5.278)(R 5.276, F 5.279)]  [G loss: -114.739] \n",
                        "5776 [D loss: (5.278)(R 5.277, F 5.279)]  [G loss: -114.747] \n",
                        "5777 [D loss: (5.278)(R 5.277, F 5.280)]  [G loss: -114.754] \n",
                        "5778 [D loss: (5.278)(R 5.277, F 5.280)]  [G loss: -114.762] \n",
                        "5779 [D loss: (5.279)(R 5.278, F 5.280)]  [G loss: -114.769] \n",
                        "5780 [D loss: (5.279)(R 5.278, F 5.281)]  [G loss: -114.777] \n",
                        "5781 [D loss: (5.280)(R 5.278, F 5.281)]  [G loss: -114.784] \n",
                        "5782 [D loss: (5.280)(R 5.279, F 5.281)]  [G loss: -114.791] \n",
                        "5783 [D loss: (5.280)(R 5.279, F 5.282)]  [G loss: -114.797] \n",
                        "5784 [D loss: (5.281)(R 5.279, F 5.282)]  [G loss: -114.804] \n",
                        "5785 [D loss: (5.281)(R 5.279, F 5.282)]  [G loss: -114.810] \n",
                        "5786 [D loss: (5.281)(R 5.280, F 5.282)]  [G loss: -114.816] \n",
                        "5787 [D loss: (5.281)(R 5.280, F 5.283)]  [G loss: -114.823] \n",
                        "5788 [D loss: (5.281)(R 5.280, F 5.283)]  [G loss: -114.830] \n",
                        "5789 [D loss: (5.282)(R 5.280, F 5.283)]  [G loss: -114.836] \n",
                        "5790 [D loss: (5.282)(R 5.280, F 5.283)]  [G loss: -114.843] \n",
                        "5791 [D loss: (5.282)(R 5.281, F 5.283)]  [G loss: -114.850] \n",
                        "5792 [D loss: (5.282)(R 5.281, F 5.284)]  [G loss: -114.858] \n",
                        "5793 [D loss: (5.282)(R 5.281, F 5.284)]  [G loss: -114.865] \n",
                        "5794 [D loss: (5.283)(R 5.282, F 5.284)]  [G loss: -114.872] \n",
                        "5795 [D loss: (5.283)(R 5.282, F 5.285)]  [G loss: -114.879] \n",
                        "5796 [D loss: (5.283)(R 5.282, F 5.285)]  [G loss: -114.887] \n",
                        "5797 [D loss: (5.284)(R 5.282, F 5.285)]  [G loss: -114.894] \n",
                        "5798 [D loss: (5.284)(R 5.283, F 5.286)]  [G loss: -114.901] \n",
                        "5799 [D loss: (5.284)(R 5.283, F 5.286)]  [G loss: -114.909] \n",
                        "5800 [D loss: (5.285)(R 5.283, F 5.286)]  [G loss: -114.916] \n",
                        "5801 [D loss: (5.285)(R 5.284, F 5.286)]  [G loss: -114.924] \n",
                        "5802 [D loss: (5.285)(R 5.284, F 5.287)]  [G loss: -114.931] \n",
                        "5803 [D loss: (5.286)(R 5.284, F 5.287)]  [G loss: -114.939] \n",
                        "5804 [D loss: (5.286)(R 5.284, F 5.287)]  [G loss: -114.947] \n",
                        "5805 [D loss: (5.286)(R 5.285, F 5.288)]  [G loss: -114.955] \n",
                        "5806 [D loss: (5.287)(R 5.285, F 5.288)]  [G loss: -114.963] \n",
                        "5807 [D loss: (5.287)(R 5.286, F 5.289)]  [G loss: -114.971] \n",
                        "5808 [D loss: (5.287)(R 5.286, F 5.289)]  [G loss: -114.979] \n",
                        "5809 [D loss: (5.288)(R 5.286, F 5.289)]  [G loss: -114.988] \n",
                        "5810 [D loss: (5.288)(R 5.287, F 5.290)]  [G loss: -114.995] \n",
                        "5811 [D loss: (5.289)(R 5.287, F 5.290)]  [G loss: -115.003] \n",
                        "5812 [D loss: (5.289)(R 5.288, F 5.291)]  [G loss: -115.011] \n",
                        "5813 [D loss: (5.290)(R 5.288, F 5.291)]  [G loss: -115.018] \n",
                        "5814 [D loss: (5.290)(R 5.288, F 5.291)]  [G loss: -115.026] \n",
                        "5815 [D loss: (5.290)(R 5.289, F 5.292)]  [G loss: -115.033] \n",
                        "5816 [D loss: (5.291)(R 5.289, F 5.292)]  [G loss: -115.040] \n",
                        "5817 [D loss: (5.291)(R 5.289, F 5.292)]  [G loss: -115.047] \n",
                        "5818 [D loss: (5.291)(R 5.290, F 5.293)]  [G loss: -115.055] \n",
                        "5819 [D loss: (5.292)(R 5.290, F 5.293)]  [G loss: -115.062] \n",
                        "5820 [D loss: (5.292)(R 5.290, F 5.293)]  [G loss: -115.069] \n",
                        "5821 [D loss: (5.292)(R 5.291, F 5.294)]  [G loss: -115.077] \n",
                        "5822 [D loss: (5.293)(R 5.291, F 5.294)]  [G loss: -115.083] \n",
                        "5823 [D loss: (5.293)(R 5.291, F 5.294)]  [G loss: -115.090] \n",
                        "5824 [D loss: (5.293)(R 5.292, F 5.295)]  [G loss: -115.096] \n",
                        "5825 [D loss: (5.293)(R 5.292, F 5.295)]  [G loss: -115.103] \n",
                        "5826 [D loss: (5.294)(R 5.292, F 5.295)]  [G loss: -115.110] \n",
                        "5827 [D loss: (5.294)(R 5.293, F 5.295)]  [G loss: -115.117] \n",
                        "5828 [D loss: (5.294)(R 5.293, F 5.295)]  [G loss: -115.124] \n",
                        "5829 [D loss: (5.294)(R 5.293, F 5.296)]  [G loss: -115.132] \n",
                        "5830 [D loss: (5.295)(R 5.293, F 5.296)]  [G loss: -115.139] \n",
                        "5831 [D loss: (5.295)(R 5.294, F 5.297)]  [G loss: -115.146] \n",
                        "5832 [D loss: (5.295)(R 5.294, F 5.297)]  [G loss: -115.154] \n",
                        "5833 [D loss: (5.296)(R 5.294, F 5.297)]  [G loss: -115.162] \n",
                        "5834 [D loss: (5.296)(R 5.295, F 5.298)]  [G loss: -115.169] \n",
                        "5835 [D loss: (5.297)(R 5.295, F 5.298)]  [G loss: -115.176] \n",
                        "5836 [D loss: (5.297)(R 5.295, F 5.298)]  [G loss: -115.184] \n",
                        "5837 [D loss: (5.297)(R 5.296, F 5.299)]  [G loss: -115.191] \n",
                        "5838 [D loss: (5.298)(R 5.296, F 5.299)]  [G loss: -115.198] \n",
                        "5839 [D loss: (5.298)(R 5.296, F 5.299)]  [G loss: -115.206] \n",
                        "5840 [D loss: (5.298)(R 5.297, F 5.300)]  [G loss: -115.213] \n",
                        "5841 [D loss: (5.298)(R 5.297, F 5.300)]  [G loss: -115.220] \n",
                        "5842 [D loss: (5.299)(R 5.297, F 5.300)]  [G loss: -115.228] \n",
                        "5843 [D loss: (5.299)(R 5.298, F 5.301)]  [G loss: -115.235] \n",
                        "5844 [D loss: (5.299)(R 5.298, F 5.301)]  [G loss: -115.243] \n",
                        "5845 [D loss: (5.300)(R 5.298, F 5.301)]  [G loss: -115.250] \n",
                        "5846 [D loss: (5.300)(R 5.299, F 5.302)]  [G loss: -115.257] \n",
                        "5847 [D loss: (5.300)(R 5.299, F 5.302)]  [G loss: -115.264] \n",
                        "5848 [D loss: (5.301)(R 5.299, F 5.302)]  [G loss: -115.272] \n",
                        "5849 [D loss: (5.301)(R 5.300, F 5.303)]  [G loss: -115.279] \n",
                        "5850 [D loss: (5.301)(R 5.300, F 5.303)]  [G loss: -115.286] \n",
                        "5851 [D loss: (5.302)(R 5.300, F 5.303)]  [G loss: -115.293] \n",
                        "5852 [D loss: (5.302)(R 5.301, F 5.303)]  [G loss: -115.299] \n",
                        "5853 [D loss: (5.302)(R 5.301, F 5.304)]  [G loss: -115.306] \n",
                        "5854 [D loss: (5.302)(R 5.301, F 5.304)]  [G loss: -115.313] \n",
                        "5855 [D loss: (5.303)(R 5.301, F 5.304)]  [G loss: -115.320] \n",
                        "5856 [D loss: (5.303)(R 5.302, F 5.304)]  [G loss: -115.327] \n",
                        "5857 [D loss: (5.303)(R 5.302, F 5.305)]  [G loss: -115.335] \n",
                        "5858 [D loss: (5.303)(R 5.302, F 5.305)]  [G loss: -115.343] \n",
                        "5859 [D loss: (5.304)(R 5.303, F 5.305)]  [G loss: -115.349] \n",
                        "5860 [D loss: (5.304)(R 5.303, F 5.306)]  [G loss: -115.357] \n",
                        "5861 [D loss: (5.304)(R 5.303, F 5.306)]  [G loss: -115.364] \n",
                        "5862 [D loss: (5.305)(R 5.303, F 5.306)]  [G loss: -115.372] \n",
                        "5863 [D loss: (5.305)(R 5.304, F 5.307)]  [G loss: -115.379] \n",
                        "5864 [D loss: (5.306)(R 5.304, F 5.307)]  [G loss: -115.387] \n",
                        "5865 [D loss: (5.306)(R 5.305, F 5.307)]  [G loss: -115.394] \n",
                        "5866 [D loss: (5.306)(R 5.305, F 5.308)]  [G loss: -115.401] \n",
                        "5867 [D loss: (5.306)(R 5.305, F 5.308)]  [G loss: -115.409] \n",
                        "5868 [D loss: (5.307)(R 5.306, F 5.308)]  [G loss: -115.416] \n",
                        "5869 [D loss: (5.307)(R 5.306, F 5.309)]  [G loss: -115.424] \n",
                        "5870 [D loss: (5.308)(R 5.306, F 5.309)]  [G loss: -115.431] \n",
                        "5871 [D loss: (5.308)(R 5.306, F 5.309)]  [G loss: -115.438] \n",
                        "5872 [D loss: (5.308)(R 5.307, F 5.310)]  [G loss: -115.446] \n",
                        "5873 [D loss: (5.309)(R 5.307, F 5.310)]  [G loss: -115.453] \n",
                        "5874 [D loss: (5.309)(R 5.307, F 5.310)]  [G loss: -115.460] \n",
                        "5875 [D loss: (5.309)(R 5.308, F 5.311)]  [G loss: -115.467] \n",
                        "5876 [D loss: (5.310)(R 5.308, F 5.311)]  [G loss: -115.474] \n",
                        "5877 [D loss: (5.310)(R 5.308, F 5.311)]  [G loss: -115.481] \n",
                        "5878 [D loss: (5.310)(R 5.309, F 5.312)]  [G loss: -115.488] \n",
                        "5879 [D loss: (5.310)(R 5.309, F 5.312)]  [G loss: -115.495] \n",
                        "5880 [D loss: (5.311)(R 5.309, F 5.312)]  [G loss: -115.502] \n",
                        "5881 [D loss: (5.311)(R 5.310, F 5.312)]  [G loss: -115.510] \n",
                        "5882 [D loss: (5.311)(R 5.310, F 5.313)]  [G loss: -115.518] \n",
                        "5883 [D loss: (5.312)(R 5.310, F 5.313)]  [G loss: -115.525] \n",
                        "5884 [D loss: (5.312)(R 5.311, F 5.313)]  [G loss: -115.532] \n",
                        "5885 [D loss: (5.312)(R 5.311, F 5.314)]  [G loss: -115.540] \n",
                        "5886 [D loss: (5.313)(R 5.311, F 5.314)]  [G loss: -115.547] \n",
                        "5887 [D loss: (5.313)(R 5.312, F 5.315)]  [G loss: -115.554] \n",
                        "5888 [D loss: (5.313)(R 5.312, F 5.315)]  [G loss: -115.561] \n",
                        "5889 [D loss: (5.314)(R 5.312, F 5.315)]  [G loss: -115.569] \n",
                        "5890 [D loss: (5.314)(R 5.313, F 5.315)]  [G loss: -115.576] \n",
                        "5891 [D loss: (5.314)(R 5.313, F 5.316)]  [G loss: -115.583] \n",
                        "5892 [D loss: (5.315)(R 5.313, F 5.316)]  [G loss: -115.590] \n",
                        "5893 [D loss: (5.315)(R 5.314, F 5.316)]  [G loss: -115.597] \n",
                        "5894 [D loss: (5.315)(R 5.314, F 5.317)]  [G loss: -115.605] \n",
                        "5895 [D loss: (5.316)(R 5.314, F 5.317)]  [G loss: -115.612] \n",
                        "5896 [D loss: (5.316)(R 5.315, F 5.317)]  [G loss: -115.619] \n",
                        "5897 [D loss: (5.316)(R 5.315, F 5.318)]  [G loss: -115.626] \n",
                        "5898 [D loss: (5.317)(R 5.315, F 5.318)]  [G loss: -115.633] \n",
                        "5899 [D loss: (5.317)(R 5.315, F 5.318)]  [G loss: -115.640] \n",
                        "5900 [D loss: (5.317)(R 5.316, F 5.319)]  [G loss: -115.647] \n",
                        "5901 [D loss: (5.317)(R 5.316, F 5.319)]  [G loss: -115.654] \n",
                        "5902 [D loss: (5.318)(R 5.316, F 5.319)]  [G loss: -115.661] \n",
                        "5903 [D loss: (5.318)(R 5.316, F 5.319)]  [G loss: -115.668] \n",
                        "5904 [D loss: (5.318)(R 5.317, F 5.320)]  [G loss: -115.675] \n",
                        "5905 [D loss: (5.318)(R 5.317, F 5.320)]  [G loss: -115.682] \n",
                        "5906 [D loss: (5.319)(R 5.317, F 5.320)]  [G loss: -115.689] \n",
                        "5907 [D loss: (5.319)(R 5.318, F 5.320)]  [G loss: -115.696] \n",
                        "5908 [D loss: (5.319)(R 5.318, F 5.321)]  [G loss: -115.704] \n",
                        "5909 [D loss: (5.320)(R 5.318, F 5.321)]  [G loss: -115.712] \n",
                        "5910 [D loss: (5.320)(R 5.319, F 5.322)]  [G loss: -115.719] \n",
                        "5911 [D loss: (5.321)(R 5.319, F 5.322)]  [G loss: -115.725] \n",
                        "5912 [D loss: (5.321)(R 5.319, F 5.322)]  [G loss: -115.733] \n",
                        "5913 [D loss: (5.321)(R 5.320, F 5.323)]  [G loss: -115.740] \n",
                        "5914 [D loss: (5.322)(R 5.320, F 5.323)]  [G loss: -115.747] \n",
                        "5915 [D loss: (5.322)(R 5.320, F 5.323)]  [G loss: -115.754] \n",
                        "5916 [D loss: (5.322)(R 5.321, F 5.324)]  [G loss: -115.761] \n",
                        "5917 [D loss: (5.322)(R 5.321, F 5.324)]  [G loss: -115.768] \n",
                        "5918 [D loss: (5.323)(R 5.321, F 5.324)]  [G loss: -115.775] \n",
                        "5919 [D loss: (5.323)(R 5.322, F 5.324)]  [G loss: -115.783] \n",
                        "5920 [D loss: (5.323)(R 5.322, F 5.325)]  [G loss: -115.790] \n",
                        "5921 [D loss: (5.324)(R 5.322, F 5.325)]  [G loss: -115.798] \n",
                        "5922 [D loss: (5.324)(R 5.323, F 5.325)]  [G loss: -115.805] \n",
                        "5923 [D loss: (5.324)(R 5.323, F 5.326)]  [G loss: -115.812] \n",
                        "5924 [D loss: (5.325)(R 5.323, F 5.326)]  [G loss: -115.820] \n",
                        "5925 [D loss: (5.325)(R 5.324, F 5.327)]  [G loss: -115.827] \n",
                        "5926 [D loss: (5.326)(R 5.324, F 5.327)]  [G loss: -115.833] \n",
                        "5927 [D loss: (5.326)(R 5.324, F 5.327)]  [G loss: -115.840] \n",
                        "5928 [D loss: (5.326)(R 5.325, F 5.328)]  [G loss: -115.848] \n",
                        "5929 [D loss: (5.327)(R 5.325, F 5.328)]  [G loss: -115.854] \n",
                        "5930 [D loss: (5.327)(R 5.325, F 5.328)]  [G loss: -115.861] \n",
                        "5931 [D loss: (5.327)(R 5.326, F 5.328)]  [G loss: -115.868] \n",
                        "5932 [D loss: (5.327)(R 5.326, F 5.329)]  [G loss: -115.875] \n",
                        "5933 [D loss: (5.328)(R 5.326, F 5.329)]  [G loss: -115.882] \n",
                        "5934 [D loss: (5.328)(R 5.327, F 5.329)]  [G loss: -115.888] \n",
                        "5935 [D loss: (5.328)(R 5.327, F 5.330)]  [G loss: -115.895] \n",
                        "5936 [D loss: (5.328)(R 5.327, F 5.330)]  [G loss: -115.902] \n",
                        "5937 [D loss: (5.329)(R 5.327, F 5.330)]  [G loss: -115.909] \n",
                        "5938 [D loss: (5.329)(R 5.328, F 5.330)]  [G loss: -115.916] \n",
                        "5939 [D loss: (5.329)(R 5.328, F 5.330)]  [G loss: -115.923] \n",
                        "5940 [D loss: (5.330)(R 5.328, F 5.331)]  [G loss: -115.930] \n",
                        "5941 [D loss: (5.330)(R 5.328, F 5.331)]  [G loss: -115.937] \n",
                        "5942 [D loss: (5.330)(R 5.329, F 5.331)]  [G loss: -115.945] \n",
                        "5943 [D loss: (5.330)(R 5.329, F 5.332)]  [G loss: -115.951] \n",
                        "5944 [D loss: (5.331)(R 5.329, F 5.332)]  [G loss: -115.958] \n",
                        "5945 [D loss: (5.331)(R 5.330, F 5.332)]  [G loss: -115.966] \n",
                        "5946 [D loss: (5.331)(R 5.330, F 5.333)]  [G loss: -115.974] \n",
                        "5947 [D loss: (5.332)(R 5.330, F 5.333)]  [G loss: -115.981] \n",
                        "5948 [D loss: (5.332)(R 5.331, F 5.334)]  [G loss: -115.988] \n",
                        "5949 [D loss: (5.332)(R 5.331, F 5.334)]  [G loss: -115.995] \n",
                        "5950 [D loss: (5.333)(R 5.331, F 5.334)]  [G loss: -116.003] \n",
                        "5951 [D loss: (5.333)(R 5.332, F 5.335)]  [G loss: -116.011] \n",
                        "5952 [D loss: (5.334)(R 5.332, F 5.335)]  [G loss: -116.017] \n",
                        "5953 [D loss: (5.334)(R 5.333, F 5.335)]  [G loss: -116.024] \n",
                        "5954 [D loss: (5.334)(R 5.333, F 5.336)]  [G loss: -116.032] \n",
                        "5955 [D loss: (5.335)(R 5.333, F 5.336)]  [G loss: -116.039] \n",
                        "5956 [D loss: (5.335)(R 5.334, F 5.336)]  [G loss: -116.045] \n",
                        "5957 [D loss: (5.335)(R 5.334, F 5.337)]  [G loss: -116.052] \n",
                        "5958 [D loss: (5.335)(R 5.334, F 5.337)]  [G loss: -116.059] \n",
                        "5959 [D loss: (5.336)(R 5.334, F 5.337)]  [G loss: -116.066] \n",
                        "5960 [D loss: (5.336)(R 5.335, F 5.338)]  [G loss: -116.073] \n",
                        "5961 [D loss: (5.336)(R 5.335, F 5.338)]  [G loss: -116.080] \n",
                        "5962 [D loss: (5.337)(R 5.335, F 5.338)]  [G loss: -116.087] \n",
                        "5963 [D loss: (5.337)(R 5.336, F 5.338)]  [G loss: -116.093] \n",
                        "5964 [D loss: (5.337)(R 5.336, F 5.339)]  [G loss: -116.101] \n",
                        "5965 [D loss: (5.337)(R 5.336, F 5.339)]  [G loss: -116.107] \n",
                        "5966 [D loss: (5.338)(R 5.336, F 5.339)]  [G loss: -116.115] \n",
                        "5967 [D loss: (5.338)(R 5.337, F 5.339)]  [G loss: -116.122] \n",
                        "5968 [D loss: (5.338)(R 5.337, F 5.340)]  [G loss: -116.130] \n",
                        "5969 [D loss: (5.339)(R 5.337, F 5.340)]  [G loss: -116.137] \n",
                        "5970 [D loss: (5.339)(R 5.338, F 5.340)]  [G loss: -116.144] \n",
                        "5971 [D loss: (5.339)(R 5.338, F 5.341)]  [G loss: -116.152] \n",
                        "5972 [D loss: (5.340)(R 5.338, F 5.341)]  [G loss: -116.159] \n",
                        "5973 [D loss: (5.340)(R 5.339, F 5.342)]  [G loss: -116.166] \n",
                        "5974 [D loss: (5.340)(R 5.339, F 5.342)]  [G loss: -116.173] \n",
                        "5975 [D loss: (5.341)(R 5.339, F 5.342)]  [G loss: -116.181] \n",
                        "5976 [D loss: (5.341)(R 5.340, F 5.343)]  [G loss: -116.188] \n",
                        "5977 [D loss: (5.342)(R 5.340, F 5.343)]  [G loss: -116.195] \n",
                        "5978 [D loss: (5.342)(R 5.340, F 5.343)]  [G loss: -116.203] \n",
                        "5979 [D loss: (5.342)(R 5.341, F 5.344)]  [G loss: -116.210] \n",
                        "5980 [D loss: (5.343)(R 5.341, F 5.344)]  [G loss: -116.217] \n",
                        "5981 [D loss: (5.343)(R 5.341, F 5.344)]  [G loss: -116.224] \n",
                        "5982 [D loss: (5.343)(R 5.342, F 5.344)]  [G loss: -116.231] \n",
                        "5983 [D loss: (5.343)(R 5.342, F 5.345)]  [G loss: -116.238] \n",
                        "5984 [D loss: (5.344)(R 5.342, F 5.345)]  [G loss: -116.245] \n",
                        "5985 [D loss: (5.344)(R 5.343, F 5.345)]  [G loss: -116.252] \n",
                        "5986 [D loss: (5.344)(R 5.343, F 5.346)]  [G loss: -116.259] \n",
                        "5987 [D loss: (5.345)(R 5.343, F 5.346)]  [G loss: -116.266] \n",
                        "5988 [D loss: (5.345)(R 5.343, F 5.346)]  [G loss: -116.273] \n",
                        "5989 [D loss: (5.345)(R 5.344, F 5.347)]  [G loss: -116.280] \n",
                        "5990 [D loss: (5.345)(R 5.344, F 5.347)]  [G loss: -116.287] \n",
                        "5991 [D loss: (5.346)(R 5.344, F 5.347)]  [G loss: -116.295] \n",
                        "5992 [D loss: (5.346)(R 5.345, F 5.348)]  [G loss: -116.302] \n",
                        "5993 [D loss: (5.347)(R 5.345, F 5.348)]  [G loss: -116.309] \n",
                        "5994 [D loss: (5.347)(R 5.345, F 5.348)]  [G loss: -116.317] \n",
                        "5995 [D loss: (5.347)(R 5.346, F 5.349)]  [G loss: -116.324] \n",
                        "5996 [D loss: (5.348)(R 5.346, F 5.349)]  [G loss: -116.331] \n",
                        "5997 [D loss: (5.348)(R 5.346, F 5.349)]  [G loss: -116.338] \n",
                        "5998 [D loss: (5.348)(R 5.347, F 5.349)]  [G loss: -116.345] \n",
                        "5999 [D loss: (5.349)(R 5.347, F 5.350)]  [G loss: -116.352] \n"
                    ]
                }
            ],
            "source": [
                "# =============================================================================\n",
                "# TRAINING EXECUTION\n",
                "# =============================================================================\n",
                "# Train the WGAN using the Wasserstein training procedure:\n",
                "#   1. Sample real images from training set\n",
                "#   2. Generate fake images from random noise\n",
                "#   3. Train critic to maximize: E[critic(real)] - E[critic(fake)]\n",
                "#   4. Clip critic weights to [-CLIP_THRESHOLD, CLIP_THRESHOLD]\n",
                "#   5. Train generator to maximize: E[critic(fake)]\n",
                "#   6. Repeat critic steps N_CRITIC times per generator step\n",
                "\n",
                "gan.train(\n",
                "    x_train,                              # Training data\n",
                "    batch_size=BATCH_SIZE,                # Samples per batch\n",
                "    epochs=EPOCHS,                        # Total epochs\n",
                "    run_folder=RUN_FOLDER,                # Output directory\n",
                "    print_every_n_batches=PRINT_EVERY_N_BATCHES,  # Checkpoint frequency\n",
                "    n_critic=N_CRITIC,                    # Critic updates per generator update\n",
                "    clip_threshold=CLIP_THRESHOLD         # Weight clipping value\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u2713 Sample images saved to: ../run/gan/0002_horses/images/\n"
                    ]
                }
            ],
            "source": [
                "# =============================================================================\n",
                "# GENERATE FINAL SAMPLES\n",
                "# =============================================================================\n",
                "# Generate and save a grid of sample images using the trained generator.\n",
                "# This provides a quick visual check of generation quality after training.\n",
                "\n",
                "gan.sample_images(RUN_FOLDER)\n",
                "print(f\"\u2713 Sample images saved to: {RUN_FOLDER}/images/\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Log Training Metrics to W&B\n",
                "\n",
                "Log final training metrics and loss curves to Weights & Biases for experiment comparison."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u2713 Training metrics logged to W&B\n"
                    ]
                }
            ],
            "source": [
                "# =============================================================================\n",
                "# LOG TRAINING METRICS TO W&B\n",
                "# =============================================================================\n",
                "# Log final metrics and loss history for experiment comparison.\n",
                "\n",
                "# Log final loss values\n",
                "final_d_loss = gan.d_losses[-1] if gan.d_losses else [0, 0, 0]\n",
                "final_g_loss = gan.g_losses[-1] if gan.g_losses else 0\n",
                "\n",
                "wandb.log({\n",
                "    \"final_d_loss\": final_d_loss[0],\n",
                "    \"final_d_loss_real\": final_d_loss[1],\n",
                "    \"final_d_loss_fake\": final_d_loss[2],\n",
                "    \"final_g_loss\": final_g_loss,\n",
                "    \"total_epochs\": len(gan.d_losses),\n",
                "})\n",
                "\n",
                "# Log loss history as table for visualization\n",
                "loss_table = wandb.Table(\n",
                "    columns=[\"epoch\", \"d_loss\", \"d_loss_real\", \"d_loss_fake\", \"g_loss\"]\n",
                ")\n",
                "for i, (d_loss, g_loss) in enumerate(zip(gan.d_losses, gan.g_losses)):\n",
                "    loss_table.add_data(i, d_loss[0], d_loss[1], d_loss[2], g_loss)\n",
                "\n",
                "wandb.log({\"loss_history\": loss_table})\n",
                "\n",
                "print(\"\u2713 Training metrics logged to W&B\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Post-Training Analysis\n",
                "\n",
                "Run automated stability analysis and generate reports.\n",
                "This cell:\n",
                "1. Analyzes training dynamics using rules-based stability checks\n",
                "2. Logs phase metrics and stability indicators to W&B\n",
                "3. Generates a markdown analysis report in the run folder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# POST-TRAINING ANALYSIS\n",
                "# =============================================================================\n",
                "# Run automated stability analysis and log to W&B + markdown report.\n",
                "\n",
                "from utils.gan.stability_analysis import analyze_training_run\n",
                "from utils.gan.report_generator import generate_run_report\n",
                "from utils.wandb_utils import log_training_report\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# Run Automated Stability Analysis\n",
                "# -----------------------------------------------------------------------------\n",
                "# Analyze training dynamics from loss curves\n",
                "analysis = analyze_training_run(gan.d_losses, gan.g_losses)\n",
                "\n",
                "# Print verdict summary\n",
                "print(\"\u2550\" * 60)\n",
                "print(\"TRAINING ANALYSIS VERDICT\")\n",
                "print(\"\u2550\" * 60)\n",
                "print(f\"Stability:    {analysis['verdict']['stability']}\")\n",
                "print(f\"Quality:      {analysis['verdict']['quality']}\")\n",
                "print(f\"Score:        {analysis['verdict']['passed']}/{analysis['verdict']['total']} checks passed\")\n",
                "print(f\"Recommendation: {analysis['verdict']['recommendation']}\")\n",
                "print(\"\u2550\" * 60)\n",
                "\n",
                "# Print stability indicators\n",
                "print(\"\\nStability Indicators:\")\n",
                "for name, (passed, obs) in analysis['indicators'].items():\n",
                "    status = \"\u2705\" if passed else \"\u274c\"\n",
                "    print(f\"  {status} {name.replace('_', ' ').title()}: {obs}\")\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# Log Complete Analysis to W&B\n",
                "# -----------------------------------------------------------------------------\n",
                "# Build config dict from global constants for logging\n",
                "config = {\n",
                "    \"batch_size\": BATCH_SIZE,\n",
                "    \"epochs\": EPOCHS,\n",
                "    \"lr_critic\": CRITIC_LEARNING_RATE,\n",
                "    \"lr_generator\": GENERATOR_LEARNING_RATE,\n",
                "    \"optimizer\": OPTIMIZER,\n",
                "    \"z_dim\": Z_DIM,\n",
                "    \"n_critic\": N_CRITIC,\n",
                "    \"clip_threshold\": CLIP_THRESHOLD,\n",
                "    \"input_dim\": INPUT_DIM,\n",
                "    \"critic_filters\": CRITIC_FILTERS,\n",
                "    \"generator_filters\": GENERATOR_FILTERS,\n",
                "}\n",
                "\n",
                "# Log to W&B\n",
                "log_training_report(\n",
                "    config=config,\n",
                "    analysis=analysis,\n",
                "    run_folder=RUN_FOLDER,\n",
                "    notes=\"Baseline run with default hyperparameters\"\n",
                ")\n",
                "\n",
                "# -----------------------------------------------------------------------------\n",
                "# Generate Markdown Report\n",
                "# -----------------------------------------------------------------------------\n",
                "# Save analysis report to run folder\n",
                "report_path = generate_run_report(\n",
                "    run_folder=RUN_FOLDER,\n",
                "    config=config,\n",
                "    d_losses=gan.d_losses,\n",
                "    g_losses=gan.g_losses,\n",
                "    wandb_url=wandb.run.url if wandb.run else None,\n",
                "    notes=\"Baseline run with default hyperparameters\"\n",
                ")\n",
                "\n",
                "print(f\"\\n\u2713 Analysis complete! Report saved to: {report_path}\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Wasserstein Loss Visualization\n",
                "\n",
                "This cell visualizes the training dynamics of the WGAN by plotting the Wasserstein distance over training epochs.\n",
                "\n",
                "**What the plots show:**\n",
                "- **Black line**: Combined critic loss (D loss) - the average of real and fake losses\n",
                "- **Green line**: Critic loss on real images (R) - how well the critic scores real samples\n",
                "- **Red line**: Critic loss on fake images (F) - how well the critic scores generated samples\n",
                "- **Orange line**: Generator loss (G loss) - the Wasserstein distance estimate\n",
                "\n",
                "**Why this matters:**\n",
                "Unlike standard GANs, the WGAN loss is meaningful - the negative generator loss approximates the Earth Mover's Distance between real and generated distributions. A steadily decreasing (more negative) G loss indicates the generator is improving."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/4AAAIqCAYAAACdRJFAAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAlqxJREFUeJzs3Xd8FHX+x/HXbnohhfSQAAkdwQACShGkCBawUGwooIIN7CKinuJ56lkOvTsV9XdKs4B3J+qJIB0BucNGkd57Ci0hpG72+/tjzUpMCOmThPfz8ZgH2ZnvzHy2ZMl7Zr7fsRljDCIiIiIiIiJSL9mtLkBEREREREREqo+Cv4iIiIiIiEg9puAvIiIiIiIiUo8p+IuIiIiIiIjUYwr+IiIiIiIiIvWYgr+IiIiIiIhIPabgLyIiIiIiIlKPKfiLiIiIiIiI1GMK/iIiIiIiIiL1mIK/iNQbu3btwmazYbfbSUtLK7HNhx9+iM1mw2az8eGHH5bYJi0tDbvdjs1mY9euXSW2Wb58OWPHjqVt27aEhobi5eVFWFgYXbt2Zfz48SxevBhjzDlrfuCBB9z1/Oc//ym17fTp091tL7zwQpxOZ4ntVq1ahc1mo2nTpufcP0DTpk3d2y3rVNZtl1fhcxw9enSVbbOw5rqoJj/TUv0mT56MzWZj8uTJNbK/ws/F8uXLa2R/55O9e/eW+fty7969VpdbZsuXL8dms3HZZZdZXYqIVDFPqwsQEakqzZo1Iz4+ngMHDrBixQqGDRtWrM2yZcvcPy9fvpxbb721WJvly5djjCE+Pp5mzZoVWXb06FFGjBjBwoULAWjUqBE9evQgODiY9PR0fvnlF9566y3eeustOnbsyE8//XTWenNzc/noo4/cjz/44AMGDx5cpue6ceNGPvzwQ0aOHFmm9qUZNmwYR48eLTIvMzOTf//73wAMHTqUwMDAIsvDw8MrvV85t5r4TEtxTZs2Zd++fezZs6faDnJJ/VHSd+SZSlsmIlJTFPxFpF7p06cPM2fOZNmyZSWGpOXLlxMREYGPj89Zz4IVzu/Tp0+R+SdPnqRnz55s27aN1q1b8/bbbxdrA/DLL7/w+uuvM3v27FJrnTt3LsePHyc2NpYjR47w1VdfkZKSQlRUVKnr+fv7k5WVxTPPPMONN96Ij49Pqe3P5bXXXis2b+/eve7g/9prr9VY+Ln++uu55JJLCA4OrrJtbtmypcq2ZYXq/ExLzRo/fjw33XSTDpzVMzX5HSkiUlG61F9E6pXCYHPmWdBCBw4cYPfu3fTu3ZvevXuza9cuDhw4UKxd4bq/D0n3338/27ZtIzExke++++6sIapdu3a8//77JdZwpvfffx+ABx98kN69e+NwOJg5c+Y5n2O3bt3o2rUr+/bt4+233z5n+7okODiY1q1bExMTU2XbbN26Na1bt66y7dW06vxMS80KDw+ndevWCv4iIlLjFPxFpF4pDDZbtmwhJSWlyLLCs56XXXYZvXv3LjKvUEpKivsM8ZkhadeuXXz88ccAvP7664SGhp6zlq5du5512d69e1myZAmenp6MHDmSO++8E3Bd7l8WL7/8MgAvvPACGRkZZVqnqpzZD//48eM89NBDNGvWDB8fnyL9QhcvXsz9999Phw4dCA8Px8fHh7i4OG688Ua+//77c277TGf2O83Pz+fll1/mggsuwM/Pj7CwMIYMGXLWM/tn6+NfOLbB3r17WbZsGQMGDCA0NBQ/Pz86depU6kGYY8eO8cADD9C4cWN8fHxo0qQJDz30ECdPnmT06NHYbDamT59+zteyLKrjM52fn8+HH37IiBEjaN26NUFBQfj5+dGqVSseeOABDh8+XGIt6enpPP3007Rv356AgAB8fHyIjY2lR48ePPPMM+Tn5xdp/+OPP3LjjTcSFxeHt7c3QUFBJCYmMnToUL744osS9/Hjjz8yYsQI92vbsGFDBg4cyNdff11i+zPfxy+++IK+ffvSsGHDIn3bc3NzefXVV7noooto0KAB3t7eREdH06VLFx5//HGOHz8O/Pb527dvHwAJCQlF+mr//rU9fPgwjzzyCG3atMHf358GDRrQpUsX3nzzTRwOR7Faz9bH/8zP/enTp5k0aRLNmzfHx8eH6OhoRo0axaFDh0p8/lVt69at3H777TRp0sT9+vfr149PP/20xPZOp5P33nuPHj16EBISgpeXF5GRkSQlJXH//fcX699+5MgRHnzwQVq2bImvry/+/v7Ex8fTr1+/Eq8+Ksk333yDzWajTZs2Z23jcDiIjo7GZrOxfv169/wdO3Zwxx13kJCQgI+PD4GBgTRp0oSrr76aadOmlWn/lXHmd1lWVhZPPvkkzZs3x9fXl9jYWO68885S3+vyvj+FfvzxR0aNGkVCQgK+vr40bNiQpKQkJkyY4P68/155v2tFpJYzIiL1TEJCggHM7Nmzi8y/4447DGA2btxotm3bZgBz++23F2kze/ZsA5iEhIQi89944w0DmNDQUFNQUFDpGv/whz8YwFxzzTXGGGOysrJMcHCwAczq1atLXGfatGkGMP369TPGGHPllVcawDz55JNF2q1cudIApkmTJhWub8+ePQYwgNmzZ0+JdVx99dUmISHBhIaGmmuuucYMHz7cjBgxwt2uWbNmxtvb23Ts2NFcc801ZsiQIaZt27YGMJ6enuZf//rXWZ/jqFGjisxftmyZAUz37t1N//79jb+/v7niiivM0KFDTXx8vAFMSEhIsVqNMe7n8XtNmjQxgPnDH/5gbDabueiii8xNN91kLrnkEvc6r7/+erH1Dh8+bJo1a2YA07BhQzNkyBBz3XXXmdDQUNOqVStz3XXXGcBMmzbtrLUsW7aspJf9rKr6M33gwAEDmODgYHPJJZeY4cOHm6uuusrExsYawERERJgdO3YU2c7p06dNu3bt3MsHDx5sbrrpJnPZZZeZ6OhoA5gTJ0642y9evNh4eXkZwCQlJZlhw4aZ66+/3nTt2tX4+PiYa6+9ttjzfOONN4zdbjeA6dChgxk2bJjp2bOn8fb2NoB57rnniq1T+D6OHz/eAKZz587m5ptvNr179zbffvutKSgoMP369TOACQoKMldeeaW5+eabTf/+/d3r/vzzz8YY1+/OqFGjTEBAgAHM0KFDzahRo9zTli1b3PtdsWKFCQ0NNYBp2rSpueaaa8zAgQPd8wYMGGDy8vKK1Prss88awDz77LNF5hd+7q+77jpz4YUXmpCQEDN48GBz7bXXmsjISPfv88mTJ4s9/9KU9/P21VdfGV9fXwOYVq1amZtuusn07dvXeHh4GMDccccdxda5/fbbDWB8fX1N//79zc0332wGDhxoWrRoYQAzd+5cd9sjR464P2ONGzc21157rbnxxhvNpZdeaho2bGiCg4PLVGdBQYGJi4szgFmzZk2Jbb788ksDmE6dOrnnbdy40QQFBbmf35AhQ8zw4cNNt27dTGBgoElKSirT/o0p/TuyNIXfZd26dTOXXHKJ8ff3N1dddZUZPny4iYmJMYCJjo4227dvL7ZuRd4fY4x55ZVX3L9XLVu2NDfccIMZPHiwadOmTbHvqsp814pI7abgLyL1TmEYuvvuu4vMT0xMNBEREcbpdBpjjImOji4W8O+5554S/4C67bbbioTuyigoKHD/AfX555+75999992l/vH2++C/bt06Y7fbjb+/vzl8+LC7XU0F/8Ja0tPTS9zG3LlzzfHjx0uc7+npacLCwkxWVlaJ2z5b8AdMx44dzZEjR9zLsrOzzcCBAw1g7rrrrmL7O1fw9/LyMv/5z39KrCM4OLhYjddff70BzGWXXVbkuZ84ccL07NnTvb+qDP5V/ZnOyMgwX3zxhcnNzS3SNi8vz0yaNMkA5qqrriqybMaMGQYwV155ZbFAW1BQYJYvX15ke3369DGA+fDDD4s9n5MnTxYLbAsWLDA2m82Eh4ebFStWFFm2YcMGd9Bbvnx5kWWF76OHh4f54osviu1rxYoV7s9NRkZGseXff/+9OXr0aInbPFu4OXLkiAkLCzM2m828/fbbRQ4GHj161PTt27fEAxXnCv6AGThwYJHP1fHjx02HDh0MYF588cUS6zmb8nzekpOT3Qcf//SnP7k/U8a4XqPCAxrvvfeee/6+ffsMYOLi4or8ThbavHmz2bdvn/vxc8895/49PXP7xrg+e4sXLy7zc3vqqadK/J0oVPh7+ve//909r/AgxZ/+9Kdi7bOysop97kpT2eAPmObNmxd5fbKzs83QoUMNYC655JIi61Xk/THGmC+++MJ9YGbOnDnF6tm0aZPZvHlzifWV97tWRGo3BX8RqXdmzZrlPrNRqPAP1KFDh7rn3XjjjQYwe/fudc9r1aqVAcysWbOKbLPw7PpNN91U4j7XrVtX5Mxg4bRy5cpibefPn28AExUVZfLz893z165dawATGBhoTp06VWy93wd/Y4y59dZbi/3xW1PB38vLy+zatatC27/55psNYObNm1fits8W/G02m1m3bl2x7f33v/81gElMTCy27FzB/5FHHimxxtatWxvAfPvtt+55e/fuNTabzdjt9iJnfwtt3LjR2Gy2swb/Vq1amVatWpn//e9/Je7zbKrjM12a2NhYY7fbiwTlV155xQBmypQpZdpG4dUdJR38KcnFF19sgBKvBDHGmE8//bTY8zXmt/fxbAfMCtd74IEHylTHmds8W6CbOHGi+yqDkhw8eNB4eXkVOShjzLmDf0BAQJGDeIUKr9ro27dvmZ+DMeUL/s8//7wBzEUXXVTi8tdee80ApkWLFu55hd9ZhVcunct9991nAPPZZ5+VqX1pdu7c6T44l52dXWRZamqq8fLyMj4+PubYsWPu+VdddZUBzE8//VTp/Z/5HVna9PurCM4M1mce+C2UkpJi/P39DRS9+qsi748xxn3Q6C9/+UuZnldlvmtFpHZTH38RqXcK+zFv376dI0eOAL/1ey7sB33mz4XLkpOT2bZtW5FtlNWBAweYMWNGsWnnzp3F2v7jH/8AYOTIkXh6/nZzlS5dutCuXTsyMzOZM2dOmfb7/PPP4+3tzfvvv8/27dvLVXNldezYkcTExFLbHD58mP/7v//j0UcfZcyYMYwePZrRo0ezadMmAPfrXVaNGzcmKSmp2PzCvr4V6Qd9tlsolrTNlStXYoyhU6dOJQ4Y2K5dOy688MKz7mvr1q1s3bq11PEfSlJdn+n169czZcoU7r//fu644w73++NwOHA6nUU+v126dAHglVdeYebMme5+8WdT+BxHjBjBqlWrSuzzXujo0aOsXbsWPz+/s74fheNHfPfddyUuL+mOBwCdOnXCw8ODDz74gLfeesv9+lXGvHnzALjxxhtLXN6oUSNatGhBWloaO3bsKPN2O3fuXOLAlpX5fJdV4Wdm1KhRJS4vHIdkx44d7jEgWrduTYMGDfj666954YUX2LNnT6n7KPxMPPHEE3z22WdkZmZWuN5mzZrRq1cv0tPTmTt3bpFlH330Efn5+Vx77bU0bNiw2P7vvfdevvnmG3Jyciq8/zMNHTqUUaNGlThdc801Ja4TEhJS4rLIyEiuuOIKoOh4HRV5f5KTk1m3bh12u929vKyq47tWRKyl4C8i9U7hH93w22jmZw6CVuj3Ianw3xYtWtCoUaMi2ywchTstLa3EfQ4aNAjjuooKYwz9+vUrsV1aWhpffvklAHfccUex5YXzyjrIX9OmTbnvvvtwOBw8+eSTZVqnqpzr9lXPPfccTZs25a677mLKlCm8//777gMiGzZsACj3wISNGzcucX5QUBDgGsStvM61zTPDwcGDB4HSn3t13Narqj/Tp0+fZsiQIXTo0IFHH32UN998k2nTprnfn9TUVKDo+3PZZZcxceJEUlNTGTVqFOHh4bRq1Yo77riDL774AqfTWaTml156iU6dOjF//nwuvfRSgoKC6NmzJ08//XSxwcH27NmDMYbs7Gx8fHyKDKhXOEVGRgJn/x082+verFkzXn/9dfLz8xk/fjyxsbE0bdqUm2++mY8++oi8vLwS1yvN7t27Abj00ktLrNVms7F58+ZS6y1JeT6LVa0wyCUkJJS4PCQkxB2iC38PGjRowLRp0/Dz8+Ppp58mMTGR2NhYhgwZwnvvvVcs2N92222MGDGC7du3M3ToUEJCQrjwwgu57777WLp0ablrLvy+/P2gfIWPb7/99iLzJ0yYQP/+/fnf//7HFVdcQVBQEF26dOHRRx8964CjZfHaa68xffr0Eqc//vGPJa5TOChlSQrfg8LXGSr2/uzfvx+AmJiYct8itTq+a0XEWp7nbiIiUvf06dOHHTt2sGzZMm655RaWL19OWFgY7dq1c7dp27YtERER7iBV2i3POnXqxKxZs/jpp59wOp3Y7RU7bjpr1izy8/Px9PRkzJgxxZYX/qH83XffsXXr1jLdhu6pp57igw8+4N///jdr166tUF0V4efnd9Zln332GZMnTyYwMJA333yTvn37Ehsbi5+fHzabjSeffJKXXnoJY0y59lnR172qt3m2P9jPtawyqvIzPWnSJObOnUvr1q3585//TJcuXQgPD8fb2xuA7t27s2bNmmLvz5///Gfuuece/vOf/7Bq1SpWr17NtGnTmDZtGl26dGHZsmUEBAQAEB0dzQ8//MCKFStYvHgxq1ev5n//+x+rV6/mxRdf5KWXXmLixIkA7oMGgYGBDB06tEKvT2mfx/vvv58bbriBL7/8klWrVrFq1Spmz57N7NmzefbZZ1m5cmW5biFZWO+wYcPcz/dswsLCyrzd6vh8V7ehQ4fSv39/vvzyS1auXMnq1auZO3cuc+fO5ZlnnmHRokW0b98ecD2/Dz/8kCeffJJ58+axevVqVq9ezdSpU5k6dSqDBw9m7ty5eHh4lGnfw4cP5/7772fJkiUcPHiQuLg4fvrpJzZs2ECjRo0YMGBAkfb+/v4sWrSI77//ngULFvDdd9/x3Xff8cMPPzBlyhTuu+8+3nrrrSp/jSqqvN+PVakufhZF5Bys62UgIlJ9PvnkEwOYZs2auftCX3/99cXaFQ6ktGfPHtOyZUsDmE8++aRYux07drhHRf79QHAlKRxF/Pf9vAv7PZdleuyxx4qsW1If/0KF/T/79OlTY338f98P/0yFfc3/+te/lrh82LBhpfZ1Plsf/969e591n4X1lnX+ufpxjxo1qth7OHPmTAOYLl26nLWOwj61JfXxr4yq/ExHRUUZwKxfv77EfYWHh5e5b/jatWvd+3nmmWdKbZudnW2mTp1q7Ha7sdvtZufOncYYYw4dOmQA06BBg3LfNeNc7+PZbNmyxXTr1s0AZuTIkeXaZuGI9d9//3259nmuPv5n+50q/H0s7+904We/LO9j4XfW2X5nT5486d7ewYMHS93W/v37zbXXXmsA06tXr1LbOp1Os3jxYvfdCz744INz1nqmO++808BvA/bdf//9Borf7eRs8vPzzT//+U/j5+dnALN06dIyrVfZwf1CQkLO2qbwd/jMQQgr8v4cOXLEAMZut5f5jhCV+a4VkdpNh/NEpF4qvPx5165dfPjhh0Xmnanw0uiPPvrI3Ue+pHbNmzd39+d95JFHSE9PL3dNa9asYfPmzfj4+HDixIkiXQPOnArvVz5r1qxS+0Wf6eGHHyY6Opply5Yxf/78ctdW1Qr7fzdp0qTYstTUVBYtWlTTJVWJwku7f/zxxxLHVNi8eXORe4ZXpar8TJf2/nzzzTccPXq0zHV16dKF++67D4B169aV2tbX15d77rmHCy+8EKfT6e7yERsby4UXXsipU6dYsGBBmfddGa1bt3ZfcfD7uguvfDjb79+VV14JcM57p9clhZ+RGTNmlLi8sPtRSV2hfi8+Pp7nnnsOOPdnwmaz0a9fP2655ZYytf+9wsv9Z8yYQW5uLh9//DEAo0ePLtP6np6eDBs2jIEDB1Zo/xV18uRJ/vOf/xSbn5aW5v4dOPP3tiLvT3R0NElJSTidzjJ3HxOR+kvBX0TqpejoaPcgRH/5y1+A0kPSlClTANfARdHR0SVu86233qJ58+bs2LGD7t27s2LFihLb7d27t0jfzELvv/8+ANdeey0hISFnrX3AgAFER0eTkpLCV199ddZ2ZwoICOCZZ54B4I033ijTOtWp8LV/7733ivShTk9PZ9SoURU6cFIbNG3alMGDB+N0Orn33ns5deqUe1l6ejr33ntvqZfntm7dmtatW1eoS0ZVfqYLt/P3v/+9yPxt27Zxzz33lLj/uXPn8u233xbry5+fn+8OKmceSHjttdfcfYzPtHXrVveAd2e2/9Of/gS4+mWXFIiMMfzvf/9j4cKFJdZ3NkuXLuXrr78mPz+/2PYKf79+fwAkLi4OwD0I5e9NmDCBkJAQpkyZwl/+8pcSxwnYs2eP+wBNXTB27FiCgoL46aefePHFF4t8jn/++Wf3+zNhwoQi8+fMmUN2dnax7RW+h2e+tjNnzuTHH38s1vbUqVPu8ShKOhhVmu7du9OqVSt27NjBxIkTOXbsGD179nSPiXGmt99+u8QBRZOTk/nhhx8qtP/KePTRR4v8X5Gbm8u4ceM4ffo0Xbt2pUePHu5lFXl/AJ599lnA1SXs3//+d7EaNm/eXGzMDRGpn9THX0TqrT59+rBlyxaOHz9Ow4YN3f1Mz9S+fXsaNmzoPgNa2mj+oaGhrF69mltuuYUlS5Zw2WWXERcXR4cOHQgJCSE7O5sdO3awceNGjDG0b9+ezp07AxQZqf9sozIX8vDw4JZbbnEPiHfdddeV6fmOHTuW119/vVyjiFeXhx56iJkzZ/L111+TmJjIJZdcQn5+PitWrMDf35877rijzp6Bmjp1Khs2bGDp0qUkJCTQu3dvjDGsWLGCsLAwrrnmGr788kv3WeMzFYaOrKysCu27qj7Tzz77LMOGDeMPf/gDn376KRdccAGpqamsXLmSSy+9lNjY2GKj569YsYK//vWvhIeH07FjRyIjIzl16hT//e9/SU1NpVGjRjz++OPu9n/605+YMGECrVu3pk2bNvj5+XH48GH3CP8jR46kU6dO7vaDBw/mr3/9K48++ijXXHMNzZs3p1WrVgQHB5OWlsb69etJTU1l4sSJxfpul2bDhg08/PDDBAUF0alTJ2JjY8nOzuann35i3759BAcHFxuAbejQoSxbtoxbb72VAQMGEBoaCrhCVatWrYiLi+OLL75g6NChPPbYY7zyyiu0a9eOmJgY0tPT2bJlC7t27eLiiy/m1ltvLXOt1eW+++5zD8pWkrlz5xITE8NHH33E8OHDeeqpp5g1axYdO3YkNTWVFStW4HA4uP322xk7dqx7vX379nHTTTfh5+dHp06diI+Px+FwsHHjRrZt24a3tzevvPKKu/1nn33GqFGjiI2NpUOHDoSGhnLixAlWr15Neno67dq1K7L9srr99tt54okn+Otf/wqUPHAquA5Ejhs3joSEBNq1a0dQUBBpaWmsXLmS7Oxs+vbte9ZR+Evz2GOPERgYeNblDzzwQJHPOkC3bt1wOp20atWKvn374u/vz6pVqzh8+DCRkZHMnDmzSPuoqKhyvz8A119/PS+88AJPP/00w4YNo3Xr1iQlJZGdnc3OnTvZvHkz06ZNcx8MFJF6zJIOBiIiNeCf//ynuy/iddddd9Z2hX1RAfPPf/6zTNtevHixueOOO0yrVq1MUFCQ8fT0NKGhoaZTp07m7rvvNosWLSrSV/n99983gImOjjYOh+Oc21+3bp0BjIeHhzl06JAxpvQ+/oUK71mOxX38C7cxYsQI07hxY+Pj42OaNGli7rnnHpOcnFzuvs61pY9/odTUVDNu3DgTFxdnvL29TXx8vBk3bpw5duyY6du3rwHMN998c9ZaytLnuiRV+Zn+9ttvTb9+/Ux4eLjx9/c37dq1My+88ILJzc01vXv3Llbnzz//bJ544gnTs2dP06hRI+Pt7W0iIiLMRRddZF588UVz9OjRItv/8MMPze23327atWtnGjZs6P4MXHnllWbu3LlF7m9/po0bN5q77rrLtGjRwvj6+hp/f3+TmJhoBg4caP72t7+5fx8Knet93Llzp5k8ebLp16+fady4sfH19TWhoaHmwgsvNE888YQ5cOBAsXUKCgrMSy+9ZC644ALj6+t71vctJSXF/OEPfzCdOnUyDRo0MN7e3iYuLs50797dPPvss2bDhg1F2lvVx/9c05mv3ebNm82oUaNMXFyc8fLyMiEhIaZPnz5m9uzZxbZ/5MgR8+c//9lcddVVJiEhwfj7+5ugoCDTtm1bM27cOLN169Yi7b/99lvz0EMPma5du5ro6Gjj7e1toqOjTbdu3czf//53k5mZWa7nV+jw4cPGw8PDACYgIMCcOnWqxHZfffWVuffee03Hjh1NRESE+/267LLLzIwZM0xeXl6Z93nmd+S5prlz57rXO/O7LDMz00yYMMEkJCQYb29vExUVZUaPHm32799/1v2W5/0505o1a8zNN99sGjVqZLy8vEzDhg1NUlKSefzxx82+fftKrO9szvadKiK1m80YC4cMFRERqUdOnjxJYmIi6enppKSkuG8DKSICrlts9unTh969e7u7N4iI1AT18RcRESmnkvrop6WlMWrUKE6cOMGgQYMU+kVERKTWUB9/ERGRcrr44ouJi4ujTZs2hIWFcejQIX7++WcyMzNp3Lgxb775ptUlioiIiLgp+IuIiJTT008/zZIlS1i/fj0nTpzA29ubZs2aMWjQIB555BHCwsKsLlFERETETX38RUREREREROox9fEXERERERERqccU/EVERERERETqMfXxrwJOp5PDhw/ToEEDbDab1eWIiIiIiIhIPWeM4dSpU8TGxmK3l35OX8G/Chw+fJj4+HiryxAREREREZHzzIEDB4iLiyu1jYJ/FWjQoAHgesGDgoIsrkZERERERETqu4yMDOLj4915tDQK/lWg8PL+oKAgBX8RERERERGpMWXpbq7B/URERERERETqMQV/ERERERERkXpMwV9ERERERESkHlMffxERERERqZeMMTgcDgoKCqwuRaRCvLy88PDwqPR2FPxFRERERKTeycvL48iRI2RlZVldikiF2Ww24uLiCAwMrNR2FPxFRERERKRecTqd7NmzBw8PD2JjY/H29i7TyOcitYkxhrS0NA4ePEiLFi0qdeZfwV9EREREROqVvLw8nE4n8fHx+Pv7W12OSIVFRESwd+9e8vPzKxX8NbifiIiIiIjUS3a74o7UbVV1pYp+E0RERERERETqMQV/ERERERERkXpMwV9ERERERESq3JIlS2jTpo3lt1Ns2rQpb7zxRqltbDYbn3/+eY3WkpeXR9OmTfnhhx+qfb8K/iIiIiIiIrXE6NGjsdls2Gw2vLy8iIqK4vLLL+eDDz7A6XSWuu7kyZPp0KFDzRRaBo8//jhPP/10ldyHvrodOXKEK6+8skb36e3tzWOPPcbEiROrfV8K/iIiIiIiIrXIFVdcwZEjR9i7dy/z58+nT58+PPjggwwaNAiHw2F1eWWyatUqdu3axdChQ60upUyio6Px8fGp8f2OGDGCVatWsWnTpmrdj4K/iIiIiIhILeLj40N0dDSNGjWiU6dOPPnkk3zxxRfMnz+f6dOnV3i7GzdupG/fvvj5+REWFsZdd91FZmame/ny5cvp2rUrAQEBhISE0KNHD/bt2wfA+vXr6dOnDw0aNCAoKIiLLrqo1EvUZ8+ezeWXX46vr2+R+f/5z3/o0qULvr6+hIeHc/3117uXnThxgpEjRxIaGoq/vz9XXnklO3bscC+fPn06ISEhfPXVV7Rq1Qp/f3+GDRtGVlYWM2bMoGnTpoSGhvLAAw8U615w6tQpbr75ZgICAmjUqBFvvfVWkeVnXuq/d+9ebDYbn332GX369MHf35+kpCTWrFlTZJ1Vq1Zx6aWX4ufnR3x8PA888ACnT592L09NTWXw4MH4+fmRkJDARx99VOx1Cg0NpUePHsyePfusr2VV8KzWrYuIiIiIiNQSOTk57Ny5s0b32bx582LhtyL69u1LUlISn332GWPGjCn3+qdPn2bgwIF069aN77//ntTUVMaMGcP48eOZPn06DoeD6667jrFjx/LJJ5+Ql5fH2rVr3beTGzFiBB07dmTq1Kl4eHiwbt06vLy8zrq/lStXcssttxSZN2/ePK6//nqeeuopZs6cSV5eHl9//bV7+ejRo9mxYwdffvklQUFBTJw4kauuuorNmze795WVlcXf/vY3Zs+ezalTpxgyZAjXX389ISEhfP311+zevZuhQ4fSo0cPbrzxRve2X331VZ588kmee+45vvnmGx588EFatmzJ5Zdfftbn8NRTT/Haa6/RokULnnrqKW6++WZ27tyJp6cnu3bt4oorruBPf/oTH3zwAWlpaYwfP57x48czbdo09/M5fPgwy5Ytw8vLiwceeIDU1NRi++natSsrV64sw7tYcQr+IiIiIiIidUDr1q3ZsGFDhdb9+OOPycnJYebMmQQEBADw5ptvMnjwYF5++WW8vLxIT09n0KBBNGvWDIA2bdq419+/fz8TJkygdevWALRo0aLU/e3bt4/Y2Ngi81544QVuuukmnnvuOfe8pKQkAHfgX716Nd27dwfgo48+Ij4+ns8//5zhw4cDkJ+fz9SpU901Dhs2jFmzZpGSkkJgYCBt27alT58+LFu2rEjw79GjB0888QQALVu2ZPXq1bz++uulBv/HHnuMq6++GoDnnnuOCy64gJ07d9K6dWteeuklRowYwUMPPeR+Pf72t7/Ru3dvpk6dyv79+5k/fz5r166lS5cuALz//vtFXtNCsbGx7isrqouCv4iIiIiInBd8fX1p166d1WVUmDHGfQa+vLZs2UJSUpI79IMrDDudTrZt20avXr0YPXo0AwcO5PLLL6d///7ccMMNxMTEAPDII48wZswYZs2aRf/+/Rk+fLg7fJckOzu72JUO69atY+zYsWetz9PTk4svvtg9LywsjFatWrFlyxb3PH9//yL7jYqKomnTpgQGBhaZ9/sz6926dSv2+Fwj/V944YXunwtfh9TUVFq3bs369evZsGFDkcv3jTE4nU727NnD9u3b8fT05KKLLnIvb926NSEhIcX24+fnR1ZWVqm1VJaCv4iIiIiISB2wZcsWEhISqm3706ZN44EHHmDBggXMmTOHp59+mkWLFnHJJZcwefJkbrnlFubNm8f8+fN59tln+eSTT7j++uvAgDFOjDFgDMYYwsPDSEtNITf7tHuen68veTlZZJ066Z5X+G/2qXQATh1LwW63u+cXOPLJyczgZPJBstKP4+npyYnD+/l1p+ScSseO4fiB3RgMGMjLOkVO5imO7dsBBpwOB1knj3J073b3c808norTkV9kXkbqYY7u3c7xgwcBinRlKDzgUnhnhczMTO6++24eeOCBYq9j48aN2b59e7H5Z3P8+HEiIiLK3L4iFPxFRERERKT2czpdU0GBa3I6cTryKcjPIy83h/y8HPJzXVNOfj75nv7kZmZgcrPdodQVNM/8uZR/fw2W7nm/hkqDqdanmZOZQV5WZpFACrDyuzVs3LiRMbfdXGxZoayTx3Dk5Za4PD6iIdN+/pl9m9cR4O8PwKJlK7Db7YT7ebrXiQ8NYOzNQxl781CuHHIj77/zNs2jGwLQ0Btuu/5qbrv+au564BHee/tNenVqB4VXIbivRrBxQevWbPj5RzKPXe2e37Z1KxYvXszw664BW2GYdt26sHnzRBwOB2t/+JGLL+6KzW7nxIkT7Ny1mzYXXICnnz8e3j7YbOATFOxaz27Hy88fu4cnARExru3ZbHj5BeKV5yAkLhGbzYbd05MNW3cS3rSl+/X4ZftuLmh/YZF5QZGxhDdtSSbe53yfOnXqxObNm2nevHmJy1u3bo3D4eDHH390X+q/bds2Tp48WaztL7/8QseOHc+5z8pQ8BcRERERsYoxUFCAMz+fgtxc8nNzyMvOIjc3i7zcbBw5hYE2m/zcHFebvBwK8nNx5ObiyM+lID8XZ14+Bfm5FOTn4czLw+nI/3XKwzgcroDsLKDAWfBriP1133BGWPst1No4++XkZW1jw1biPirEGIzNhtNux9jA2O047TaMzYax27F7emHz8MTu4YnNyxP/4Eia97yGrKxMHN5e7kBos9mw2eyun+32Xx+75v3+MTYbdvc8Ozb7r//abO7nXtHL7kvjGxhE+ulsHL5BFBQUkJKSwoIFC3jppZcYNGgQ4x55HA8PjxLX9Q8JI7/AycGTRS8bb9CgAXc/+Aiv/v1tHn3mT0yePJm0tDSe/tOfue2222jbpTt79uzhvffe45prriE2NpZt27axZ/8B7hh7FwFR8UyYMIFhw4aRkJDAwYMH2bB5K0OHDi0SnM80+LrrmTFjBmFxv12h8PyLL9GvXz/atruQm266CYfDwddff83EiRO5KDKWa6+9locnPMG7775LgwYNeOKJJ2jUqBE33XIrXl5e+PgFADb8A4Pd2/T08sZmt+Pj6+ee53rf7EVep9WrV/PKK69w3XXXsWjRIv75z38yb968irxFAEycOJFLLrmE8ePHM2bMGAICAti8eTOLFi3izTffpFWrVlxxxRXcfffdTJ06FU9PTx566CH8/PyKbWvlypU8//zzFa6lLBT8RURERKTGGKeTgrw8crNPk5uTRW72afKzs8nNOU1+TrYr4OZkk5+XgyMnB0duDo68HArycotO7oDr+tc4HJj8PJw4cWLcl/wClJRPbQbMmfPPPIlrK/7YZkpuVuS5lbyrUl4MAzYbTpsNp4cd42nH5uGFzdMTu6cXdi9v189e3nh6+2D38sbD0xu7t+uxp58PHkEBeHr74uXji6e3D14+fnh6++Dt64+3rx9e3r54+/nj4+2Ht7cPPl4+eHp6VktgrU1ycnLYs2cPoZGxVTKifk1bsGABMTExeHp6EhoaSlJSEn/7298YNWqU6zL4Umzfvr3Y2eN+/fqxePFi92j2Xbp0wd/fn6FDhzJlyhTA1Xd+69atzJgxg2PHjhETE8O4ceO4++67cTgcHDt2jJEjR5KSkkJ4eDhDhgwpMkjf740YMYLHH3+cbdu20apVKwAuu+wy/vnPf/L888/z5z//maCgIHr16uVeZ9q0aTz44IMMGjSIvLw8evXqxddff13q3QPK6tFHH+WHH37gueeeIygoiClTpjBw4MAKb+/CCy9kxYoVPPXUU1x66aUYY2jWrFmRAQWnTZvGmDFj6N27N1FRUfzpT3/iD3/4Q5HtrFmzhvT0dIYNG1bhWsrCZtyH/KSiMjIyCA4OJj09naCgIKvLERERkTrI6XCQn5VFflYWjqwsHNnZOLKzycs6TW5WJnnZp11TThZ5OVk4fj0b7MjLIT8vm4K8XJy5rrO/psBBAU6cxonTFPwahJ0UxlJXrnWdkTXFHv82p7QzuiUxGFdA/n2o/DXguh962LF5emHzcoVb9+Tt+tfD2wdPb1/Xvz6unz19ffHy9sPL1w9PH19XsPXzdwdcH78AfHz98fbywcPugd1WejiS+q0w+CckJNTJ4F9fTJgwgYyMDN59912rS6m1brzxRpKSknjyySdLXF7aZ7k8OVRn/EVERKTKGWNwOBwUFBTg5eVV7LLUvOws1xnMs1yu6nQ6ycnL4XTOabKyMsk+dZKsk8fJzjhBTmY6uadcU/7pUziyMinIOk1B1mlMbs6v/Xdd27HDWS8xdhqD7YzzHza7Hacx2H932XNh+DVnCcJnXvZc0hnf388zZ8yxnTnPZsd4eYG3J8bbE+PtBV5e2Ly98fDxxePXwOvl6493sD9eviF4+/oT4O+Pr18gvn6B+PgF4BfQAB8ff7w8vPCye7n/9bCX/FqLiFSXp556irfffhun03nOKxXOR3l5ebRv356HH3642velM/5VQGf8RUSkNnE48sk6ncGxPbs4vnEDZGXi4+kBzgJOnkzj0P7tZJ0+ia2EP8KM04nNbj/7mduirX9NtbbfzXUFZJvNhs1uw1ng/C1Q/3rm1+bhgXE4XO1trtBc2KZwfU8PTzztnnh6emHz9cMzIBAv/0C8AhrgFRiEb4Ng/IJD8Q9qiF9IKA1CwgkMCcPD69yDMolI/aYz/lJf6Iy/iIhINTJOJ3m5WeSdziA/K5O8bNeZ5dzMDLLSj5N7KoO8zAzyMk/hOJ1J/ulM8vJzyHfk4yhw4HQ4XQNpFRT8usHfHWf/NQQX9jE+M2QXhm5T2K+4sL9xaWeuz1hms9lcl0s3DMOzWROcwf7kOArA5kl4fGu63XIHjRud/d7LIiIiUr8o+IuISO1kDCY/n5xTJ8g8nkbGiVSyThxzXeqdcYLcjJPkZ54iPzMDR85p8hz5FBQU4Pj1Fk/lHUG6yGXcxrjCtocXTi8f8PLG6e0NXj7YffxdZ50DG+ATGIpfdCK+DYIJDmpIQEAQgX6B+Pr64uvri4+PD97e3vV+EC0RERGp3RT8RUSkfJxOnKdOkX3sKCeTD5KRdoTMo8lkHztKzsmj5GVmkJWbRU5BrntsbSj9tk9nDvx1ZgAv8LBjvH2x+/vj6d8A74AgvBsE49MgBN+oOMJCIggIDaNBcBgN/IPcYftstzkSEREROR8p+IuI1DPGGLJzsjl69AjpqYdJTztMZloy2SeOknvyGHnpJ3BmZmAK8oFzBPLCbZ45eJnNRp63J85Af7yCQvAJCcO3YRgBjVsRFH4ZjcKiCQ+OINg/GB9PH53tFhEREbGYgr+IiAXy8vNIPZlK2olUjh09QnrqIU4fTyXrWCq5J49jsjLxzXPgnZeP3fwWvAudOcr4mY8L53l5eLlucRUUindoQ/xCw2nYtCVBkTGERMcRGhWHt2+AQrmIiIjIeUDBX0SkFMYYMrIzSD5+hKPJ+zmZcoiMtMNkHU8j5+RRTEYm3rl5eOcXFB+87XfbOTNk2212/H38CfANxDcwmLiwCALDownu2IGQ6DgCGkZia9AA/P1Bl62LiIiISCUo+ItIvWGM4XRWBqmH9nA8+QDpKYfIOHrEdRY9/TjOUxl45hfg5Nd7e5dhezabDR8PH/x9A/ANbkhQaASNImIIvqA9odHxBEXFYA8KAj+/cg8mJyIiIiJSExT8RcQa+fnkHj/G0cP7OJ68n/TUw5w+mkLWiTRyTx4jJ+e0e5A3569n0u22opezw699zs+4DN7Twxvv4FB8Q8PxbxhBbGQCIe17Et6oKSFRjfBQQBcRERGpEUuWLGH8+PH88ssv1T7wbtOmTXnooYd46KGHyMvLo2XLlvzrX/+ic+fO1brfukLB/wxvvfUWr776KsnJySQlJfH3v/+drl27Wl2WSK1Q4HCQnnKEtIO7OXF4Hxmph8hKSyH3eBr5GSdwGIe7rTEG7Lbf7kNeQngvsNtx+PngFRyKf2gEAWFRBLdtT0xMHBExTYkIj8HTQ19RIiIicn4ZPXo0M2bMAMDT05OGDRty4YUXcvPNNzN69Gjs9rNftzh58mQ+//xz1q1bV0PVlu7xxx/n6aefrvG77Xh7e/PYY48xceJElixZUqP7rq30V/Wv5syZwyOPPMI777zDxRdfzBtvvMHAgQPZtm0bkZGRVpcnUi7GGHKyszmRcpijh/ZyMvUgGamHyTqWSvbJNAoyM7Dl57oGhCupX3pJZ8RtNuwBAXiHhuEXHkVgZCyN2lxAWFwCkTFNCfAN1EBxIiIiIlXgiiuuYNq0aRQUFJCSksKCBQt48MEH+de//sWXX36Jp2ftj3GrVq1i165dDB069Kxt8vLy8Pb2rpb9jxgxgkcffZRNmzZxwQUXVMs+6pLa/4mpIVOmTGHs2LHcfvvtALzzzjvMmzePDz74gCeeeMLi6qQ+Mk4nmSeOcezwXk4c3u8aMO5oMjnH08g7eYzsrEwwBie/5fDCW53bDEXDeWF4P3PwOA8PPBsE4RPSEP+GkQRHRNO0XRJh0Y1pGN0Yb/8G2Eo5YiwiIiIi1vDx8SE6OhqARo0a0alTJy655BL69evH9OnTGTNmTIW2u3HjRh588EHWrFmDv78/Q4cOZcqUKQQGBgKwfPlyHn/8cTZt2oSXlxcXXHABH3/8MU2aNGH9+vU89NBD/PDDD9hsNlq0aMG777571kvpZ8+ezeWXX46vr697XuEVCePHj+eFF15g3759OJ1OTp48yWOPPcYXX3xBbm4unTt35vXXXycpKQmAXbt28cgjj/Df//6X06dP06ZNG1566SX69+9/1ucaGhpKjx49mD17Ns8//3yFXq/6RMEf15GmH3/8kUmTJrnn2e12+vfvz5o1a4q1z83NJTc31/04IyOjRuqsDs6CAowxGOMs8WenswDjdLou3S6cfnXmZUY2Q9ERywuXndHeYHA6nTjP3L5x4vx1+07jxBQuN85fazFn1GGKLHO3dxZgHA5wOnE6HFBQgCkooKAg3/WvIw+nw4GzwIHT6aAgLw9HXh75+bkU5OfjyPv13/w8CvLzcObm4sxzTTjysTscrvud22zuPufu51bKGe7Ctr8/n+4aMM51+bvN1w+P4BB8QyPwC4ukQfO2xEXH0zC2CVGRjerE0VwRERGRuiLHkcPO4ztrdJ/NGzbH19P33A3PoW/fviQlJfHZZ59VKPifPn2agQMH0q1bN77//ntSU1MZM2YM48ePZ/r06TgcDq677jrGjh3LJ598Ql5eHmvXrnX/jT9ixAg6duzI1KlT8fDwYN26dXh5eZ11fytXruSWW24pNn/nzp38+9//5rPPPnN3ARg+fDh+fn7Mnz+f4OBg3n33Xfr168f27dtp2LAhmZmZXHXVVbzwwgv4+Pgwc+ZMBg8ezLZt22jcuPFZa+jatSsrV64s92tVHylVAEePHqWgoICoqKgi86Oioti6dWux9i+99BLPPfdcTZVXpb59cyTmyLrfzbVRmGVt2DG2wlm//mD/tc0Z7YBigRYDxmZKacCvp6t/Db2/bsxmK9zur3u0Fe7PtcxpPHE4vcgt8HNNDj9sNjt27GADY7Nhs3tgPOxgt7t/tnl4YLd7YrN7YPN0/Wv38MTD0wtPHx8CfAPw8vbB09sHb19fvLx98PLxwdsvAG//AHz9G+AdEIiXjz9e3r7Y7bqlmoiIiIhYp3Xr1mzYsKFC63788cfk5OQwc+ZMAgICAHjzzTcZPHgwL7/8Ml5eXqSnpzNo0CCaNWsGQJs2bdzr79+/nwkTJtC6dWsAWrRoUer+9u3bR2xsbLH5eXl5zJw5k4iICMDVJWDt2rWkpqbi4+MDwGuvvcbnn3/Ov/71L+666y6SkpLcZ/8Bnn/+eebOncuXX37J+PHjz1pDbGws+/btK8vLU+8p+FfApEmTeOSRR9yPMzIyiI+Pt7Cisus1fqbVJZRfQS7kp0NOKuSkQO5RMAXF29k8IaApBMSDTzjYz34EUkRERETOP76evrSLbGd1GRVWeKvhitiyZQtJSUnu0A/Qo0cPnE4n27Zto1evXowePZqBAwdy+eWX079/f2644QZiYmIAeOSRRxgzZgyzZs2if//+DB8+3H2AoCTZ2dlFLvMv1KRJE3foB1i/fj2ZmZmEhYUVW3/Xrl0AZGZmMnnyZObNm8eRI0dwOBxkZ2ezf//+Up+zn58fWVlZ535xzgMK/kB4eDgeHh6kpKQUmZ+SkuLuW3MmHx8f99EoqQEePuARCb6RQClf1M58yNwLGdsgdxU4CwCna5kxYLNDSHsIbuv6WURERESkDtmyZQsJCQnVtv1p06bxwAMPsGDBAubMmcPTTz/NokWLuOSSS5g8eTK33HIL8+bNY/78+Tz77LPMnj2b66+/vsRthYeHc+LEiWLzzzzwAK5QHxMTw/Lly4u1DQkJAeCxxx5j0aJFvPbaazRv3hw/Pz+GDRtGXl5eqc/n+PHjRQ4ynM8U/HHd7uGiiy5iyZIlXHfddQA4nU73fSeljrB7QVAL11QSpwNOboR9s8H8ekDANxLCLgbv4JqrU0RERESknJYuXcrGjRt5+OGHK7R+mzZtmD59OqdPn3aH79WrV2O322nVqpW7XceOHenYsSOTJk2iW7dufPzxx1xyySUAtGzZkpYtW/Lwww9z8803M23atLMG/44dO7J58+Zz1tWpUyeSk5Px9PSkadOmJbZZvXo1o0ePdu8rMzOTvXv3nnPbv/zyCx07djxnu/OBgv+vHnnkEUaNGkXnzp3p2rUrb7zxBqdPn3aP8i/1gN0TGnZ0TYVyUiF1BeRnAAYCmkBET10RICIiIiKWyc3NJTk5ucjt/F566SUGDRrEyJEjS103OzubdevWFZnXoEEDRowYwbPPPsuoUaOYPHkyaWlp3H///dx2221ERUWxZ88e3nvvPa655hpiY2PZtm0bO3bsYOTIkWRnZzNhwgSGDRtGQkICBw8e5Pvvvy/1Vn0DBw5kxowZ53yu/fv3p1u3blx33XW88sortGzZksOHDzNv3jyuv/56OnfuTIsWLfjss88YPHgwNpuNP/zhDzidznNue+XKlRrR/1cK/r+68cYbSUtL45lnniE5OZkOHTqwYMGCYgP+ST3jGwlx1/z2OHMv7P3YNYZARE9ocPZ+SyIiIiIi1WHBggXExMTg6elJaGgoSUlJ/O1vf2PUqFFF7qxVku3btxc7y92vXz8WL17MN998w4MPPkiXLl2K3M4PwN/fn61btzJjxgyOHTtGTEwM48aN4+6778bhcHDs2DFGjhxJSkoK4eHhDBkypNQBz0eMGMHjjz/Otm3bilxR8Hs2m42vv/6ap556ittvv520tDSio6Pp1auXO4tNmTKFO+64g+7duxMeHs7EiRPPeWe1NWvWkJ6ezrBhw0ptd76wGWNKGntdyiEjI4Pg4GDS09MJCgqyuhypCsZA2irI3AXeDSHmCvDwtroqERERESmDnJwc9uzZQ0JCQokDzEnNmDBhAhkZGbz77rs1vu8bb7yRpKQknnzyyRrfd1Uq7bNcnhyq65lFSmKzQeSlkDjadeb/wL9h93TIOmh1ZSIiIiIidcJTTz1FkyZNynRZflXKy8ujffv2FR4PoT7SGf8qoDP+5wnjhCOLIPsQhHWFkLp7KxgRERGR+kxn/KW+0Bl/kZpms0PsQGh2BzhOw873XLcOFBERERERqcUU/EUqIvxiaH4XZB+Gnf8HOUetrkhERERERKREGtVfpDKi+kBELzg41/U4fohuBSgiIiIiIrWKEopIZdk9oPEwiOwNu/4BJzdaXZGIiIiIiIibgr9IVfGNcF3+n3sM9n7iGgxQRERERETEYgr+IlUt6jKIvhy2vw3ZKVZXIyIiIiIi5zkFf5Hq4BsOLcdB8iI4/pPV1YiIiIiIyHlMwV+kuthskHAr5KfDoa+srkZEREREpEYtWbKENm3aUFBQ4J733nvvER8fj91u54033ijTdpo2bVrmtiW56aab+Mtf/lLh9esDBX+R6hbVBwKbw65p6vcvIiIiIqUaPXo0NpsNm82Gl5cXUVFRXH755XzwwQc4naX/LTl58mQ6dOhQM4WWweOPP87TTz+Nh4cHABkZGYwfP56JEydy6NAh7rrrrhqp4+mnn+aFF14gPT29RvZXGyn4i9SE4NYQdw1sfwscp62uRkRERERqsSuuuIIjR46wd+9e5s+fT58+fXjwwQcZNGgQDofD6vLKZNWqVezatYuhQ4e65+3fv5/8/HyuvvpqYmJi8Pf3r5Fa2rVrR7Nmzfjwww9rZH+1kYK/SE3xCYPmd8PuGZB10OpqRERERKSW8vHxITo6mkaNGtGpUyeefPJJvvjiC+bPn8/06dMrvN2NGzfSt29f/Pz8CAsL46677iIzM9O9fPny5XTt2pWAgABCQkLo0aMH+/btA2D9+vX06dOHBg0aEBQUxEUXXcQPP/xw1n3Nnj2byy+/HF9fXwCmT59O+/btAUhMTMRms7F371527drFtddeS1RUFIGBgXTp0oXFixeX+jz+8Y9/EBISwpIlSwD45ZdfuPLKKwkMDCQqKorbbruNo0ePFlln8ODBzJ49u/wvWj3haXUBIucVD29ocS/smQURPaBBM6srEhERETl/5OTAzp01u8/mzeHX8FsZffv2JSkpic8++4wxY8aUe/3Tp08zcOBAunXrxvfff09qaipjxoxh/PjxTJ8+HYfDwXXXXcfYsWP55JNPyMvLY+3atdhsNgBGjBhBx44dmTp1Kh4eHqxbtw4vL6+z7m/lypXccsst7sc33ngj8fHx9O/fn7Vr1xIfH09ERAS//PILV111FS+88AI+Pj7MnDmTwYMHs23bNho3blxsu6+88gqvvPIKCxcupGvXrpw8eZK+ffsyZswYXn/9dbKzs5k4cSI33HADS5cuda/XtWtXXnjhBXJzc/Hx8Sn361fXKfiL1DSbDRJHwt5PwDggqJXVFYmIiIhIHdC6dWs2bNhQoXU//vhjcnJymDlzJgEBAQC8+eabDB48mJdffhkvLy/S09MZNGgQzZq5Tk61adPGvf7+/fuZMGECrVu3BqBFixal7m/fvn3Exsa6HxdeZQAQERFBdHQ0AElJSSQlJbnbPf/888ydO5cvv/yS8ePHF9nmxIkTmTVrFitWrOCCCy5wP4eOHTvy4osvutt98MEHxMfHs337dlq2bAlAbGwseXl5JCcn06RJk3K8cvWDgr+IVZreDPv/CU4HhFxgdTUiIiIi9Z+vL7RrZ3UVFWaMcZ+BL68tW7aQlJTkDv0APXr0wOl0sm3bNnr16sXo0aMZOHAgl19+Of379+eGG24gJiYGgEceeYQxY8Ywa9Ys+vfvz/Dhw90HCEqSnZ3tvsy/NJmZmUyePJl58+Zx5MgRHA4H2dnZ7N+/v0i7v/zlL5w+fZoffviBxMRE9/z169ezbNkyAgMDi217165d7uDv5+cHQFZW1jlrqo/Ux1/ESo2Hw6ntcGK91ZWIiIiISC23ZcsWEhISqm3706ZNY82aNXTv3p05c+bQsmVL/vvf/wKuOwZs2rSJq6++mqVLl9K2bVvmzp171m2Fh4dz4sSJc+7zscceY+7cubz44ousXLmSdevW0b59e/Ly8oq0u/TSSykoKODTTz8tMj8zM5PBgwezbt26ItOOHTvo1auXu93x48cB19UG5yMFfxGrxV8Pp/fDsbMPjiIiIiIi57elS5eycePGIqPkl0ebNm1Yv349p0//doep1atXY7fbadXqt66nHTt2ZNKkSXz33Xe0a9eOjz/+2L2sZcuWPPzwwyxcuJAhQ4Ywbdq0s+6vY8eObN68+Zx1rV69mtGjR3P99dfTvn17oqOj2bt3b7F2Xbt2Zf78+bz44ou89tpr7vmdOnVi06ZNNG3alObNmxeZzry64ZdffiEuLo7w8PBz1lQfKfiL1AZxgyE3DY7+1+pKRERERMRiubm5JCcnc+jQIX766SdefPFFrr32WgYNGsTIkSNLXTc7O7vY2e9du3YxYsQIfH19GTVqFL/88gvLli3j/vvv57bbbiMqKoo9e/YwadIk1qxZw759+1i4cCE7duygTZs2ZGdnM378eJYvX86+fftYvXo133//fZExAH5v4MCBrFq16pzPtUWLFnz22WesW7eO9evXc8stt+B0Okts2717d77++muee+453njjDQDGjRvH8ePHufnmm/n+++/ZtWsX33zzDbfffjsFBQXudVeuXMmAAQPOWU99pT7+IrVF7JVwZBGkroLInlZXIyIiIiIWWbBgATExMXh6ehIaGkpSUhJ/+9vfGDVqFHZ76edut2/fTseOHYvM69evH4sXL+abb77hwQcfpEuXLvj7+zN06FCmTJkCgL+/P1u3bmXGjBkcO3aMmJgYxo0bx913343D4eDYsWOMHDmSlJQUwsPDGTJkCM8999xZ6xgxYgSPP/4427ZtK3JFwe9NmTKFO+64g+7duxMeHs7EiRPJyMg4a/uePXsyb948rrrqKjw8PLj//vtZvXo1EydOZMCAAeTm5tKkSROuuOIK92uVk5PD559/zoIFC0p97eozmzHGWF1EXZeRkUFwcDDp6ekEBQVZXY7UdUcWgU8YNOxkdSUiIiIidVJOTg579uwhISGhTAPMSfWYMGECGRkZvPvuu5bWMXXqVObOncvChQstraMiSvsslyeH6lJ/kdom5nLXYH+5x62uRERERESkwp566imaNGly1kv3a4qXlxd///vfLa3Bagr+IrVRwkjYMwuc+VZXIiIiIiJSISEhITz55JPn7J5Q3caMGVNqd4PzgYK/SG1k94Bmd8COqeB0WF2NiIiIiIjUYQr+IrWVVwNIHA3b3wJHttXViIiIiIhIHaXgL1KbeQVBi3tg9weQdcjqakREREREpA5S8Bep7Tx8oMV9kPotHP/R6mpERERERKSOUfAXqQtsNmh6M+Slw6F5VlcjIiIiIiJ1iIK/SF0S3RcCGsOBuVZXIiIiIiIidYSCv0hdE9IeAhPg8DdWVyIiIiIiInWAgr9IXRTaATBwcpPVlYiIiIiISC2n4C9SV8VeAcfWQt4JqysRERERkSqUnJzMgw8+SPPmzfH19SUqKooePXowdepUsrKyzrre5MmT6dChQ80VKnWGp9UFiEglJIyEHW9Di3vBrl9nERERkbpu9+7d9OjRg5CQEF588UXat2+Pj48PGzdu5L333qNRo0Zcc801VpcpdYzO+IvUZXYPSBwFu/4PjLG6GhERERGppPvuuw9PT09++OEHbrjhBtq0aUNiYiLXXnst8+bNY/DgwRXe9saNG+nbty9+fn6EhYVx1113kZmZ6V6+fPlyunbtSkBAACEhIfTo0YN9+/YBsH79evr06UODBg0ICgrioosu4ocffqj085WaoVOEInWdVxDEDYHd06HZ7VZXIyIiIlJ7FeTAqZ01u88GzcHDt0xNjx07xsKFC3nxxRcJCAgosY3NZqtQGadPn2bgwIF069aN77//ntTUVMaMGcP48eOZPn06DoeD6667jrFjx/LJJ5+Ql5fH2rVr3fsbMWIEHTt2ZOrUqXh4eLBu3Tq8vLwqVIvUPAV/kfrALwqiesO+OdDkRqurEREREZEK2LlzJ8YYWrVqVWR+eHg4OTk5AIwbN46XX3653Nv++OOPycnJYebMme6DCm+++SaDBw/m5ZdfxsvLi/T0dAYNGkSzZs0AaNOmjXv9/fv3M2HCBFq3bg1AixYtKvQcxRoK/iL1RWAiOLJh/7+h8VCrqxERERGpfTx8IaSd1VWU29q1a3E6nYwYMYLc3NwKbWPLli0kJSUVuZKgR48eOJ1Otm3bRq9evRg9ejQDBw7k8ssvp3///txwww3ExMQA8MgjjzBmzBhmzZpF//79GT58uPsAgdR+6uMvUp+EXADBbWD/v6yuRERERETKqXnz5thsNrZt21ZkfmJiIs2bN8fPz69a9z9t2jTWrFlD9+7dmTNnDi1btuS///0v4LpjwKZNm7j66qtZunQpbdu2Ze7cudVaj1QdBX+R+ia4rWs6+KXVlYiIiIhIOYSFhXH55Zfz5ptvcvr06Srddps2bVi/fn2R7a5evRq73V6ka0HHjh2ZNGkS3333He3atePjjz92L2vZsiUPP/wwCxcuZMiQIUybNq1Ka5Tqo+AvUh8FtwXvEDi61upKRERERKQc3n77bRwOB507d2bOnDls2bKFbdu28eGHH7J161Y8PDxKXT87O5t169YVmXbt2sWIESPw9fVl1KhR/PLLLyxbtoz777+f2267jaioKPbs2cOkSZNYs2YN+/btY+HChezYsYM2bdqQnZ3N+PHjWb58Ofv27WP16tV8//33RcYAkNpNffxF6qvIXrDvU/CLhoDGVlcjIiIiImXQrFkzfv75Z1588UUmTZrEwYMH8fHxoW3btjz22GPcd999pa6/fft2OnbsWGRev379WLx4Md988w0PPvggXbp0wd/fn6FDhzJlyhQA/P392bp1KzNmzODYsWPExMQwbtw47r77bhwOB8eOHWPkyJGkpKQQHh7OkCFDeO6556rtdZCqZTNGN/+urIyMDIKDg0lPTycoKMjqckR+YwzsfAcSRoGnv9XViIiIiNSInJwc9uzZQ0JCAr6+ZbuVnkhtVNpnuTw5VJf6i9RnNhsk3gm7PnAdBBARERERkfOOgr9IfefhDY2Hw96PrK5EREREREQsoOAvcj7wi4LgCyB5idWViIiIiIhIDVPwFzlfNOwIBTmQvtnqSkREREREpAYp+IucTxpdDcfWQnay1ZWIiIiIVDuNYy51XVV9hhX8Rc43CaNg/78gP9PqSkRERESqhZeXFwBZWVkWVyJSOXl5eQB4eHhUajueVVGMiNQhNhs0vwt2TIWW48CurwERERGpXzw8PAgJCSE1NRVw3aPeZrNZXJVI+TidTtLS0vD398fTs3J/s+svfpHzkYc3JI6G3dOg+VirqxERERGpctHR0QDu8C9SF9ntdho3blzpA1cK/iLnK+9giOwFh76CRoOsrkZERESkStlsNmJiYoiMjCQ/P9/qckQqxNvbG7u98j30FfxFzmdBreD0fjixHkKTrK5GREREpMp5eHhUun+0SF2nwf1Ezncxl8OJnyH3mNWViIiIiIhINVDwFxFIGAl7PwJngdWViIiIiIhIFVPwFxGw2V3hf890qysREREREZEqpuAvIi7eIRB2CRxeYHUlIiIiIiJShRT8ReQ3IRe4/j25ydo6RERERESkyij4i0hRsVfAsf9C3kmrKxERERERkSpQr4P/3r17ufPOO0lISMDPz49mzZrx7LPPkpeXV6Tdhg0buPTSS/H19SU+Pp5XXnnFoopFaomE0bBnFhin1ZWIiIiIiEgleVpdQHXaunUrTqeTd999l+bNm/PLL78wduxYTp8+zWuvvQZARkYGAwYMoH///rzzzjts3LiRO+64g5CQEO666y6Ln4GIRewe0PQW2DMTEkdbXY2IiIiIiFSCzRhjrC6iJr366qtMnTqV3bt3AzB16lSeeuopkpOT8fb2BuCJJ57g888/Z+vWrWXaZkZGBsHBwaSnpxMUFFRttYvUuBPrITcNovtbXYmIiIiIiJyhPDm0Xl/qX5L09HQaNmzofrxmzRp69erlDv0AAwcOZNu2bZw4caLEbeTm5pKRkVFkEqmXQpOgIAcytlldiYiIiIiIVNB5Ffx37tzJ3//+d+6++273vOTkZKKiooq0K3ycnJxc4nZeeuklgoOD3VN8fHz1FS1itUaDIHUFOE5bXYmIiIiIiFRAnQz+TzzxBDabrdTp95fpHzp0iCuuuILhw4czduzYSu1/0qRJpKenu6cDBw5UansitV7CaNg9Hc6vnkEiIiIiIvVCnRzc79FHH2X06NGltklMTHT/fPjwYfr06UP37t157733irSLjo4mJSWlyLzCx9HR0SVu28fHBx8fnwpULlJHeXhDo8FwcC7ED7G6GhERERERKYc6GfwjIiKIiIgoU9tDhw7Rp08fLrroIqZNm4bdXvQih27duvHUU0+Rn5+Pl5cXAIsWLaJVq1aEhoZWee0idVZAY8jcBSfWQWgHq6sREREREZEyqpOX+pfVoUOHuOyyy2jcuDGvvfYaaWlpJCcnF+m7f8stt+Dt7c2dd97Jpk2bmDNnDn/961955JFHLKxcpJaK6uMK/rnHrK5ERERERETKqE6e8S+rRYsWsXPnTnbu3ElcXFyRZYV3MQwODmbhwoWMGzeOiy66iPDwcJ555hnuuusuK0oWqf0SRsL2t6DFvWCv118hIiIiIiL1gs0YjdZVWeW5f6JIvZCXDvv/Cc3HWF2JiIiIiMh5qTw5tF5f6i8i1cQ7GCJ6wKF5VlciIiIiIiLnoOAvIhUT3AbsXnBig9WViIiIiIhIKRT8RaTiYgbAiZ802J+IiIiISC2m4C8ilZMwEvZ+DE6H1ZWIiIiIiEgJFPxFpHJsdlf43z3d6kpERERERKQECv4iUnmFg/0dXmB1JSIiIiIi8jsK/iJSNYLbgDMfTu2yuhIRERERETmDgr+IVJ24wZC8EApyrK5ERERERER+peAvIlUrYbT6+4uIiIiI1CIK/iJStTz9IKovHJpndSUiIiIiIoKCv4hUh6CWgA0ytltdiYiIiIjIeU/BX0SqR6OrIGUZOLKsrkRERERE5Lym4C8i1SdxNOz6BzgLrK5EREREROS8peAvItXHwwcSbnOFf2OsrkZERERE5Lyk4C8i1cs7FGKvgH1zrK5EREREROS8pOAvItUvoAkEt4bD31hdiYiIiIjIeUfBX0RqRmgHsHvC8Z+trkRERERE5Lyi4C8iNSe6H6T/AjmpVlciIiIiInLeUPAXkZrV9FZXf/+CPKsrERERERE5Lyj4i0jNstmg2R0a6V9EREREpIYo+ItIzfMMgLhrYN9sqysREREREan3FPxFxBr+cRDUCpKXWF2JiIiIiEi9puAvItZp2Amc+XBig9WViIiIiIjUWwr+ImKt2CvgxDrITra6EhERERGReknBX0Ssl3Ab7P8XOLKtrkREREREpN5R8BcR69ls0HwM7P5AI/2LiIiIiFQxBX8RqR08fCHuWtg3x+pKRERERETqFQV/Eak9/OMgMAFSV1pdiYiIiIhIvaHgLyK1S/jFkJsGmXusrkREREREpF5Q8BeR2id+CBxZAI7TVlciIiIiIlLnKfiLSO2UeCfsmqbB/kREREREKknBX0RqJw9vaDwM9n5sdSUiIiIiInWagr+I1F5+0RDcBlKWWV2JiIiIiEidpeAvIrVbw07gyIaTG62uRERERESkTlLwF5Har9FVcGI9ZB20uhIRERERkTpHwV9E6oamI+DQfyA/w+pKRERERETqFAV/EakbbDZoNtY10r+zwOpqRERERETqDAV/Eak77J6QOAp2f2B1JSIiIiIidYaCv4jULd4hENUHDnxudSUiIiIiInWCgr+I1D0NmoNPOBz9r9WViIiIiIjUegr+IlI3RfaErEOQudvqSkREREREajUFfxGpuxoPheTFkJdudSUiIiIiIrWWgr+I1G2Jd8CemeB0WF2JiIiIiEitpOAvInWb3RMSRmqkfxERERGRs1DwF5G6zzsYogfAvk+trkREREREpNZR8BeR+iGwKQS1hMPfWF2JiIiIiEitouAvIvVHaAfw8NVt/kREREREzqDgLyL1S1RvyEmF9C1WVyIiIiIiUiso+ItI/RN3DRz/AbIOW12JiIiIiIjlFPxFpH5qeisc+gLyM6yuRERERETEUgr+IlI/2WzQbCzsngEFeVZXIyIiIiJiGQV/Eam/7J7Q7A7Y+R4Yp9XViIiIiIhYQsFfROo3zwBoegvs+gcYY3U1IiIiIiI1TsFfROo/n4YQexXsmWV1JSIiIiIiNU7BX0TOD/5xEH4x7P+31ZWIiIiIiNQoBX8ROX8EtYIGzeHQV1ZXIiIiIiJSYxT8ReT8EpoEPpFwZJHVlYiIiIiI1AgFfxE5/4R3BQ9fSP3W6kpERERERKrdeRP8c3Nz6dChAzabjXXr1hVZtmHDBi699FJ8fX2Jj4/nlVdesaZIEak5kZeCMw+OfW91JSIiIiIi1eq8Cf6PP/44sbGxxeZnZGQwYMAAmjRpwo8//sirr77K5MmTee+99yyoUkRqVHR/yDoIGdutrkREREREpNqcF8F//vz5LFy4kNdee63Yso8++oi8vDw++OADLrjgAm666SYeeOABpkyZYkGlIlLj4q+Ho/+FrENWVyIiIiIiUi3qffBPSUlh7NixzJo1C39//2LL16xZQ69evfD29nbPGzhwINu2bePEiRMlbjM3N5eMjIwik4jUYQm3weGvITvZ6kpERERERKpcvQ7+xhhGjx7NPffcQ+fOnUtsk5ycTFRUVJF5hY+Tk0sOAS+99BLBwcHuKT4+vmoLF5GaZbNBszFw8HPISbO6GhERERGRKlUng/8TTzyBzWYrddq6dSt///vfOXXqFJMmTarS/U+aNIn09HT3dODAgSrdvohYwGaD5nfB/k8hX1fxiIiIiEj94Wl1ARXx6KOPMnr06FLbJCYmsnTpUtasWYOPj0+RZZ07d2bEiBHMmDGD6OhoUlJSiiwvfBwdHV3itn18fIptU0TqAZsdmt8NO96GZmPB08/qikREREREKq1OBv+IiAgiIiLO2e5vf/sbf/rTn9yPDx8+zMCBA5kzZw4XX3wxAN26deOpp54iPz8fLy8vABYtWkSrVq0IDQ2tnicgIrWX3dN15n/ne9DiXrB7WV2RiIiIiEil1MlL/cuqcePGtGvXzj21bNkSgGbNmhEXFwfALbfcgre3N3feeSebNm1izpw5/PWvf+WRRx6xsnQRsZKHLyTe7gr/xml1NSIiIiIilVKvg39ZBAcHs3DhQvbs2cNFF13Eo48+yjPPPMNdd91ldWkiYiWvBtDkJtj1DzDG6mpERERERCrMZoz+oq2sjIwMgoODSU9PJygoyOpyRKQqZR2C5CWQONLqSkRERERE3MqTQ8/7M/4iIqXybwQR3WHvbKsrERERERGpEAV/EZFzadAcGnaEfZ9aXYmIiIiISLkp+IuIlEVQKwhpD/v/aXUlIiIiIiLlouAvIlJWwW0gqC3s/7fVlYiIiIiIlJmCv4hIeYRc4Dr7f+AzqysRERERESkTBX8RkfIKaQcNWsCBz62uRERERETknBT8RUQqIqQ9BCbAwS+trkREREREpFQK/iIiFRWaBAGN4dBXVlciIiIiInJWCv4iIpUR2gH8YuHQPKsrEREREREpkYK/iEhlNewEvlFw6GurKxERERERKUbBX0SkKoR1Bp9wODzf6kpERERERIpQ8BcRqSrhXcG7IRz+xupKRERERETcFPxFRKpS+MXgFQRHFlpdiYiIiIgIoOAvIlL1IrqBZyAkL7a6EhERERERBX8RkWoR0R3svpC8xOpKREREROQ8p+AvIlJdInuC3RtSllldiYiIiIicxxT8RUSqU+SlgB1SVlhdiYiIiIicpxT8RUSqW1RvwEDqt1ZXIiIiIiLnIQV/EZGaEHUZOB2QusrqSkRERETkPKPgLyJSU6L7gjMX0lZbXYmIiIiInEcU/EVEalJ0P3BkQdp3VlciIiIiIucJBX8RkZoWczkUZEHqSqsrEREREZHzgIK/iIgVovuDKYDkJVZXIiIiIiL1nIK/iIhVoi4DD3849LXVlYiIiIhIPabgLyJipYhu4BcNB+ZaXYmIiIiI1FMK/iIiVmvYCRq0hL2fWF2JiIiIiNRDCv4iIrVByAUQ1hV2TwdjrK5GREREROoRBX8RkdqiQTPXoH+7/qHwLyIiIiJVRsFfRKQ28Y+DuGthx1RwFlhdjYiIiIjUAwr+IiK1jW8kNL0FdrwNznyrqxERERGROq5Swf/AgQMsXbqUrKws9zyn08nLL79Mjx496N+/P/Pmzat0kSIi5x3vEEi8Hba/DY5sq6sRERERkTrMZkzFO5KOHj2a//znPyQnJ+Pl5QXA888/z7PPPutu4+HhwXfffUeXLl0qX20tlZGRQXBwMOnp6QQFBVldjojUJwU5sPM9SBwNXvp+ERERERGX8uTQSp3xX716Nf3793eHfmMMb775Jq1bt2b//v2sXbuWgIAAXn311crsRkTk/OXhCy3ugz2zIOeo1dWIiIiISB1UqeCfmppKkyZN3I/XrVtHWloa999/P3FxcXTu3JnrrruO77//vtKFioict+ye0OJeOPgZZB20uhoRERERqWMqFfydTidOp9P9ePny5dhsNvr27eue16hRI5KTkyuzGxERsdmh2VhIWQYZ26yuRkRERETqkEoF/8aNG7N27Vr3488//5yYmBhatWrlnpecnExISEhldiMiIgA2GyTcBumb4PhPVlcjIiIiInVEpYL/0KFDWb16NcOGDePWW29l1apVDB06tEibzZs3k5iYWKkiRUTkDPFDICcNUldaXYmIiIiI1AGVCv6PPfYYXbp04bPPPuPjjz+mffv2TJ482b183759rF27lssuu6ySZYqISBGxAwEbHJ5vdSUiIiIiUst5VmbloKAg/vvf//LLL78A0KZNGzw8PIq0+eyzz+jcuXNldiMiIiWJ7Akn1sH+f0HjYVZXIyIiIiK1lM0YY6wuoq4rz/0TRUSqXMYOSFsFiaNd4wCIiIiISL1XnhxaqUv9T506xe7du8nPzy8yf86cOYwYMYIxY8bw888/V2YXIiJyLkEtIPYq2P4WFORZXY2IiIiI1DKVCv6PP/44SUlJRYL/1KlTueWWW/jkk0/44IMP6NmzJ1u3bq10oSIiUgq/KNcZ/53vQH6G1dWIiIiISC1SqeC/YsUK+vfvj7+/v3ven//8Zxo1asS3337Lp59+ijGGV199tdKFiojIOXgFQov7YO/HkHXQ6mpEREREpJaoVPA/cuQICQkJ7sdbtmzhwIEDPPDAA/Ts2ZNhw4ZxzTXX8O2331a6UBERKQO7JzS/G1JWwMmNVlcjIiIiIrVApYJ/bm4u3t7e7scrVqzAZrMxYMAA97zExEQOHTpUmd2IiEh52GyQMAJOH4DUlVZXIyIiIiIWq1Twj4uLY8OGDe7HX331FQ0bNuTCCy90zzt27BiBgYGV2Y2IiFREo6vA5gkHv7S6EhERERGxkGdlVr7yyit56623eOyxx/D19WXBggWMHDmySJvt27fTuHHjShUpIiIVFNEN0rfC7pmQcJtu9yciIiJyHrIZY0xFV05OTqZ79+7s3bsXgJiYGP73v/8RFxcHQGpqKnFxcYwfP54pU6ZUScG1UXnunygiYonsZDjwb2g2Bjx8rK5GRERERCqpPDm0Umf8o6Oj2bRpE0uWLAGgV69eRXZ49OhRXn31VQYOHFiZ3YiISGX5RUPi7bDz/6DxcNft/0RERETkvFCpM/7iojP+IlJnGAN7P4SQJAi98NztRURERKRWqrEz/mc6dOgQ69atIyMjg6CgIDp06ECjRo2qavMiIlIVbDZXX/8jC+FIMsQMOPc6IiIiIlKnVTr479y5k3vvvZelS5cWW9avXz/efvttmjdvXtndiIhIVYoZACc2aNA/ERERkfNApYL/gQMH6NmzJ6mpqbRu3ZpevXoRExNDcnIy3377LYsXL+bSSy9l7dq1xMfHV1XNIiJSFUIvdPX93/4WNLsDPP2trkhEREREqkGlgv9zzz1Hamoqb7/9NnfffTe2350xevfdd7n33nv54x//yP/93/9VqlAREakGvpHQfCzseh/irgV/ddESERERqW8qNbhffHw8nTp14osvvjhrm2uvvZYff/yRgwcPVnQ3tZ4G9xOROs8Y2DcHGjSHsM5WVyMiIiIi51CeHGqvzI5SU1Np165dqW3atWtHWlpaZXYjIiLVzWaDpjeBIxMOzbO6GhERERGpQpUK/hEREWzevLnUNps3byYiIqIyuxERkZoSdRkEJsKuD8A4ra5GRERERKpApYL/wIED+fLLL3n//fdLXP7BBx/wn//8hyuuuKIyuxERkZoU3AbirnMN+pd/yupqRERERKSSKtXHf//+/XTu3Jljx47Rtm1bevfuTVRUFCkpKXz77bds2rSJsLAwfvzxR0tH9Z83bx5//OMf2bBhA76+vvTu3ZvPP/+8yPO49957WbZsGYGBgYwaNYqXXnoJT8+yjX2oPv4iUi85811n/mMGQGCC1dWIiIiIyBnKk0MrNap/48aNWb16NXfffTfLly9n06ZNRZb36dOHd955x9LQ/+9//5uxY8fy4osv0rdvXxwOB7/88ot7eUFBAVdffTXR0dF89913HDlyhJEjR+Ll5cWLL75oWd0iIpaze0GLu2H/vyA7GSK6WV2RiIiIiFRApc74n+nAgQOsW7eOjIwMgoKC6NChA/Hx8bz88sssXLiQJUuWVMVuysXhcNC0aVOee+457rzzzhLbzJ8/n0GDBnH48GGioqIAeOedd5g4cSJpaWl4e3ufcz864y8i9V7aashJhfjrra5ERERERKjBM/5nio+PL/HM/tatW1m+fHlV7aZcfvrpJw4dOoTdbqdjx44kJyfToUMHXn31VffdCNasWUP79u3doR9cYxfce++9bNq0iY4dOxbbbm5uLrm5ue7HGRkZ1f9kRESsFNEDTu2CHe9CszvBXmX/fYiIiIhINavU4H613e7duwGYPHkyTz/9NF999RWhoaFcdtllHD9+HIDk5OQioR9wP05OTi5xuy+99BLBwcHuycquDCIiNaZBM2h6M+yYCjm6TauIiIhIXVEng/8TTzyBzWYrddq6dStOp+tWVE899RRDhw7loosuYtq0adhsNv75z39WeP+TJk0iPT3dPR04cKCqnpqISO3mFQQtx8GRhXD0v1ZXIyIiIiJlUCev1Xz00UcZPXp0qW0SExM5cuQIAG3btnXP9/HxITExkf379wMQHR3N2rVri6ybkpLiXlYSHx8ffHx8Klq+iEjdZrNDwghX8N8zC5qOcM0TERERkVqpTgb/iIgIIiIiztnuoosuwsfHh23bttGzZ08A8vPz2bt3L02aNAGgW7duvPDCC6SmphIZGQnAokWLCAoKKnLAQEREfif8EghsDtvfgiY3ge+5v5dFREREpObV61M0QUFB3HPPPTz77LMsXLiQbdu2ce+99wIwfPhwAAYMGEDbtm257bbbWL9+Pd988w1PP/0048aN01l9EZFz8Q3/7dL/tO+srkZERERESlDuM/5XXXVVudpv3LixvLuoUq+++iqenp7cdtttZGdnc/HFF7N06VJCQ0MB8PDw4KuvvuLee++lW7duBAQEMGrUKP74xz9aWreISJ3hvvT/f7r0X0RERKQWshljTHlWsNvL/8eczWajoKCg3OvVFeW5f6KISL2Wewz2fgxNbgTfSKurEREREam3ypNDy33Gf8+ePRUuTERE6jmfMNel//tmQ0BTiOhudUUiIiIi571yB//CQfFERERKZLND01vg6FrYPdN16b/dw+qqRERERM5b6oQpIiLVI7wrNBoEO6ZC9hGrqxERERE5byn4i4hI9fFp6Lr0P3UlJC+2uhoRERGR85KCv4iIVC+bDZrcAH6xsONdcGRZXZGIiIjIeaXcffxFREQqJLgtBCbCng8hrAuEJlldkYiIiMh5QWf8RUSk5nj4QvMxkHcS9n4Cxml1RSIiIiL1noK/iIjUvKjeEN0ftr8NWYetrkZERESkXlPwFxERa/hGuAb+O7oGjiy0uhoRERGRekvBX0RErGOzQeOhENDENfBffobVFYmIiIjUOxrcT0RErBfUyjXw396PoUFLiOhmdUUiIiIi9YbO+IuISO1g94LEUa5/d70PjmyrKxIRERGpF3TGX0REapewzhB8Aez7GEIudN36T0REREQqTGf8RUSk9vH0g2Z3um73t+sDKMixuiIRERGROkvBX0REaq/wi6HJTbDnQzj+k9XViIiIiNRJCv4iIlK7efpD8zGus/67pkFBrtUViYiIiNQpCv4iIlI3RHSHJjfAnplwYp3V1YiIiIjUGQr+IiJSd3gGQPOxkJ8Ju2eCM9/qikRERERqPQV/ERGpeyJ7Qvz1sHsanNxodTUiIiIitZqCv4iI1E1eDaD5XZB7HPbMAqfD6opEREREaiVPqwsQERGplKjekJcOu96HyEshuK3VFYmIiIjUKjrjLyIidZ93MLS4G3JSXLf+09l/ERERETed8RcRkfojqg/knYTdH0DYxRCaZHVFIiIiIpbTGX8REalfvENcff/zT8GuaVCQY3VFIiIiIpZS8BcRkfopsic0uQn2fgxpa6yuRkRERMQyCv4iIlJ/efpBszvAwxd2vucaBFBERETkPKM+/iIiUv817Agh7WDfHPAJh5iBYLNZXZWIiIhIjdAZfxEROT/YvSDhVghqCTumwul9VlckIiIiUiMU/EVE5PwSmAgt7oWTv7j6/+vWfyIiIlLP6VJ/ERE5/9hs0OjqX2/9Nw1CO0JYZ6urEhEREakWOuMvIiLnL+8QaD4WMLDz/yD3uNUViYiIiFQ5nfEXEREJ6wKhneDAv8DuDXHXgk3HxkVERKR+0F81IiIiAHYPaHIjhHdznf0/udHqikRERESqhIK/iIjImfyiocXdkJcOO/8B+RlWVyQiIiJSKbrUX0REpCSRPSGsK+z/J3j66/J/ERERqbP0F4yIiMjZeHhDwgjX5f+7/gFH11pdkYiIiEi5KfiLiIici180NL8L7J6w413IOmx1RSIiIiJlpuAvIiJSVg07uQ4AHP8R9syCghyrKxIRERE5J/XxFxERKQ+bDeIGg+M07JsNvlEQc4VrvoiIiEgtpDP+IiIiFeEZAImjIbgt7HxPt/8TERGRWkvBX0REpDICmrhu/5efATv/D3KOWl2RiIiISBG61F9ERKQqRPSAsEvg4Fxw5kPjYWD3sroqEREREQV/ERGRKmP3cAX+vJOwZyYEJkJUH6urEhERkfOcLvUXERGpat4h0OxO8I1x3f7v1E6rKxIREZHzmIK/iIhIdQlu7er/f3of7PoA8k9ZXZGIiIich3Spv4iISHWL7gcFubD/n+AVDI0G6fZ/IiIiUmN0xl9ERKQmePhAwq3QsOOvt//7xeqKRERE5Dyh4C8iIlKT/ONcl//nnYBd06Agz+qKREREpJ7Tpf4iIiJWiLwUQjvCvk/A5gExV4BvuNVViYiISD2k4C8iImIVr0BIHOU663/4K3BkQfxQ8PSzujIRERGpRxT8RURErObhDfFDXKP+H57nGgjQOCDsYtedAUREREQqQcFfRESktvBqAI2HuX42BtJWwc6VEHGpDgCIiIhIhWlwPxERkdrIZnONA9BsDOQdhx3vwukDVlclIiIidZDO+IuIiNRmNhtEdIfwbnBkAaQsg4DGENnbtUxERETkHBT8RURE6gKbDWKvdP18aifs+geEdoCwLpaWJSIiIrWfLvUXERGpaxo0h+ZjweYJO/8BJ9ZbXZGIiIjUYgr+IiIidVXDjtB8jOsuALve1wEAERERKZGCv4iISF0X3hWa3QnOPNcVAOlbra5IREREapF6H/y3b9/OtddeS3h4OEFBQfTs2ZNly5YVabN//36uvvpq/P39iYyMZMKECTgcDosqFhERqaCwLq4rADJ3wrHvra5GREREaol6H/wHDRqEw+Fg6dKl/PjjjyQlJTFo0CCSk5MBKCgo4OqrryYvL4/vvvuOGTNmMH36dJ555hmLKxcREamgRoMg6yBk7rG6EhEREakFbMYYY3UR1eXo0aNERETw7bffcumllwJw6tQpgoKCWLRoEf3792f+/PkMGjSIw4cPExUVBcA777zDxIkTSUtLw9vb+5z7ycjIIDg4mPT0dIKCgqr1OYmIiJTZjqkQMxBy0iDvJET3B7uH1VWJiIhIFShPDq3XZ/zDwsJo1aoVM2fO5PTp0zgcDt59910iIyO56KKLAFizZg3t27d3h36AgQMHkpGRwaZNm0rcbm5uLhkZGUUmERGRWqfZXZB1GHzCIDQJdrwFTnVlExEROd/U6+Bvs9lYvHgxP//8Mw0aNMDX15cpU6awYMECQkNDAUhOTi4S+gH348LuAL/30ksvERwc7J7i4+Or94mIiIhUhN0DInu6bv/nFw0Jo1yj/9ffi/1ERESkBHUy+D/xxBPYbLZSp61bt2KMYdy4cURGRrJy5UrWrl3Lddddx+DBgzly5EiF9z9p0iTS09Pd04EDB6rw2YmIiFQT72CIvQp2vgunD4Az3+qKREREpAZ4Wl1ARTz66KOMHj261DaJiYksXbqUr776ihMnTrj7PLz99tssWrSIGTNm8MQTTxAdHc3atWuLrJuSkgJAdHR0idv28fHBx8en8k9ERESkpgXEQ+JoOLYWjv0XHNmu+TGXg1+MpaWJiIhI9aiTwT8iIoKIiIhztsvKygLAbi96YYPdbsfpdALQrVs3XnjhBVJTU4mMjARg0aJFBAUF0bZt2yquXEREpBbw8IXIXr89dhZA8iI4vAACE13LbDbr6hMREZEqVScv9S+rbt26ERoayqhRo1i/fj3bt29nwoQJ7Nmzh6uvvhqAAQMG0LZtW2677TbWr1/PN998w9NPP824ceN0Vl9ERM4Pdg+IvQKa3Q7+jWDPDNj3KThOW12ZiIiIVIF6HfzDw8NZsGABmZmZ9O3bl86dO7Nq1Sq++OILkpKSAPDw8OCrr77Cw8ODbt26ceuttzJy5Ej++Mc/Wly9iIiIBRo0d3UFiL0KDn0Fu6dDdsmD3YqIiEjdYDNGQ/tWVnnunygiIlKnOAvgyALISYbwHhDc2uqKREREhPLl0DrZx19ERERqiN0DGl3tugXg0e9g1wcQ1ArCu2scABERkTqiXl/qLyIiIlXEZoOIHtDsDvAOg93T4NA81xUBIiIiUqvpjL+IiIiUT3Br15SdDHtngYcfNBoMnv5WVyYiIiIlUPAXERGRivGLdg0EmJ8JB78AZx7EXgm+kVZXJiIiImdQ8BcREZHK8QqEpjeD0/HrQIBpENYFQtpZXZmIiIigPv4iIiJSVeye0GgQNLsd8jNg5/9B7jGrqxIRETnv6Yy/iIiIVL2I7hB2MRz6EgqywRRA01t1JwARERELKPiLiIhI9bB7QPz1rp+zU2D3B5B4h8K/iIhIDdOl/iIiIlL9/KIgegDs/cjqSkRERM47Cv4iIiJSMwLiIfgCSFkOzgJIXgr75rgGAxQREZFqo+AvIiIiNadhR7B5wv45ENQS4odA2irY/y8wTqurExERqZfUx19ERERqVmTPoo/jr4ecVNddAILbQkQPsOnchIiISFVR8BcRERHr+UZCi7vh1E7Y+wlgABtEdIPARKurExERqdMU/EVERKT2aNDcNYHr0v+07yB1lesKgOh+4BdjbX0iIiJ1kIK/iIiI1E42u6tbQGRPcDogeQkcWQg+Ya47BHh4W12hiIhInaDgLyIiIrWf3RNiB7p+zjkKB/4NzlwI7wFBLaytTUREpJZT8BcREZG6xTccmt4MxkDaatddAXwjIGYg2L2srk5ERKTWUfAXERGRuslm+60rQE4q7JsDxgGRl0FgU6urExERqTUU/EVERKTu842EhFtdAwKmroDU5eATCTEDXN0EREREzmP6n1BERETqD5sdovq4fs5OgX2fAHaIvRJ8GlpamoiIiFUU/EVERKR+8ouChNugIA+OzIfcY66DAoEJVlcmIiJSoxT8RUREpH7z8Ia4a12DAaYudw0IiA28gqDRINdYASIiIvWYgr+IiIicH2y237oBAGQdhr0fQ8II62oSERGpAXarCxARERGxhH8sNGgOad9ZXYmIiEi1UvAXERGR81f4xZB7FPb/G/JOWF2NiIhItdCl/iIiInJ+i7sG8tIhbSXknXTdEjDuWvAOtroyERGRKqHgLyIiIuId7BroD1x3ATg4F0zBb8tNAdi8IPJS8G9kTY0iIiIVpOAvIiIiciYPb2hyY/H5BXmQtgpSlgI2sHmAVzD4RUNIe7B71XipIiIiZaHgLyIiIlIWHt4Q3fe3x8ZAfjpkH4YDn4EzH2x2aNgZGrTQbQJFRKTWUPAXERERqQibDbxDXFNwW9c8ZwEc/xH2fuR67BkAkb3AJ8yqKkVERBT8RURERKqM3QPCu7omgPxTkLoS8o67Hvs3gvDu4OFjXY0iInLeUfAXERERqS5eDaDRVb89zjoIh76EglzABqEdXFcLqFuAiIhUIwV/ERERkZriHweNh7t+Nk44sf63bgE+4RDZGzz9rKtPRETqJQV/ERERESvY7NCwo2sCyEmFw/OgIMd1h4DIXuAXY22NIiJSLyj4i4iIiNQGvpHQeJjr54JcSFsJyUsAA8EXQGhHdQkQEZEKUfAXERERqW08fCC6v+tnYyB9869dAsyvXQIuU5cAEREpMwV/ERERkdrMZoOQC1wTQM7Rol0CIi4F/1hraxQRkVpNwV9ERESkLvENP6NLQJ6rS0DKEtfj4LYQ2kldAkREpAgFfxEREZG6ysMbovu5fjYGMra4ugQYJwQ2hbCu4OFraYkiImI9BX8RERGR+sBmc53xD27renx6Hxz6ChynwasBBDSBgATwaWhtnSIiUuMU/EVERETqo4AmrgkgPwOyj0DaKshNg6A2rkECA5q6rhoQEZF6TcFfREREpL7zCnJNQa1cXQIyd0PeCTjwLzAFEHc9eAVaXaWIiFQTBX8RERGR84nNBg2auX4O6wwFua4uAc5c10EAr2CIu8baGkVEpEop+IuIiIiczzx8oPHQ3x5n7oWd/weeAcCvdweIGaixAURE6jAFfxERERH5TWBTaD72t8dOBxz4DAqywDvMdQCg4UW6W4CISB2i4C8iIiIiZ2f3hCY3uMYGcJyG3KOuAwHOfGg0CHzCrK5QRETOQcFfRERERM7NZnMNAOgV6LoqwJkPRxZCfrprbAC7D8QMAO8QqysVEZHfUfAXERERkfKze0Gjq3977MiC5MWuWwcap2vgwOC21tUnIiJuCv4iIiIiUnme/r/dDcAYOP4j7PkIcLruFBAzQOMCiIhYRMFfRERERKqWzeY64x/W2fU49xgc+BxMPngFQVRf8GpgaYkiIucTBX8RERERqV4+YdD0JtfPeemuLgGOTLB7Q2Rv8Iu2tj4RkXpOwV9EREREao53MMRf7/q5IBdSV0DyIsAGwW0gpAPYPaysUESk3lHwFxERERFrePx6JwBwjQuQsRX2zwGnA3yjIKoPeHhbW6OISD2g4C8iIiIi1rP9esY/uI3rcXYKHPgMnHmuqwQiL3P9KyIi5abgLyIiIiK1j19U0XEBUle4bhUI0LATBLVxHSwQEZFzUvAXERERkdrNO/iMWwU64fhPsPdD12OfSIjqrVsFioiUQsFfREREROoOm73orQKzU+DQf6AgB7BDWBdo0EJXA4iInEHBX0RERETqLr8oaDzc9bOzAI7/AHs/Agz4NYLIS8HuZWmJIiJWs1tdQGW88MILdO/eHX9/f0JCQkpss3//fq6++mr8/f2JjIxkwoQJOByOIm2WL19Op06d8PHxoXnz5kyfPr36ixcRERGRqmX3gPCLIeFWSLgNglrC/n/D7pmw71PIOmx1hSIilqjTZ/zz8vIYPnw43bp14/333y+2vKCggKuvvpro6Gi+++47jhw5wsiRI/Hy8uLFF18EYM+ePVx99dXcc889fPTRRyxZsoQxY8YQExPDwIEDa/opiYiIiEhV8Y/7bYDAghxIWw0pS1yP/RpBRA/XLQVFROo5mzHGWF1EZU2fPp2HHnqIkydPFpk/f/58Bg0axOHDh4mKigLgnXfeYeLEiaSlpeHt7c3EiROZN28ev/zyi3u9m266iZMnT7JgwYIy7T8jI4Pg4GDS09MJCgqqsuclIiIiItUk6yCkfQfOXLB5QNjF0KCZ1VWJiJRZeXJonb7U/1zWrFlD+/bt3aEfYODAgWRkZLBp0yZ3m/79+xdZb+DAgaxZs+as283NzSUjI6PIJCIiIiJ1iH8cNLnB1SWg8Q2QkwJ7PnRNh+eD47TVFYqIVJk6fan/uSQnJxcJ/YD7cXJycqltMjIyyM7Oxs/Pr9h2X3rpJZ577rlqqlpEREREapTdEyK6uyaAnKNw5BtwZLkeh3aA4At0pwARqbNq3Rn/J554ApvNVuq0detWS2ucNGkS6enp7unAgQOW1iMiIiIiVcg3HOKHuAYJbHoLGIfrTgF7PoSDX0DeCasrFBEpl1p3xv/RRx9l9OjRpbZJTEws07aio6NZu3ZtkXkpKSnuZYX/Fs47s01QUFCJZ/sBfHx88PHRQDAiIiIi9Z7N7jrjH9rB9Tg/A1K/hbyTrscBTVzjA3h4W1SgiMi51brgHxERQURERJVsq1u3brzwwgukpqYSGRkJwKJFiwgKCqJt27buNl9//XWR9RYtWkS3bt2qpAYRERERqUe8gqDRoN8en94HBz8HZx5gg5ALIORC1wEDEZFaotYF//LYv38/x48fZ//+/RQUFLBu3ToAmjdvTmBgIAMGDKBt27bcdtttvPLKKyQnJ/P0008zbtw49xn7e+65hzfffJPHH3+cO+64g6VLl/Lpp58yb948C5+ZiIiIiNQJAU1cE4AxkL4J9s0G4wSbJ4R3hcCyXa0qIlJd6vTt/EaPHs2MGTOKzV+2bBmXXXYZAPv27ePee+9l+fLlBAQEMGrUKP785z/j6fnbMY/ly5fz8MMPs3nzZuLi4vjDH/5wzu4GZ9Lt/ERERESkGGc+HPsBMncDBjz9IaIn+EZaXZmI1APlyaF1OvjXFgr+IiIiInJOjtOQthpyUl2PfcJcBwK8Glhbl4jUSeXJoXX6Un8RERERkTrDMwBiBvz2OOcoJC9xHRDAaKBAEak2Cv4iIiIiIlbwDYf/b+/eg6Oq7/+Pv3Y32U1CsrsJuXEJAQxyEYPciUBFiQRkWvHnWEvRwVprsTiFegUvRaejOO30YtXSOzilXwNSby0YmgJBUQRFAgQwIgSCaEIQsgkSyO3z++OQDUvCJUKyyeb5mDkznM/55ORzyJuwr8/u+ZyUaY37AQsFSvIOZqFAAJcFwR8AAABoD5pbKHD/K5KMtVBg3DAppp9kswV1mAA6HoI/AAAA0N7YbKff8R9s7dfXSkc/lvb/nxonAoZLMWlMBAC4III/AAAA0N7ZTz8aMH6UtR8wESDJ5mAiAMA5EfwBAACAjoaJAAAtQPAHAAAAOjomAgCcB8EfAAAACDUXNRHAYoFAZ0HwBwAAAELdBScC7KcnAq5kIgAIQQR/AAAAoLNpbiLg2NYznhrgkGKHSu7+TAQAIYDgDwAAAHR29jCp60hrk05PBOSfMRFgl2KvkdwDmQgAOiCCPwAAAIBA9jCp6whrk6T6Oql8W+BEgHeI5BnERADQARD8AQAAAJyf/fRigHHDrP36Oql8e+NEgGxSbLrkucqaFADQrhD8AQAAALSM3SHFDbU2yZoI8O2Q9r8iqV6STXIPsG4PsBM5gGDjXyEAAACAS2N3WCE/9hpr3xipolAqXiGZWqstspvUdbQUHh2sUQKdFsEfAAAAwOVls0meAdbW4MQXUkmuVPu1tR8eI8VnSBGJwRkj0IkQ/AEAAAC0vqjuUtQtjfs1FdKRD6STh619e7gUN1yKvoIFA4HLjOAPAAAAoO2Fu6Vukxr366qlYx9LRzbJWjBQkmeg9fQA1gkALgn/ggAAAAAEn8MpxY+xNkky9afXCXhVMnVWW1QPqesoKaxL8MYJdEAEfwAAAADtj81uvePvGdjYduKQ9OVqqfaEtR/uPr1OQEJwxgh0EAR/AAAAAB1DVA8p6v817lf7pCMbpVNHrH17uBQ3QoruyzoBwBkI/gAAAAA6JqdH6j65cb/ulHT0Y2vRwIZ1AqJ6WJMB4TFBGSLQHhD8AQAAAIQGh0tKyLC2BicOSaVrpZpKa98ebi0Y6L7Sup0A6AQI/gAAAABCV1QPa2tQXyOVb5cOZFsLCEqSK0GKHyU5Y4MzRqCVEfwBAAAAdB72cCluuLU1OFkmlb0vVR+z9m12yT1Q8g62+gMdHMEfAAAAQOcWkSD1mNq4X18nVRZKB1+T6mutNofLukUgJo2FA9HhEPwBAAAA4Ex2h+QZZG0N6k5Kx7ZLB16RzOmFA8PdUteRUmRycMYJXCSCPwAAAABciCPCWgcgflRjW3W5dHSLVLKmsS2qu3UbQbi7zYcInAvBHwAAAAC+CadXSp4Y2HbikFSa1/gUAZtNiuknedOt2wWAICD4AwAAAMDlcvZTBEy9VPmZdOgtqa7aarOHWxMBPFIQbYTgDwAAAACtxWa3Ar77ysa2umrJt0M6sKzxkYLh0VLciMBJA+AyIfgDAAAAQFtyOJs+UrCm0lovoDSvsS0iUeo6QnLGtvkQEVoI/gAAAAAQbOExUtKEwLaqUqnsfWsRQZvNeppAzBVS7DXWYoPARSL4AwAAAEB7FJkk9ZjauG+MdHyfdGil9XhBSbKHSZ6rJPdA6zGEQDMI/gAAAADQEdhs1jv+MVc0ttXXSr6dUvHyxvUCHE7Jc7X1NAEmAyCCPwAAAAB0XPYwKXaItTWoOyX5CgInA+xhkmfQ6U8GEAM7G37iAAAAABBKHK6miwfW10i+3dLBf1mfEpAkm0Ny97duFXA4gzNWtAmCPwAAAACEOnu4FJtubQ3q66TKQunQm9bEgDHW7QRd+kjeq61HDCIkEPwBAAAAoDOyO6yP/3sGNbYZI329XypdI9Ucb2x3eq3bCSJ7WJMD6FAI/gAAAAAAi80mRfextjNVH5OObZdK1ze22cOsNQM8A61PFKDdIvgDAAAAAM7PGSslXRfYVl8jVXwiff6GVFfd2B7Vw/p0gDO2TYeIcyP4AwAAAABazh5urQXgvbqxzRip6gupbKP1KQGbzWoL62L1i+4j2ezBG3MnRfAHAAAAAFweNpv1jn9Uj8D22q+l8h3SV5sbHzFos0sxaZJnsBQW2fZj7UQI/gAAAACA1hXWRYofY20N6uuk43ulL9+Waqsa213xpxcSTG77cYYogj8AAAAAoO3ZHZL7Sms708kyqXybVLKmsc3hlDxXSTFXWosKokX4GwMAAAAAtB8RCVJyZmBb3UnJt0sqXiGZusb2LqlSbLoU7m7bMXYwBH8AAAAAQPvmiJDihllbA2OkE8VSaZ5UU2mtLyBJdpfkGSTF9OPTAafxtwAAAAAA6HhsNusd/y6pge11JyXfbunga5KpbWx3xloLCUb1bJwk6CQI/gAAAACA0OGIkOKGWtuZTh2VfDulsg2STGN7VE/rUYPO2DYdZlsi+AMAAAAAQp8rTkocH9hmjHTic+nIB1L1scb27jdJTm+bDq81EfwBAAAAAJ2TzSZ1SbG2EGYP9gAAAAAAAEDrIfgDAAAAABDCCP4AAAAAAIQwgj8AAAAAACGM4A8AAAAAQAgj+AMAAAAAEMII/gAAAAAAhLAOHfyfeeYZXXvttYqKipLX621yfNu2bZo+fbpSUlIUGRmpgQMH6vnnn2/SLy8vT8OGDZPL5VJaWpqWLFnS+oMHAAAAAKANdOjgX11drdtuu0333Xdfs8e3bNmixMRELV26VDt37tTjjz+u+fPn68UXX/T3KSoq0tSpU3X99dcrPz9fc+fO1T333KPVq1e31WUAAAAAANBqbMYYE+xBXKolS5Zo7ty5Ki8vv2Df2bNna/fu3Vq7dq0k6dFHH9XKlStVUFDg7/O9731P5eXlysnJuajvX1FRIY/HI5/PJ7fb/Y2uAQAAAACAi9WSHNqh3/H/Jnw+n+Li4vz7GzduVGZmZkCfrKwsbdy48ZznOHXqlCoqKgI2AAAAAADao04V/N9//30tW7ZM9957r7+tpKRESUlJAf2SkpJUUVGhqqqqZs+zcOFCeTwe/5aSktKq4wYAAAAA4Jtqd8F/3rx5stls590++eSTFp+3oKBAN998sxYsWKBJkyZd0hjnz58vn8/n3w4ePHhJ5wMAAAAAoLWEBXsAZ3vwwQd11113nbdP3759W3TOXbt2aeLEibr33nv1xBNPBBxLTk5WaWlpQFtpaancbrciIyObPZ/L5ZLL5WrRGAAAAAAACIZ2F/wTEhKUkJBw2c63c+dO3XDDDZo5c6aeeeaZJsczMjK0atWqgLbc3FxlZGRctjEAAAAAABAs7S74t0RxcbGOHj2q4uJi1dXVKT8/X5KUlpam6OhoFRQU6IYbblBWVpYeeOABlZSUSJIcDod/cmHWrFl68cUX9cgjj+juu+/W2rVrtXz5cq1cuTJYlwUAAAAAwGXToR/nd9ddd+nll19u0r5u3TpNmDBBTz31lJ5++ukmx1NTU7V//37/fl5enn72s59p165d6tmzp5588skL3m5wJh7nBwAAAABoSy3JoR06+LcXBH8AAAAAQFtqSQ5td6v6AwAAAACAy6dD3+PfXjR8aKKioiLIIwEAAAAAdAYN+fNiPsRP8L8MKisrJUkpKSlBHgkAAAAAoDOprKyUx+M5bx/u8b8M6uvr9cUXXygmJkY2my3YwzmviooKpaSk6ODBg6xHgAuiXtBS1AxaippBS1EzaClqBi3VUWrGGKPKykp1795ddvv57+LnHf/LwG63q2fPnsEeRou43e52XcRoX6gXtBQ1g5aiZtBS1AxaippBS3WEmrnQO/0NWNwPAAAAAIAQRvAHAAAAACCEEfw7GZfLpQULFsjlcgV7KOgAqBe0FDWDlqJm0FLUDFqKmkFLhWLNsLgfAAAAAAAhjHf8AQAAAAAIYQR/AAAAAABCGMEfAAAAAIAQRvAHAAAAACCEEfw7kZdeekm9e/dWRESERo8erc2bNwd7SGgD77zzjr797W+re/fustlseuONNwKOG2P085//XN26dVNkZKQyMzO1Z8+egD5Hjx7VjBkz5Ha75fV69cMf/lDHjx8P6LN9+3aNHz9eERERSklJ0S9/+cvWvjS0koULF2rkyJGKiYlRYmKipk2bpsLCwoA+J0+e1OzZs9W1a1dFR0fr1ltvVWlpaUCf4uJiTZ06VVFRUUpMTNTDDz+s2tragD55eXkaNmyYXC6X0tLStGTJkta+PLSCRYsWKT09XW63W263WxkZGXr77bf9x6kXnM9zzz0nm82muXPn+tuoGZztqaeeks1mC9gGDBjgP07NoDmHDh3SHXfcoa5duyoyMlJXX321PvroI//xTvU62KBTyM7ONk6n0/z97383O3fuND/60Y+M1+s1paWlwR4aWtmqVavM448/bl577TUjybz++usBx5977jnj8XjMG2+8YbZt22a+853vmD59+piqqip/n8mTJ5shQ4aYDz74wLz77rsmLS3NTJ8+3X/c5/OZpKQkM2PGDFNQUGBeeeUVExkZaf70pz+11WXiMsrKyjKLFy82BQUFJj8/39x0002mV69e5vjx4/4+s2bNMikpKWbNmjXmo48+MmPGjDHXXnut/3htba0ZPHiwyczMNFu3bjWrVq0y8fHxZv78+f4++/btM1FRUeaBBx4wu3btMi+88IJxOBwmJyenTa8Xl+6tt94yK1euNJ9++qkpLCw0jz32mAkPDzcFBQXGGOoF57Z582bTu3dvk56ebubMmeNvp2ZwtgULFpirrrrKfPnll/6trKzMf5yawdmOHj1qUlNTzV133WU2bdpk9u3bZ1avXm0+++wzf5/O9DqY4N9JjBo1ysyePdu/X1dXZ7p3724WLlwYxFGhrZ0d/Ovr601ycrL51a9+5W8rLy83LpfLvPLKK8YYY3bt2mUkmQ8//NDf5+233zY2m80cOnTIGGPMH/7wBxMbG2tOnTrl7/Poo4+a/v37t/IVoS0cPnzYSDLr1683xlg1Eh4ebl599VV/n927dxtJZuPGjcYYa8LJbrebkpISf59FixYZt9vtr5NHHnnEXHXVVQHf6/bbbzdZWVmtfUloA7Gxseavf/0r9YJzqqysNP369TO5ubnmuuuu8wd/agbNWbBggRkyZEizx6gZNOfRRx8148aNO+fxzvY6mI/6dwLV1dXasmWLMjMz/W12u12ZmZnauHFjEEeGYCsqKlJJSUlAbXg8Ho0ePdpfGxs3bpTX69WIESP8fTIzM2W327Vp0yZ/n29961tyOp3+PllZWSosLNSxY8fa6GrQWnw+nyQpLi5OkrRlyxbV1NQE1M2AAQPUq1evgLq5+uqrlZSU5O+TlZWliooK7dy509/nzHM09OH3UsdWV1en7Oxsff3118rIyKBecE6zZ8/W1KlTm/xcqRmcy549e9S9e3f17dtXM2bMUHFxsSRqBs176623NGLECN12221KTEzU0KFD9Ze//MV/vLO9Dib4dwJHjhxRXV1dwC86SUpKSlJJSUmQRoX2oOHnf77aKCkpUWJiYsDxsLAwxcXFBfRp7hxnfg90TPX19Zo7d67Gjh2rwYMHS7J+pk6nU16vN6Dv2XVzoZo4V5+KigpVVVW1xuWgFe3YsUPR0dFyuVyaNWuWXn/9dQ0aNIh6QbOys7P18ccfa+HChU2OUTNozujRo7VkyRLl5ORo0aJFKioq0vjx41VZWUnNoFn79u3TokWL1K9fP61evVr33XeffvrTn+rll1+W1PleB4cFewAAgPZr9uzZKigo0IYNG4I9FLRz/fv3V35+vnw+n1asWKGZM2dq/fr1wR4W2qGDBw9qzpw5ys3NVURERLCHgw5iypQp/j+np6dr9OjRSk1N1fLlyxUZGRnEkaG9qq+v14gRI/Tss89KkoYOHaqCggL98Y9/1MyZM4M8urbHO/6dQHx8vBwOR5OVTUtLS5WcnBykUaE9aPj5n682kpOTdfjw4YDjtbW1Onr0aECf5s5x5vdAx3P//ffrP//5j9atW6eePXv625OTk1VdXa3y8vKA/mfXzYVq4lx93G43L+I6IKfTqbS0NA0fPlwLFy7UkCFD9Pzzz1MvaGLLli06fPiwhg0bprCwMIWFhWn9+vX6/e9/r7CwMCUlJVEzuCCv16srr7xSn332Gb9n0Kxu3bpp0KBBAW0DBw703yLS2V4HE/w7AafTqeHDh2vNmjX+tvr6eq1Zs0YZGRlBHBmCrU+fPkpOTg6ojYqKCm3atMlfGxkZGSovL9eWLVv8fdauXav6+nqNHj3a3+edd95RTU2Nv09ubq769++v2NjYNroaXC7GGN1///16/fXXtXbtWvXp0yfg+PDhwxUeHh5QN4WFhSouLg6omx07dgT8Z5mbmyu32+3/TzgjIyPgHA19+L0UGurr63Xq1CnqBU1MnDhRO3bsUH5+vn8bMWKEZsyY4f8zNYMLOX78uPbu3atu3brxewbNGjt2bJPHEX/66adKTU2V1AlfBwd7dUG0jezsbONyucySJUvMrl27zL333mu8Xm/AyqYITZWVlWbr1q1m69atRpL5zW9+Y7Zu3WoOHDhgjLEeY+L1es2bb75ptm/fbm6++eZmH2MydOhQs2nTJrNhwwbTr1+/gMeYlJeXm6SkJHPnnXeagoICk52dbaKiotrdY0xwce677z7j8XhMXl5ewGOTTpw44e8za9Ys06tXL7N27Vrz0UcfmYyMDJORkeE/3vDYpEmTJpn8/HyTk5NjEhISmn1s0sMPP2x2795tXnrpJR6b1EHNmzfPrF+/3hQVFZnt27ebefPmGZvNZv773/8aY6gXXNiZq/obQ82gqQcffNDk5eWZoqIi895775nMzEwTHx9vDh8+bIyhZtDU5s2bTVhYmHnmmWfMnj17zD//+U8TFRVlli5d6u/TmV4HE/w7kRdeeMH06tXLOJ1OM2rUKPPBBx8Ee0hoA+vWrTOSmmwzZ840xliPMnnyySdNUlKScblcZuLEiaawsDDgHF999ZWZPn26iY6ONm632/zgBz8wlZWVAX22bdtmxo0bZ1wul+nRo4d57rnn2uoScZk1Vy+SzOLFi/19qqqqzE9+8hMTGxtroqKizC233GK+/PLLgPPs37/fTJkyxURGRpr4+Hjz4IMPmpqamoA+69atM9dcc41xOp2mb9++Ad8DHcfdd99tUlNTjdPpNAkJCWbixIn+0G8M9YILOzv4UzM42+233266detmnE6n6dGjh7n99tsDnsdOzaA5//73v83gwYONy+UyAwYMMH/+858Djnem18E2Y4wJzmcNAAAAAABAa+MefwAAAAAAQhjBHwAAAACAEEbwBwAAAAAghBH8AQAAAAAIYQR/AAAAAABCGMEfAAAAAIAQRvAHAAAAACCEEfwBAAAAAAhhBH8AABAyevfurd69ewd7GAAAtCsEfwAAEGD//v2y2Wzn3QjXAAB0HGHBHgAAAGifrrjiCt1xxx3NHvN6vW07GAAA8I0R/AEAQLPS0tL01FNPBXsYAADgEvFRfwAAcElsNpsmTJigzz//XNOnT1d8fLyioqI0duxY/e9//2v2a44cOaK5c+eqT58+crlcSkxM1He/+10VFBQ027+6ulq//e1vNXLkSMXExCg6OlqDBg3SAw88oGPHjjXpf/z4cc2ZM0fdu3eXy+VSenq6VqxYcVmvGwCAjsJmjDHBHgQAAGg/9u/frz59+igrK0s5OTkX7G+z2ZSenq7y8nIlJCQoMzNTZWVlWrZsmU6ePKkVK1Zo2rRp/v5lZWXKyMjQ3r17NWHCBI0ZM0ZFRUVasWKFXC6XVq9erXHjxvn7V1VV6cYbb9R7772nfv36afLkyXK5XNqzZ49yc3P13nvv6ZprrpFkLe5XU1Oj1NRUHTt2TJmZmTpx4oSys7NVVVWlnJwcTZo06XL/lQEA0K4R/AEAQICG4H++e/zHjBmjyZMnS7KCvyR9//vf19KlS/3727dv18iRI+XxeHTgwAFFRkZKku6++24tXrxY8+fP17PPPus/56pVqzR16lSlpaWpsLBQdrv1wcSHHnpIv/71r3XnnXdq8eLFcjgc/q/x+XxyOByKjo6WZAX/AwcO6Oabb9by5cvldDolSWvWrFFmZuZFT2YAABBKCP4AACBAQ/A/nzlz5uh3v/udJCv4OxwO7d27V6mpqQH97rnnHv3tb3/TihUrdOutt6q6uloej0ddunRRcXGxoqKiAvpPmjRJubm5eueddzR+/HjV1tYqLi5OdrtdRUVFio2NPe+4GoL/vn37mlxD7969VVlZqa+++uoi/yYAAAgN3OMPAACalZWVJWNMs1tD6G/Qq1evJqFfksaPHy9J2rp1qyTpk08+0cmTJzVq1KgmoV+Srr/+eklSfn6+v39lZaVGjhx5wdDfwOv1Njtx0bNnT5WXl1/UOQAACCUEfwAAcMmSkpLO2+7z+SRJFRUV5+3frVu3gH4NX9ejR4+LHovH42m2PSwsTPX19Rd9HgAAQgXBHwAAXLLS0tLztjeEcbfbfd7+JSUlAf28Xq8k6dChQ5dtrAAAdDYEfwAAcMmKi4t14MCBJu3vvvuuJGno0KGSpAEDBigiIkIffvihTpw40aR/Xl6eJPlX6e/fv7/cbrc+/PDDZh/bBwAALozgDwAALlldXZ0ee+wxnblm8Pbt2/WPf/xDCQkJuummmyRJTqdT06dP15EjR7Rw4cKAc+Tk5Gj16tVKS0vT2LFjJVkfz//xj38sn8+nOXPmqK6uLuBrfD6fjh8/3spXBwBAx8aq/gAAIMDFPM5PkubNm6eIiAjZbDalp6ervLxcCQkJyszMVFlZmZYtW6aqqir961//0rRp0/xfV1ZWpjFjxmjfvn264YYbNHr0aO3fv1+vvvqqnE6nVq9erXHjxvn7nzx5UpMmTdK7776rfv36acqUKXK5XNq3b59ycnK0YcMG/ycEevfu7b+Gs02YMEHr168XL30AAJ0NwR8AAAS4mMf5SdKxY8fk9Xpls9l03XXXaenSpXrooYeUm5urEydOaOjQoXr66ad14403NvnaI0eO6Be/+IXefPNNffHFF/J4PJowYYIWLFigwYMHN+l/6tQpvfjii1q6dKkKCwvlcDjUq1cvTZkyRU888YR/LQCCPwAATRH8AQDAJWkI/g335wMAgPaFe/wBAAAAAAhhBH8AAAAAAEIYwR8AAAAAgBAWFuwBAACAjo3lggAAaN94xx8AAAAAgBBG8AcAAAAAIIQR/AEAAAAACGEEfwAAAAAAQhjBHwAAAACAEEbwBwAAAAAghBH8AQAAAAAIYQR/AAAAAABC2P8HaIwt33sV7L8AAAAASUVORK5CYII=",
                        "text/plain": [
                            "<Figure size 1200x600 with 1 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Saved loss plot to: ../run/gan/0002_horses/wassersteinloss_vs_batch.png\n"
                    ]
                }
            ],
            "source": [
                "# =============================================================================\n",
                "# Wasserstein Loss Plot\n",
                "# =============================================================================\n",
                "# This plot shows the progression of critic and generator losses during\n",
                "# training. The Wasserstein distance serves as a measure of how well the\n",
                "# generator is learning to approximate the real data distribution.\n",
                "# =============================================================================\n",
                "\n",
                "# Create figure for the loss visualization\n",
                "fig = plt.figure(figsize=(12, 6))\n",
                "\n",
                "# Plot critic losses:\n",
                "# - d_losses[0]: Combined D loss (average of real and fake)\n",
                "# - d_losses[1]: D loss on real images (R)\n",
                "# - d_losses[2]: D loss on fake images (F)\n",
                "plt.plot(\n",
                "    [x[0] for x in gan.d_losses],\n",
                "    color=\"black\",\n",
                "    linewidth=0.25,\n",
                "    label=\"D Loss (combined)\"\n",
                ")\n",
                "plt.plot(\n",
                "    [x[1] for x in gan.d_losses],\n",
                "    color=\"green\",\n",
                "    linewidth=0.25,\n",
                "    label=\"D Loss (real)\"\n",
                ")\n",
                "plt.plot(\n",
                "    [x[2] for x in gan.d_losses],\n",
                "    color=\"red\",\n",
                "    linewidth=0.25,\n",
                "    label=\"D Loss (fake)\"\n",
                ")\n",
                "\n",
                "# Plot generator loss (Wasserstein distance estimate)\n",
                "plt.plot(gan.g_losses, color=\"orange\", linewidth=0.25, label=\"G Loss\")\n",
                "\n",
                "# Configure axis labels and legend\n",
                "plt.xlabel(\"Epoch\", fontsize=14)\n",
                "plt.ylabel(\"Loss\", fontsize=14)\n",
                "plt.title(\"WGAN Training: Wasserstein Loss vs Epoch\", fontsize=16)\n",
                "plt.legend(loc=\"upper right\")\n",
                "\n",
                "# Display the figure in the notebook\n",
                "plt.show()\n",
                "\n",
                "# Save the figure to the run folder for later reference\n",
                "fig.savefig(os.path.join(RUN_FOLDER, \"wassersteinloss_vs_batch.png\"), dpi=300)\n",
                "print(f\"Saved loss plot to: {os.path.join(RUN_FOLDER, 'wassersteinloss_vs_batch.png')}\")\n",
                "\n",
                "# Log loss plot to W&B\n",
                "wandb.log({\"loss_plot\": wandb.Image(fig)})"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Image Comparison Utility\n",
                "\n",
                "Define a helper function to compare images using Mean Absolute Error (MAE)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compare_images(img1, img2):\n",
                "    \"\"\"\n",
                "    Compute Mean Absolute Error (MAE) between two images.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    img1 : np.ndarray\n",
                "        First image array.\n",
                "    img2 : np.ndarray\n",
                "        Second image array (same shape as img1).\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    float\n",
                "        Mean absolute difference between the two images.\n",
                "    \"\"\"\n",
                "    return np.mean(np.abs(img1 - img2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Real Training Images Grid\n",
                "\n",
                "This cell displays a 5x5 grid of randomly sampled real training images from the CIFAR-10 horses dataset.\n",
                "\n",
                "**Purpose:**\n",
                "Provides a visual reference of what the real training data looks like, allowing direct comparison with generated samples to assess generator quality."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 1500x1500 with 25 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Saved real images to: ../run/gan/0002_horses/images/real.png\n"
                    ]
                }
            ],
            "source": [
                "# =============================================================================\n",
                "# Real Training Images Visualization\n",
                "# =============================================================================\n",
                "# Display a grid of real training images for visual comparison with\n",
                "# generated samples. This helps assess how well the generator has learned.\n",
                "# =============================================================================\n",
                "\n",
                "# Grid dimensions\n",
                "r, c = 5, 5\n",
                "\n",
                "# Randomly sample images from the training set\n",
                "idx = np.random.randint(0, x_train.shape[0], BATCH_SIZE)\n",
                "\n",
                "# Rescale from [-1, 1] to [0, 1] for visualization\n",
                "true_imgs = (x_train[idx] + 1) * 0.5\n",
                "\n",
                "# Create the figure and subplot grid\n",
                "fig, axs = plt.subplots(r, c, figsize=(15, 15))\n",
                "cnt = 0\n",
                "\n",
                "# Plot each image in the grid\n",
                "for i in range(r):\n",
                "    for j in range(c):\n",
                "        axs[i, j].imshow(true_imgs[cnt])\n",
                "        axs[i, j].axis(\"off\")\n",
                "        cnt += 1\n",
                "\n",
                "plt.suptitle(\"Real Training Images (CIFAR-10 Horses)\", fontsize=16)\n",
                "\n",
                "# Display the figure in the notebook\n",
                "plt.show()\n",
                "\n",
                "# Save the figure\n",
                "fig.savefig(os.path.join(RUN_FOLDER, \"images/real.png\"), dpi=300)\n",
                "print(f\"Saved real images to: {os.path.join(RUN_FOLDER, 'images/real.png')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Generated Sample Images\n",
                "\n",
                "This cell generates a 5x5 grid of images by sampling random noise vectors and passing them through the trained generator.\n",
                "\n",
                "**What to look for:**\n",
                "- **Diversity**: Are the generated horses varied in pose, color, and background?\n",
                "- **Quality**: Do the images have recognizable horse-like features?\n",
                "- **Artifacts**: Are there obvious visual glitches or noise patterns?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "ename": "SyntaxError",
                    "evalue": "'(' was never closed (889111357.py, line 18)",
                    "output_type": "error",
                    "traceback": [
                        "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mgen_imgs = 0.5 * (gen_imgs + 1\u001b[39m\n                     ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m '(' was never closed\n"
                    ]
                }
            ],
            "source": [
                "# =============================================================================\n",
                "# Generated Sample Images Visualization\n",
                "# =============================================================================\n",
                "# Generate new images by sampling from the latent space and passing\n",
                "# through the trained generator network.\n",
                "# =============================================================================\n",
                "\n",
                "# Grid dimensions\n",
                "r, c = 5, 5\n",
                "\n",
                "# Sample random noise vectors from standard normal distribution\n",
                "noise = np.random.normal(0, 1, (r * c, gan.z_dim))\n",
                "\n",
                "# Generate images using the trained generator\n",
                "gen_imgs = gan.generator.predict(noise)\n",
                "\n",
                "# Rescale from [-1, 1] to [0, 1] for visualization\n",
                "gen_imgs = 0.5 * (gen_imgs + 1)\n",
                "\n",
                "# Create the figure and subplot grid\n",
                "fig, axs = plt.subplots(r, c, figsize=(15, 15))\n",
                "cnt = 0\n",
                "\n",
                "# Plot each generated image\n",
                "for i in range(r):\n",
                "    for j in range(c):\n",
                "        axs[i, j].imshow(np.squeeze(gen_imgs[cnt, :, :, :]))\n",
                "        axs[i, j].axis(\"off\")\n",
                "        cnt += 1\n",
                "\n",
                "plt.suptitle(\"Generated Horse Images from WGAN\", fontsize=16)\n",
                "\n",
                "# Display the figure in the notebook\n",
                "plt.show()\n",
                "\n",
                "# Save the figure\n",
                "fig.savefig(os.path.join(RUN_FOLDER, \"images/sample.png\"), dpi=300)\n",
                "print(f\"Saved generated samples to: {os.path.join(RUN_FOLDER, 'images/sample.png')}\")\n",
                "\n",
                "# Log generated images to W&B\n",
                "log_images(\n",
                "    [gen_imgs[i] for i in range(min(16, len(gen_imgs)))],\n",
                "    key=\"final_generated_images\"\n",
                ")\n",
                "print(\"\u2713 Generated images logged to W&B\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Closest Matching Training Images\n",
                "\n",
                "This cell finds the closest real training image for each generated sample using Mean Absolute Error (MAE) as the distance metric.\n",
                "\n",
                "**Why this matters:**\n",
                "This visualization helps detect **memorization** - if the generator is simply copying training images, the closest matches will be nearly identical to the generated samples. Ideally, generated images should be similar in style but not exact copies, indicating the model has learned the underlying data distribution rather than memorizing specific examples.\n",
                "\n",
                "**What to look for:**\n",
                "- Generated and closest images should be similar in style/content\n",
                "- They should NOT be pixel-perfect matches (would indicate memorization)\n",
                "- Diverse closest matches indicate good mode coverage"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# Closest Matching Training Images\n",
                "# =============================================================================\n",
                "# For each generated image, find the closest real training image.\n",
                "# This helps verify the generator is learning the distribution,\n",
                "# not simply memorizing training examples.\n",
                "# =============================================================================\n",
                "\n",
                "# Create the figure and subplot grid\n",
                "fig, axs = plt.subplots(r, c, figsize=(15, 15))\n",
                "cnt = 0\n",
                "\n",
                "# Rescale training images for comparison\n",
                "x_train_scaled = (x_train + 1) * 0.5\n",
                "\n",
                "# Find closest match for each generated image\n",
                "for i in range(r):\n",
                "    for j in range(c):\n",
                "        # Initialize with a large difference\n",
                "        c_diff = 99999\n",
                "        c_img = None\n",
                "\n",
                "        # Search through all training images\n",
                "        for k_idx, k in enumerate(x_train_scaled):\n",
                "            # Compute Mean Absolute Error between generated and real image\n",
                "            diff = compare_images(gen_imgs[cnt, :, :, :], k)\n",
                "\n",
                "            # Update if this is the closest match so far\n",
                "            if diff < c_diff:\n",
                "                c_img = np.copy(k)\n",
                "                c_diff = diff\n",
                "\n",
                "        # Plot the closest matching real image\n",
                "        axs[i, j].imshow(c_img)\n",
                "        axs[i, j].axis(\"off\")\n",
                "        cnt += 1\n",
                "\n",
                "plt.suptitle(\"Closest Real Images to Generated Samples\", fontsize=16)\n",
                "\n",
                "# Display the figure in the notebook\n",
                "plt.show()\n",
                "\n",
                "# Save the figure\n",
                "fig.savefig(os.path.join(RUN_FOLDER, \"images/sample_closest.png\"), dpi=300)\n",
                "print(f\"Saved closest matches to: {os.path.join(RUN_FOLDER, 'images/sample_closest.png')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Finalize W&B Run\n",
                "\n",
                "Complete the W&B run and sync all logged data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# FINALIZE W&B RUN\n",
                "# =============================================================================\n",
                "# Complete the experiment run and sync all data to W&B servers.\n",
                "\n",
                "wandb.finish()\n",
                "print(\"\u2713 W&B run completed and synced\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cleanup: Restart Kernel\n",
                "\n",
                "Restart the kernel to fully release GPU memory. TensorFlow/CUDA does not release\n",
                "GPU memory within a running Python process - kernel restart is the only guaranteed method."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# CLEANUP: Restart kernel to fully release GPU memory\n",
                "# =============================================================================\n",
                "# TensorFlow/CUDA does not release GPU memory within a running Python process.\n",
                "# Restarting the kernel is the only guaranteed way to free all GPU resources.\n",
                "#\n",
                "# UNCOMMENT the lines below when you want to release GPU memory:\n",
                "\n",
                "# import IPython\n",
                "# print(\"Restarting kernel to release GPU memory...\")\n",
                "# IPython.Application.instance().kernel.do_shutdown(restart=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Experiment Log\n",
                "\n",
                "This section tracks all training experiments. **Full analysis details are logged to W&B and saved as markdown reports.**\n",
                "\n",
                "### Master Experiment Log\n",
                "\n",
                "| Run | Date | W&B URL | Batch Size | Epochs | LR | Stability | Final D Loss | Final G Loss | Notes |\n",
                "|-----|------|---------|------------|--------|-----|-----------|--------------|--------------|-------|\n",
                "| 001 | 2026-01-07 | [View](https://wandb.ai/cataluna84/generative-deep-learning/runs/x5ln97by) | 512 | 6000 | 5e-5 | \u2705 Stable | 5.35 | -116.4 | Baseline |\n",
                "| 002 | - | - | - | - | - | - | - | - | *Placeholder* |\n",
                "| 003 | - | - | - | - | - | - | - | - | *Placeholder* |\n",
                "\n",
                "### Comparison Notes\n",
                "\n",
                "*Add observations comparing runs here after experiments.*\n",
                "\n",
                "---\n",
                "\n",
                "## What's Logged to W&B\n",
                "\n",
                "Each run automatically logs the following (click W&B URL to view):\n",
                "\n",
                "| Category | Items Logged |\n",
                "|----------|--------------|\n",
                "| **Config** | batch_size, epochs, lr_critic, lr_gen, n_critic, clip_threshold, z_dim |\n",
                "| **Metrics** | d_loss, d_loss_real, d_loss_fake, g_loss (per epoch) |\n",
                "| **Tables** | phase_metrics, stability_indicators |\n",
                "| **Summary** | training_stability, training_quality, final losses, verdict |\n",
                "| **Media** | Generated images, loss plots |\n",
                "\n",
                "### Per-Run Analysis Reports\n",
                "\n",
                "Each run generates `analysis_report.md` in the run folder containing:\n",
                "- Training verdict (stability score)\n",
                "- Full configuration table\n",
                "- Phase-wise metrics breakdown\n",
                "- Stability indicators with observations\n",
                "\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}